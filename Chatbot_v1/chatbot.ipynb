{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from konlpy.tag import Komoran; tokenizer = Komoran()\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "아내가 이제 건강해져서 여행도 다니고 있어. 그런 모습을 보는 것이 참 기뻐.\t아내가 건강해져서 여행도 다니실 수 있게 되다니 참 다행이에요.\n\n그냥 공무원 친구들의 노력을 생각하면서 인정해주려고. 걔들이 노력한 만큼의 대가를 받는 거겠지 뭐.\t당신의 생각대로 잘 돼서 질투심을 덜길 바라요.\n\n생각도 못했는데 현명한 아내를 둬서 너무 고마워.\t아내에게 고마운 마음을 어떻게 전하면 좋을까요?\n\n오래 사귄 여자 친구와는 처음 만날 때처럼 떨림은 없지만 편안한 안정감을 느낄 수 있어.\t오래 사귄 여자친구에게 편안한 안정감을 느껴서 행복하시군요!\n\n응. 너무 편하다. 다음 달 대출 상황도 연체되지 않게 열심히 일해야겠어.\t열심히 일해서 다음 달 대출 상환도 기한을 지키길 바라요.\n\n다 내 잘못이지\t아니에요. 너무 자책하지 마세요.\n\n착해서 잘해주는 건지 좋아하는 건지\t헷갈린다고 말해보세요.\n\n선생님께 따돌림 사실을 진작 알렸다면 이렇게까지 심각해지지 않았을지도 모를 것 같아. 요새 너무 슬퍼.\t일찍 선생님께 말씀 드리지 못한 것이 후회되시나 봐요.\n\n나이를 먹으면서 몸 관리를 소홀하게 했더니 이렇게 빨리 늙어버렸네.\t다른 일로 바쁘셨나 봐요. 지금이라도 운동하며 관리 하면 어떨까요?\n\n주말은 근무시간이 아닌데 자꾸 회사 사람들이 불러내서 짜증나.\t주말에 회사 사람들 때문에 쉬질 못하여서 화가 나셨군요. 조금 더 자세히 말씀해주시겠어요?\n\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"normal chatbot data\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "textfilename = \"shuffled_data.txt\"\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'r', encoding = 'utf-8') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, textfilename))\n",
    "datafile = os.path.join(corpus, textfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 단어 토큰 값\n",
    "PAD_token = 0  # 짧은 문장을 채울(패딩, PADding) 때 사용할 제로 토큰\n",
    "SOS_token = 1  # 문장의 시작(SOS, Start Of Sentence)을 나타내는 토큰\n",
    "EOS_token = 2  # 문장의 끝(EOS, End Of Sentence)을 나태는 토큰\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # SOS, EOS, PAD를 센 것\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in tokenizer.morphs(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # 등장 횟수가 기준 이하인 단어를 정리합니다\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # 사전을 다시 초기화힙니다\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # 기본 토큰을 센 것\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 37846 sentence pairs\n",
      "Trimmed to 13457 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 5969\n",
      "\n",
      "pairs:\n",
      "['다 내 잘못이지', '아니에요. 너무 자책하지 마세요.']\n",
      "['착해서 잘해주는 건지 좋아하는 건지', '헷갈린다고 말해보세요.']\n",
      "['참 힘들다', '말해줘서 고마워요.']\n",
      "['헤어지고 나니까 휘청거리네여', '정신 차리세요.']\n",
      "['남자친구와 진도가 보통 어때?', '사람마다 다르겠죠.']\n",
      "['부모님이 기뻐하실 생각을 하니까 너무 신난다.', '부모님께 알려드릴 생각하니 기쁘시군요.']\n",
      "['4개월째.', '이별 중인가봐요.']\n",
      "['친구들한테 털어놓으니까 한결 편해졌어.', '주변에 좋은 친구들을 둬서 다행이에요!']\n",
      "['헤어지고 나니 많은 생각이 들어', '정리할 시간이 필요하지요.']\n",
      "['이별후 3주 아니 차인후 3주', '이별을 받아들일 시간이 필요했을텐데.']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 15  # 고려할 문장의 최대 길이\n",
    "\n",
    "# 유니코드 문자열을 아스키로 변환합니다\n",
    "# https://stackoverflow.com/a/518232/2809427 참고\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다\n",
    "def normalizeString(s):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣 ^☆; ^a-zA-Z.!?;0-9]+')\n",
    "    result = hangul.sub('', s)\n",
    "    return result\n",
    "\n",
    "# 질의/응답 쌍을 읽어서 voc 객체를 반환합니다\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # 파일을 읽고, 쪼개어 lines에 저장합니다\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 각 줄을 쪼개어 pairs에 저장하고 정규화합니다\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# 문장의 쌍 'p'에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다\n",
    "def filterPair(p):\n",
    "    # EOS 토큰을 위해 입력 시퀀스의 마지막 단어를 보존해야 합니다\n",
    "    return len(tokenizer.morphs(p[0])) < MAX_LENGTH and len(tokenizer.morphs(p[1])) < MAX_LENGTH\n",
    "\n",
    "# 조건식 filterPair에 따라 pairs를 필터링합니다\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 앞에서 정의한 함수를 이용하여 만든 voc 객체와 리스트 pairs를 반환합니다\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# voc와 pairs를 읽고 재구성합니다\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# 검증을 위해 pairs의 일부 내용을 출력해 봅니다\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "keep_words 4202 / 5966 = 0.7043\n",
      "Trimmed from 13457 pairs to 11914, 0.8853 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2    # 제외할 단어의 기준이 되는 등장 횟수\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # MIN_COUNT 미만으로 사용된 단어는 voc에서 제외합니다\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # 제외할 단어가 포함된 경우를 pairs에서도 제외합니다\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # 입력 문장을 검사합니다\n",
    "        for word in tokenizer.morphs(input_sentence):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # 출력 문장을 검사합니다\n",
    "        for word in tokenizer.morphs(output_sentence):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # 입출력 문장에 제외하기로 한 단어를 포함하지 않는 경우만을 남겨둡니다\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# voc와 pairs를 정돈합니다\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in tokenizer.morphs(sentence)] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 입력 시퀀스 텐서에 패딩한 결과와 lengths를 반환합니다\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# 패딩한 목표 시퀀스 텐서, 패딩 마스크, 그리고 최대 목표 길이를 반환합니다\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# 입력 배치를 이루는 쌍에 대한 모든 아이템을 반환합니다\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # GRU를 초기화합니다. input_size와 hidden_size 패러미터는 둘 다 'hidden_size'로\n",
    "        # 둡니다. 이는 우리 입력의 크기가 hideen_size 만큼의 피처를 갖는 단어 임베딩이기\n",
    "        # 때문입니다.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 단어 인덱스를 임베딩으로 변환합니다\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        # GRU로 포워드 패스를 수행합니다\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # 패딩을 언패킹합니다\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 양방향 GRU의 출력을 합산합니다\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # 출력과 마지막 은닉 상태를 반환합니다\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong 어텐션 레이어\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Attention 가중치(에너지)를 제안된 방법에 따라 계산합니다\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # max_length와 batch_size의 차원을 뒤집습니다\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # 정규화된 softmax 확률 점수를 반환합니다 (차원을 늘려서)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 참조를 보존해 둡니다\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 레이어를 정의합니다\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n",
    "        # 현재의 입력 단어에 대한 임베딩을 구합니다\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # 무방향 GRU로 포워드 패스를 수행합니다\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # 인코더 출력에 어텐션을 곱하여 새로운 \"가중치 합\" 문백 벡터를 구합니다\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Luong의 논문에 나온 식 5를 이용하여 가중치 문백 벡터와 GRU 출력을 결합합니다\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Luong의 논문에 나온 식 6을 이용하여 다음 단어를 예측합니다\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # 출력과 마지막 은닉 상태를 반환합니다\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # 제로 그라디언트\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # device 옵션을 설정합니다\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # 변수를 초기화합니다\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # 인코더로 포워드 패스를 수행합니다\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # 초기 디코더 입력을 생성합니다(각 문장을 SOS 도큰으로 시작합니다)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로 둡니다\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # 이번 반복에서 teacher forcing을 사용할지를 결정합니다\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # 손실을 계산하고 누적합니다\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # 손실을 계산하고 누적합니다\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # 역전파를 수행합니다\n",
    "    loss.backward()\n",
    "\n",
    "    # 그라디언트 클리핑: 그라디언트를 제자리에서 수정합니다\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # 모델의 가중치를 수정합니다\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 각 단계에 대한 배치를 읽어옵니다\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # 초기화\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # 학습 루프\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # 배치에서 각 필드를 읽어옵니다\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # 배치에 대해 학습을 한 단계 진행합니다\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # 경과를 출력합니다\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Checkpoint를 저장합니다\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # 인코더 모델로 입력을 포워드 패스합니다\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # 인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # 디코더의 첫 번째 입력을 SOS_token으로 초기화합니다\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # 반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다\n",
    "        for _ in range(max_length):\n",
    "            # 디코더로의 포워드 패스를 수행합니다\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # 토큰과 점수를 기록합니다\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # 단어 토큰과 점수를 모아서 반환합니다\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### 입력 시퀀스를 배치 형태로 만듭니다\n",
    "    # 단어 -> 인덱스\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # lengths 텐서를 만듭니다\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # 적절한 디바이스를 사용합니다\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # searcher를 이용하여 문장을 디코딩합니다\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # 인덱스 -> 단어\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 입력 문장을 받아옵니다\n",
    "            input_sentence = input('> ')\n",
    "            # 종료 조건인지 검사합니다\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # 문장을 정규화합니다\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # 문장을 평가합니다\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 응답 문장을 형식에 맞춰 출력합니다\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building encoder and decoder ...\nModels built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# 모델을 설정합니다\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# 불러올 checkpoint를 설정합니다. 처음부터 시작할 때는 None으로 둡니다.\n",
    "loadFilename = None\n",
    "checkpoint_iter = 5000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# loadFilename이 제공되는 경우에는 모델을 불러옵니다\n",
    "if loadFilename:\n",
    "    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# 단어 임베딩을 초기화합니다\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# 인코더 및 디코더 모델을 초기화합니다\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# 적절한 디바이스를 사용합니다\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".7%; Average loss: 0.1146\n",
      "Iteration: 4684; Percent complete: 93.7%; Average loss: 0.1081\n",
      "Iteration: 4685; Percent complete: 93.7%; Average loss: 0.1143\n",
      "Iteration: 4686; Percent complete: 93.7%; Average loss: 0.0770\n",
      "Iteration: 4687; Percent complete: 93.7%; Average loss: 0.0818\n",
      "Iteration: 4688; Percent complete: 93.8%; Average loss: 0.0924\n",
      "Iteration: 4689; Percent complete: 93.8%; Average loss: 0.1181\n",
      "Iteration: 4690; Percent complete: 93.8%; Average loss: 0.0971\n",
      "Iteration: 4691; Percent complete: 93.8%; Average loss: 0.0845\n",
      "Iteration: 4692; Percent complete: 93.8%; Average loss: 0.0758\n",
      "Iteration: 4693; Percent complete: 93.9%; Average loss: 0.0998\n",
      "Iteration: 4694; Percent complete: 93.9%; Average loss: 0.0762\n",
      "Iteration: 4695; Percent complete: 93.9%; Average loss: 0.1206\n",
      "Iteration: 4696; Percent complete: 93.9%; Average loss: 0.1196\n",
      "Iteration: 4697; Percent complete: 93.9%; Average loss: 0.0972\n",
      "Iteration: 4698; Percent complete: 94.0%; Average loss: 0.1198\n",
      "Iteration: 4699; Percent complete: 94.0%; Average loss: 0.0859\n",
      "Iteration: 4700; Percent complete: 94.0%; Average loss: 0.1069\n",
      "Iteration: 4701; Percent complete: 94.0%; Average loss: 0.1042\n",
      "Iteration: 4702; Percent complete: 94.0%; Average loss: 0.1077\n",
      "Iteration: 4703; Percent complete: 94.1%; Average loss: 0.1098\n",
      "Iteration: 4704; Percent complete: 94.1%; Average loss: 0.1213\n",
      "Iteration: 4705; Percent complete: 94.1%; Average loss: 0.1086\n",
      "Iteration: 4706; Percent complete: 94.1%; Average loss: 0.0793\n",
      "Iteration: 4707; Percent complete: 94.1%; Average loss: 0.0688\n",
      "Iteration: 4708; Percent complete: 94.2%; Average loss: 0.0998\n",
      "Iteration: 4709; Percent complete: 94.2%; Average loss: 0.0702\n",
      "Iteration: 4710; Percent complete: 94.2%; Average loss: 0.0907\n",
      "Iteration: 4711; Percent complete: 94.2%; Average loss: 0.1032\n",
      "Iteration: 4712; Percent complete: 94.2%; Average loss: 0.1131\n",
      "Iteration: 4713; Percent complete: 94.3%; Average loss: 0.0728\n",
      "Iteration: 4714; Percent complete: 94.3%; Average loss: 0.0989\n",
      "Iteration: 4715; Percent complete: 94.3%; Average loss: 0.0768\n",
      "Iteration: 4716; Percent complete: 94.3%; Average loss: 0.0661\n",
      "Iteration: 4717; Percent complete: 94.3%; Average loss: 0.0900\n",
      "Iteration: 4718; Percent complete: 94.4%; Average loss: 0.1289\n",
      "Iteration: 4719; Percent complete: 94.4%; Average loss: 0.1417\n",
      "Iteration: 4720; Percent complete: 94.4%; Average loss: 0.0829\n",
      "Iteration: 4721; Percent complete: 94.4%; Average loss: 0.0590\n",
      "Iteration: 4722; Percent complete: 94.4%; Average loss: 0.0820\n",
      "Iteration: 4723; Percent complete: 94.5%; Average loss: 0.0815\n",
      "Iteration: 4724; Percent complete: 94.5%; Average loss: 0.1063\n",
      "Iteration: 4725; Percent complete: 94.5%; Average loss: 0.0630\n",
      "Iteration: 4726; Percent complete: 94.5%; Average loss: 0.1043\n",
      "Iteration: 4727; Percent complete: 94.5%; Average loss: 0.0835\n",
      "Iteration: 4728; Percent complete: 94.6%; Average loss: 0.0896\n",
      "Iteration: 4729; Percent complete: 94.6%; Average loss: 0.0918\n",
      "Iteration: 4730; Percent complete: 94.6%; Average loss: 0.0788\n",
      "Iteration: 4731; Percent complete: 94.6%; Average loss: 0.1086\n",
      "Iteration: 4732; Percent complete: 94.6%; Average loss: 0.0966\n",
      "Iteration: 4733; Percent complete: 94.7%; Average loss: 0.0919\n",
      "Iteration: 4734; Percent complete: 94.7%; Average loss: 0.0806\n",
      "Iteration: 4735; Percent complete: 94.7%; Average loss: 0.0684\n",
      "Iteration: 4736; Percent complete: 94.7%; Average loss: 0.1288\n",
      "Iteration: 4737; Percent complete: 94.7%; Average loss: 0.0793\n",
      "Iteration: 4738; Percent complete: 94.8%; Average loss: 0.0764\n",
      "Iteration: 4739; Percent complete: 94.8%; Average loss: 0.0814\n",
      "Iteration: 4740; Percent complete: 94.8%; Average loss: 0.0993\n",
      "Iteration: 4741; Percent complete: 94.8%; Average loss: 0.1203\n",
      "Iteration: 4742; Percent complete: 94.8%; Average loss: 0.1256\n",
      "Iteration: 4743; Percent complete: 94.9%; Average loss: 0.0897\n",
      "Iteration: 4744; Percent complete: 94.9%; Average loss: 0.0922\n",
      "Iteration: 4745; Percent complete: 94.9%; Average loss: 0.0892\n",
      "Iteration: 4746; Percent complete: 94.9%; Average loss: 0.0896\n",
      "Iteration: 4747; Percent complete: 94.9%; Average loss: 0.0816\n",
      "Iteration: 4748; Percent complete: 95.0%; Average loss: 0.0840\n",
      "Iteration: 4749; Percent complete: 95.0%; Average loss: 0.0957\n",
      "Iteration: 4750; Percent complete: 95.0%; Average loss: 0.0915\n",
      "Iteration: 4751; Percent complete: 95.0%; Average loss: 0.0902\n",
      "Iteration: 4752; Percent complete: 95.0%; Average loss: 0.1181\n",
      "Iteration: 4753; Percent complete: 95.1%; Average loss: 0.0765\n",
      "Iteration: 4754; Percent complete: 95.1%; Average loss: 0.0715\n",
      "Iteration: 4755; Percent complete: 95.1%; Average loss: 0.0941\n",
      "Iteration: 4756; Percent complete: 95.1%; Average loss: 0.0943\n",
      "Iteration: 4757; Percent complete: 95.1%; Average loss: 0.0718\n",
      "Iteration: 4758; Percent complete: 95.2%; Average loss: 0.0688\n",
      "Iteration: 4759; Percent complete: 95.2%; Average loss: 0.0780\n",
      "Iteration: 4760; Percent complete: 95.2%; Average loss: 0.0832\n",
      "Iteration: 4761; Percent complete: 95.2%; Average loss: 0.0658\n",
      "Iteration: 4762; Percent complete: 95.2%; Average loss: 0.0974\n",
      "Iteration: 4763; Percent complete: 95.3%; Average loss: 0.0871\n",
      "Iteration: 4764; Percent complete: 95.3%; Average loss: 0.0834\n",
      "Iteration: 4765; Percent complete: 95.3%; Average loss: 0.0873\n",
      "Iteration: 4766; Percent complete: 95.3%; Average loss: 0.0996\n",
      "Iteration: 4767; Percent complete: 95.3%; Average loss: 0.0906\n",
      "Iteration: 4768; Percent complete: 95.4%; Average loss: 0.0793\n",
      "Iteration: 4769; Percent complete: 95.4%; Average loss: 0.0780\n",
      "Iteration: 4770; Percent complete: 95.4%; Average loss: 0.0944\n",
      "Iteration: 4771; Percent complete: 95.4%; Average loss: 0.1135\n",
      "Iteration: 4772; Percent complete: 95.4%; Average loss: 0.0865\n",
      "Iteration: 4773; Percent complete: 95.5%; Average loss: 0.0757\n",
      "Iteration: 4774; Percent complete: 95.5%; Average loss: 0.0894\n",
      "Iteration: 4775; Percent complete: 95.5%; Average loss: 0.0877\n",
      "Iteration: 4776; Percent complete: 95.5%; Average loss: 0.0513\n",
      "Iteration: 4777; Percent complete: 95.5%; Average loss: 0.0961\n",
      "Iteration: 4778; Percent complete: 95.6%; Average loss: 0.1007\n",
      "Iteration: 4779; Percent complete: 95.6%; Average loss: 0.0884\n",
      "Iteration: 4780; Percent complete: 95.6%; Average loss: 0.0881\n",
      "Iteration: 4781; Percent complete: 95.6%; Average loss: 0.0836\n",
      "Iteration: 4782; Percent complete: 95.6%; Average loss: 0.1063\n",
      "Iteration: 4783; Percent complete: 95.7%; Average loss: 0.1158\n",
      "Iteration: 4784; Percent complete: 95.7%; Average loss: 0.0839\n",
      "Iteration: 4785; Percent complete: 95.7%; Average loss: 0.1062\n",
      "Iteration: 4786; Percent complete: 95.7%; Average loss: 0.0853\n",
      "Iteration: 4787; Percent complete: 95.7%; Average loss: 0.0864\n",
      "Iteration: 4788; Percent complete: 95.8%; Average loss: 0.0775\n",
      "Iteration: 4789; Percent complete: 95.8%; Average loss: 0.1058\n",
      "Iteration: 4790; Percent complete: 95.8%; Average loss: 0.1000\n",
      "Iteration: 4791; Percent complete: 95.8%; Average loss: 0.0983\n",
      "Iteration: 4792; Percent complete: 95.8%; Average loss: 0.0976\n",
      "Iteration: 4793; Percent complete: 95.9%; Average loss: 0.0716\n",
      "Iteration: 4794; Percent complete: 95.9%; Average loss: 0.0675\n",
      "Iteration: 4795; Percent complete: 95.9%; Average loss: 0.1132\n",
      "Iteration: 4796; Percent complete: 95.9%; Average loss: 0.0746\n",
      "Iteration: 4797; Percent complete: 95.9%; Average loss: 0.0763\n",
      "Iteration: 4798; Percent complete: 96.0%; Average loss: 0.1062\n",
      "Iteration: 4799; Percent complete: 96.0%; Average loss: 0.0787\n",
      "Iteration: 4800; Percent complete: 96.0%; Average loss: 0.0721\n",
      "Iteration: 4801; Percent complete: 96.0%; Average loss: 0.0915\n",
      "Iteration: 4802; Percent complete: 96.0%; Average loss: 0.0807\n",
      "Iteration: 4803; Percent complete: 96.1%; Average loss: 0.0735\n",
      "Iteration: 4804; Percent complete: 96.1%; Average loss: 0.0890\n",
      "Iteration: 4805; Percent complete: 96.1%; Average loss: 0.0783\n",
      "Iteration: 4806; Percent complete: 96.1%; Average loss: 0.1055\n",
      "Iteration: 4807; Percent complete: 96.1%; Average loss: 0.0840\n",
      "Iteration: 4808; Percent complete: 96.2%; Average loss: 0.0540\n",
      "Iteration: 4809; Percent complete: 96.2%; Average loss: 0.0960\n",
      "Iteration: 4810; Percent complete: 96.2%; Average loss: 0.0634\n",
      "Iteration: 4811; Percent complete: 96.2%; Average loss: 0.0744\n",
      "Iteration: 4812; Percent complete: 96.2%; Average loss: 0.1008\n",
      "Iteration: 4813; Percent complete: 96.3%; Average loss: 0.0783\n",
      "Iteration: 4814; Percent complete: 96.3%; Average loss: 0.0794\n",
      "Iteration: 4815; Percent complete: 96.3%; Average loss: 0.0754\n",
      "Iteration: 4816; Percent complete: 96.3%; Average loss: 0.0948\n",
      "Iteration: 4817; Percent complete: 96.3%; Average loss: 0.1113\n",
      "Iteration: 4818; Percent complete: 96.4%; Average loss: 0.0938\n",
      "Iteration: 4819; Percent complete: 96.4%; Average loss: 0.0955\n",
      "Iteration: 4820; Percent complete: 96.4%; Average loss: 0.0701\n",
      "Iteration: 4821; Percent complete: 96.4%; Average loss: 0.0702\n",
      "Iteration: 4822; Percent complete: 96.4%; Average loss: 0.0853\n",
      "Iteration: 4823; Percent complete: 96.5%; Average loss: 0.0826\n",
      "Iteration: 4824; Percent complete: 96.5%; Average loss: 0.0915\n",
      "Iteration: 4825; Percent complete: 96.5%; Average loss: 0.0816\n",
      "Iteration: 4826; Percent complete: 96.5%; Average loss: 0.0680\n",
      "Iteration: 4827; Percent complete: 96.5%; Average loss: 0.0886\n",
      "Iteration: 4828; Percent complete: 96.6%; Average loss: 0.0978\n",
      "Iteration: 4829; Percent complete: 96.6%; Average loss: 0.0674\n",
      "Iteration: 4830; Percent complete: 96.6%; Average loss: 0.0853\n",
      "Iteration: 4831; Percent complete: 96.6%; Average loss: 0.0736\n",
      "Iteration: 4832; Percent complete: 96.6%; Average loss: 0.0601\n",
      "Iteration: 4833; Percent complete: 96.7%; Average loss: 0.1036\n",
      "Iteration: 4834; Percent complete: 96.7%; Average loss: 0.0877\n",
      "Iteration: 4835; Percent complete: 96.7%; Average loss: 0.0818\n",
      "Iteration: 4836; Percent complete: 96.7%; Average loss: 0.1030\n",
      "Iteration: 4837; Percent complete: 96.7%; Average loss: 0.0708\n",
      "Iteration: 4838; Percent complete: 96.8%; Average loss: 0.0790\n",
      "Iteration: 4839; Percent complete: 96.8%; Average loss: 0.0776\n",
      "Iteration: 4840; Percent complete: 96.8%; Average loss: 0.0888\n",
      "Iteration: 4841; Percent complete: 96.8%; Average loss: 0.0612\n",
      "Iteration: 4842; Percent complete: 96.8%; Average loss: 0.0797\n",
      "Iteration: 4843; Percent complete: 96.9%; Average loss: 0.0713\n",
      "Iteration: 4844; Percent complete: 96.9%; Average loss: 0.0666\n",
      "Iteration: 4845; Percent complete: 96.9%; Average loss: 0.1010\n",
      "Iteration: 4846; Percent complete: 96.9%; Average loss: 0.0778\n",
      "Iteration: 4847; Percent complete: 96.9%; Average loss: 0.1011\n",
      "Iteration: 4848; Percent complete: 97.0%; Average loss: 0.0812\n",
      "Iteration: 4849; Percent complete: 97.0%; Average loss: 0.0885\n",
      "Iteration: 4850; Percent complete: 97.0%; Average loss: 0.0699\n",
      "Iteration: 4851; Percent complete: 97.0%; Average loss: 0.0752\n",
      "Iteration: 4852; Percent complete: 97.0%; Average loss: 0.0866\n",
      "Iteration: 4853; Percent complete: 97.1%; Average loss: 0.1009\n",
      "Iteration: 4854; Percent complete: 97.1%; Average loss: 0.1033\n",
      "Iteration: 4855; Percent complete: 97.1%; Average loss: 0.0571\n",
      "Iteration: 4856; Percent complete: 97.1%; Average loss: 0.0596\n",
      "Iteration: 4857; Percent complete: 97.1%; Average loss: 0.0882\n",
      "Iteration: 4858; Percent complete: 97.2%; Average loss: 0.0803\n",
      "Iteration: 4859; Percent complete: 97.2%; Average loss: 0.0724\n",
      "Iteration: 4860; Percent complete: 97.2%; Average loss: 0.0758\n",
      "Iteration: 4861; Percent complete: 97.2%; Average loss: 0.0758\n",
      "Iteration: 4862; Percent complete: 97.2%; Average loss: 0.0903\n",
      "Iteration: 4863; Percent complete: 97.3%; Average loss: 0.0574\n",
      "Iteration: 4864; Percent complete: 97.3%; Average loss: 0.1003\n",
      "Iteration: 4865; Percent complete: 97.3%; Average loss: 0.0730\n",
      "Iteration: 4866; Percent complete: 97.3%; Average loss: 0.0855\n",
      "Iteration: 4867; Percent complete: 97.3%; Average loss: 0.0883\n",
      "Iteration: 4868; Percent complete: 97.4%; Average loss: 0.0989\n",
      "Iteration: 4869; Percent complete: 97.4%; Average loss: 0.1052\n",
      "Iteration: 4870; Percent complete: 97.4%; Average loss: 0.0853\n",
      "Iteration: 4871; Percent complete: 97.4%; Average loss: 0.0802\n",
      "Iteration: 4872; Percent complete: 97.4%; Average loss: 0.1139\n",
      "Iteration: 4873; Percent complete: 97.5%; Average loss: 0.0807\n",
      "Iteration: 4874; Percent complete: 97.5%; Average loss: 0.0917\n",
      "Iteration: 4875; Percent complete: 97.5%; Average loss: 0.0957\n",
      "Iteration: 4876; Percent complete: 97.5%; Average loss: 0.0699\n",
      "Iteration: 4877; Percent complete: 97.5%; Average loss: 0.0879\n",
      "Iteration: 4878; Percent complete: 97.6%; Average loss: 0.0642\n",
      "Iteration: 4879; Percent complete: 97.6%; Average loss: 0.0873\n",
      "Iteration: 4880; Percent complete: 97.6%; Average loss: 0.0977\n",
      "Iteration: 4881; Percent complete: 97.6%; Average loss: 0.0671\n",
      "Iteration: 4882; Percent complete: 97.6%; Average loss: 0.0815\n",
      "Iteration: 4883; Percent complete: 97.7%; Average loss: 0.0817\n",
      "Iteration: 4884; Percent complete: 97.7%; Average loss: 0.0914\n",
      "Iteration: 4885; Percent complete: 97.7%; Average loss: 0.0602\n",
      "Iteration: 4886; Percent complete: 97.7%; Average loss: 0.0966\n",
      "Iteration: 4887; Percent complete: 97.7%; Average loss: 0.0660\n",
      "Iteration: 4888; Percent complete: 97.8%; Average loss: 0.0862\n",
      "Iteration: 4889; Percent complete: 97.8%; Average loss: 0.1028\n",
      "Iteration: 4890; Percent complete: 97.8%; Average loss: 0.0886\n",
      "Iteration: 4891; Percent complete: 97.8%; Average loss: 0.0712\n",
      "Iteration: 4892; Percent complete: 97.8%; Average loss: 0.0717\n",
      "Iteration: 4893; Percent complete: 97.9%; Average loss: 0.0766\n",
      "Iteration: 4894; Percent complete: 97.9%; Average loss: 0.0748\n",
      "Iteration: 4895; Percent complete: 97.9%; Average loss: 0.0509\n",
      "Iteration: 4896; Percent complete: 97.9%; Average loss: 0.0749\n",
      "Iteration: 4897; Percent complete: 97.9%; Average loss: 0.0640\n",
      "Iteration: 4898; Percent complete: 98.0%; Average loss: 0.0634\n",
      "Iteration: 4899; Percent complete: 98.0%; Average loss: 0.0687\n",
      "Iteration: 4900; Percent complete: 98.0%; Average loss: 0.0766\n",
      "Iteration: 4901; Percent complete: 98.0%; Average loss: 0.0935\n",
      "Iteration: 4902; Percent complete: 98.0%; Average loss: 0.0688\n",
      "Iteration: 4903; Percent complete: 98.1%; Average loss: 0.0758\n",
      "Iteration: 4904; Percent complete: 98.1%; Average loss: 0.0758\n",
      "Iteration: 4905; Percent complete: 98.1%; Average loss: 0.0748\n",
      "Iteration: 4906; Percent complete: 98.1%; Average loss: 0.0902\n",
      "Iteration: 4907; Percent complete: 98.1%; Average loss: 0.0998\n",
      "Iteration: 4908; Percent complete: 98.2%; Average loss: 0.0851\n",
      "Iteration: 4909; Percent complete: 98.2%; Average loss: 0.0703\n",
      "Iteration: 4910; Percent complete: 98.2%; Average loss: 0.1010\n",
      "Iteration: 4911; Percent complete: 98.2%; Average loss: 0.0822\n",
      "Iteration: 4912; Percent complete: 98.2%; Average loss: 0.0990\n",
      "Iteration: 4913; Percent complete: 98.3%; Average loss: 0.1033\n",
      "Iteration: 4914; Percent complete: 98.3%; Average loss: 0.0781\n",
      "Iteration: 4915; Percent complete: 98.3%; Average loss: 0.0745\n",
      "Iteration: 4916; Percent complete: 98.3%; Average loss: 0.0824\n",
      "Iteration: 4917; Percent complete: 98.3%; Average loss: 0.0780\n",
      "Iteration: 4918; Percent complete: 98.4%; Average loss: 0.0509\n",
      "Iteration: 4919; Percent complete: 98.4%; Average loss: 0.0876\n",
      "Iteration: 4920; Percent complete: 98.4%; Average loss: 0.0625\n",
      "Iteration: 4921; Percent complete: 98.4%; Average loss: 0.1014\n",
      "Iteration: 4922; Percent complete: 98.4%; Average loss: 0.0893\n",
      "Iteration: 4923; Percent complete: 98.5%; Average loss: 0.0802\n",
      "Iteration: 4924; Percent complete: 98.5%; Average loss: 0.0563\n",
      "Iteration: 4925; Percent complete: 98.5%; Average loss: 0.0634\n",
      "Iteration: 4926; Percent complete: 98.5%; Average loss: 0.0999\n",
      "Iteration: 4927; Percent complete: 98.5%; Average loss: 0.0808\n",
      "Iteration: 4928; Percent complete: 98.6%; Average loss: 0.1146\n",
      "Iteration: 4929; Percent complete: 98.6%; Average loss: 0.0676\n",
      "Iteration: 4930; Percent complete: 98.6%; Average loss: 0.1064\n",
      "Iteration: 4931; Percent complete: 98.6%; Average loss: 0.1137\n",
      "Iteration: 4932; Percent complete: 98.6%; Average loss: 0.0993\n",
      "Iteration: 4933; Percent complete: 98.7%; Average loss: 0.1083\n",
      "Iteration: 4934; Percent complete: 98.7%; Average loss: 0.0604\n",
      "Iteration: 4935; Percent complete: 98.7%; Average loss: 0.0665\n",
      "Iteration: 4936; Percent complete: 98.7%; Average loss: 0.0628\n",
      "Iteration: 4937; Percent complete: 98.7%; Average loss: 0.0903\n",
      "Iteration: 4938; Percent complete: 98.8%; Average loss: 0.0859\n",
      "Iteration: 4939; Percent complete: 98.8%; Average loss: 0.1122\n",
      "Iteration: 4940; Percent complete: 98.8%; Average loss: 0.0632\n",
      "Iteration: 4941; Percent complete: 98.8%; Average loss: 0.0805\n",
      "Iteration: 4942; Percent complete: 98.8%; Average loss: 0.0883\n",
      "Iteration: 4943; Percent complete: 98.9%; Average loss: 0.0828\n",
      "Iteration: 4944; Percent complete: 98.9%; Average loss: 0.0759\n",
      "Iteration: 4945; Percent complete: 98.9%; Average loss: 0.0862\n",
      "Iteration: 4946; Percent complete: 98.9%; Average loss: 0.1218\n",
      "Iteration: 4947; Percent complete: 98.9%; Average loss: 0.1089\n",
      "Iteration: 4948; Percent complete: 99.0%; Average loss: 0.1030\n",
      "Iteration: 4949; Percent complete: 99.0%; Average loss: 0.0778\n",
      "Iteration: 4950; Percent complete: 99.0%; Average loss: 0.0620\n",
      "Iteration: 4951; Percent complete: 99.0%; Average loss: 0.1113\n",
      "Iteration: 4952; Percent complete: 99.0%; Average loss: 0.0743\n",
      "Iteration: 4953; Percent complete: 99.1%; Average loss: 0.0970\n",
      "Iteration: 4954; Percent complete: 99.1%; Average loss: 0.0902\n",
      "Iteration: 4955; Percent complete: 99.1%; Average loss: 0.0468\n",
      "Iteration: 4956; Percent complete: 99.1%; Average loss: 0.0996\n",
      "Iteration: 4957; Percent complete: 99.1%; Average loss: 0.0980\n",
      "Iteration: 4958; Percent complete: 99.2%; Average loss: 0.0873\n",
      "Iteration: 4959; Percent complete: 99.2%; Average loss: 0.1019\n",
      "Iteration: 4960; Percent complete: 99.2%; Average loss: 0.1003\n",
      "Iteration: 4961; Percent complete: 99.2%; Average loss: 0.0803\n",
      "Iteration: 4962; Percent complete: 99.2%; Average loss: 0.0613\n",
      "Iteration: 4963; Percent complete: 99.3%; Average loss: 0.0582\n",
      "Iteration: 4964; Percent complete: 99.3%; Average loss: 0.0831\n",
      "Iteration: 4965; Percent complete: 99.3%; Average loss: 0.0741\n",
      "Iteration: 4966; Percent complete: 99.3%; Average loss: 0.0647\n",
      "Iteration: 4967; Percent complete: 99.3%; Average loss: 0.0866\n",
      "Iteration: 4968; Percent complete: 99.4%; Average loss: 0.0743\n",
      "Iteration: 4969; Percent complete: 99.4%; Average loss: 0.0892\n",
      "Iteration: 4970; Percent complete: 99.4%; Average loss: 0.0869\n",
      "Iteration: 4971; Percent complete: 99.4%; Average loss: 0.0824\n",
      "Iteration: 4972; Percent complete: 99.4%; Average loss: 0.0536\n",
      "Iteration: 4973; Percent complete: 99.5%; Average loss: 0.0877\n",
      "Iteration: 4974; Percent complete: 99.5%; Average loss: 0.0948\n",
      "Iteration: 4975; Percent complete: 99.5%; Average loss: 0.0871\n",
      "Iteration: 4976; Percent complete: 99.5%; Average loss: 0.0922\n",
      "Iteration: 4977; Percent complete: 99.5%; Average loss: 0.0786\n",
      "Iteration: 4978; Percent complete: 99.6%; Average loss: 0.0722\n",
      "Iteration: 4979; Percent complete: 99.6%; Average loss: 0.0706\n",
      "Iteration: 4980; Percent complete: 99.6%; Average loss: 0.0732\n",
      "Iteration: 4981; Percent complete: 99.6%; Average loss: 0.0865\n",
      "Iteration: 4982; Percent complete: 99.6%; Average loss: 0.1231\n",
      "Iteration: 4983; Percent complete: 99.7%; Average loss: 0.1035\n",
      "Iteration: 4984; Percent complete: 99.7%; Average loss: 0.1155\n",
      "Iteration: 4985; Percent complete: 99.7%; Average loss: 0.0517\n",
      "Iteration: 4986; Percent complete: 99.7%; Average loss: 0.0739\n",
      "Iteration: 4987; Percent complete: 99.7%; Average loss: 0.0743\n",
      "Iteration: 4988; Percent complete: 99.8%; Average loss: 0.0843\n",
      "Iteration: 4989; Percent complete: 99.8%; Average loss: 0.0861\n",
      "Iteration: 4990; Percent complete: 99.8%; Average loss: 0.0747\n",
      "Iteration: 4991; Percent complete: 99.8%; Average loss: 0.0993\n",
      "Iteration: 4992; Percent complete: 99.8%; Average loss: 0.0822\n",
      "Iteration: 4993; Percent complete: 99.9%; Average loss: 0.0610\n",
      "Iteration: 4994; Percent complete: 99.9%; Average loss: 0.0894\n",
      "Iteration: 4995; Percent complete: 99.9%; Average loss: 0.0719\n",
      "Iteration: 4996; Percent complete: 99.9%; Average loss: 0.0695\n",
      "Iteration: 4997; Percent complete: 99.9%; Average loss: 0.0976\n",
      "Iteration: 4998; Percent complete: 100.0%; Average loss: 0.0636\n",
      "Iteration: 4999; Percent complete: 100.0%; Average loss: 0.0758\n",
      "Iteration: 5000; Percent complete: 100.0%; Average loss: 0.0758\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 최적화 설정\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 5000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Dropout 레이어를 학습 모드로 둡니다\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Optimizer를 초기화합니다\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# cuda가 있다면 cuda를 설정합니다\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "    \n",
    "# 학습 단계를 수행합니다\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: 불안 하 니까 너무 힘들 시 겠 어요 . 이 에요 .\n",
      "Bot: 우산 만나 지 말 아 시 어요 . 이 에요 .\n",
      "Bot: 컴 패 니어 ㄴ 이 ㅂ니다 . 하 ㅂ니다 .\n",
      "Error: Encountered unknown word.\n",
      "Bot: 제가 채우 어 주 ㄹ게요 . 이 에요 . 하 시\n"
     ]
    }
   ],
   "source": [
    "# Dropout 레이어를 평가 모드로 설정합니다\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 탐색 모듈을 초기화합니다\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# 채팅을 시작합니다 (다음 줄의 주석을 제거하면 시작해볼 수 있습니다)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}