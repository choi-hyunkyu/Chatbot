{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from konlpy.tag import Komoran; tokenizer = Komoran()\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "곧 만기 은퇴를 앞두고 있어. 노후 준비를 미리 꼼꼼하게 해두어서 기뻐.\t노후 준비를 미리 꼼꼼하게 해두어서 기쁘시겠군요.\n",
      "\n",
      "이렇게 노후 준비를 할 수 있는 건 옆에서 도와준 친구 덕분이야. 이 고마움을 뭐로 갚지?\t노후 준비를 도운 친구에게 고마움을 느끼시고 있군요. 상대방에게 고마움을 표현할 수 있는 좋은 방법이 있을까요?\n",
      "\n",
      "그 친구가 술을 좋아하니까 조만간 술을 들고 친구 집에 찾아가야겠어.\t선물로 친구에게 고마움을 표현할 수 있으면 좋겠어요.\n",
      "\n",
      "친구가 노후 자금 마련을 도와줘서 노후 걱정이 없어. 마음이 아주 든든해.\t노후자금 마련을 도와준 친구 덕분에 노후 걱정이 없어 기쁘시겠군요.\n",
      "\n",
      "다 그 친구가 주식 정보를 물어다 준 덕분이지. 이 고마움을 뭐로 표현하지?\t주식 정보를 알려준 친구에게 고마움을 느끼시고 있군요. 이 고마움을 무엇으로 표현할 수 있을까요?\n",
      "\n",
      "친구에게 내가 잘 된 건 다 너 덕분이라고 고맙다는 문자 한 통 넣어야겠어.\t문자로 친구에게 고마움을 표현할 수 있으면 좋겠어요.\n",
      "\n",
      "회사에 다시 복직하게 돼서 기쁘네. 부당 해고 당할까 봐 어찌나 맘 졸였는지.\t부당하게 해고당할 뻔 한 회사에 다시 복직하게 돼서 기쁘시겠군요.\n",
      "\n",
      "이건 다 처음 해고당했을 때 같이 목숨 걸고 싸워준 우리 노조 위원장 덕분이야.\t처음 해고당했을 때 함께 싸워준 분께 고마워하시고 있군요. 어떻게 이 고마운 마음을 표현할 수 있을까요?\n",
      "\n",
      "조만간 제대로 인사차 노조 사무실에 들러야겠어. 복직하게 도와줘서 고맙다고 해야지.\t직접 방문해 인사를 드려서 고마움이 표현되면 좋겠어요.\n",
      "\n",
      "회사에서 정년퇴직 후에 대리점 사장을 하면 어떻겠냐고 물어 보더군. 기분이 참 좋아.\t정년퇴직 후에 대리점 사장을 제안해서 기분이 좋으시겠어요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"normal chatbot data\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "textfilename = \"speech_data.txt\"\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'r', encoding = 'utf-8') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, textfilename))\n",
    "datafile = os.path.join(corpus, textfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 단어 토큰 값\n",
    "PAD_token = 0  # 짧은 문장을 채울(패딩, PADding) 때 사용할 제로 토큰\n",
    "SOS_token = 1  # 문장의 시작(SOS, Start Of Sentence)을 나타내는 토큰\n",
    "EOS_token = 2  # 문장의 끝(EOS, End Of Sentence)을 나태는 토큰\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # SOS, EOS, PAD를 센 것\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in tokenizer.morphs(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # 등장 횟수가 기준 이하인 단어를 정리합니다\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # 사전을 다시 초기화힙니다\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # 기본 토큰을 센 것\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 27513 sentence pairs\n",
      "Trimmed to 2366 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 2653\n",
      "\n",
      "pairs:\n",
      "['감사하더라고. 좋지.', '감사한 마음을 어떻게 전하면 좋을까요?']\n",
      "['고맙다고 전화라도 많이 해야겠어.', '전화로 고마운 마음이 전달되면 좋겠어요.']\n",
      "['남편에게 편지를 적어봐야겠어.', '남편 분에게 마음이 잘 전달되면 좋겠어요.']\n",
      "['손은 떨리지만 아직 일을 더 하고 싶어.', '그렇게 하려면 어떻게 해야 할까요?']\n",
      "['아빠와 이야기를 좀 나눠봐야겠어.', '아버지와의 대화가 도움이 되길 바랄게요.']\n",
      "['박 씨를 위해서 병문안을 가려고 해.', '병문안으로 기쁜 감정이 계속 유지되길 바랄게요.']\n",
      "['지난번 기말고사보다 성적이 더 떨어져서 걱정이야.', '성적이 더 떨어지셨군요. 많이 속상하시겠어요.']\n",
      "['이놈 저놈 아주 내가 은행인줄 아는구먼!', '왜 화가 나셨나요? 무슨 일이 있었나요?']\n",
      "['이렇게 계속 약속을 안 지켜서 너무 속상해.', '아버지가 어떻게 하셨으면 좋으시겠어요?']\n",
      "['아니. 형은 계속 이유 없이 나를 괴롭혀.', '어떻게 하면 형이 그만 괴롭힐까요?']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 15  # 고려할 문장의 최대 길이\n",
    "\n",
    "# 유니코드 문자열을 아스키로 변환합니다\n",
    "# https://stackoverflow.com/a/518232/2809427 참고\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다\n",
    "def normalizeString(s):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣 ^☆; ^a-zA-Z.!?;0-9]+')\n",
    "    result = hangul.sub('', s)\n",
    "    return result\n",
    "\n",
    "# 질의/응답 쌍을 읽어서 voc 객체를 반환합니다\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # 파일을 읽고, 쪼개어 lines에 저장합니다\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 각 줄을 쪼개어 pairs에 저장하고 정규화합니다\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# 문장의 쌍 'p'에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다\n",
    "def filterPair(p):\n",
    "    # EOS 토큰을 위해 입력 시퀀스의 마지막 단어를 보존해야 합니다\n",
    "    return len(tokenizer.morphs(p[0])) < MAX_LENGTH and len(tokenizer.morphs(p[1])) < MAX_LENGTH\n",
    "\n",
    "# 조건식 filterPair에 따라 pairs를 필터링합니다\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 앞에서 정의한 함수를 이용하여 만든 voc 객체와 리스트 pairs를 반환합니다\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# voc와 pairs를 읽고 재구성합니다\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# 검증을 위해 pairs의 일부 내용을 출력해 봅니다\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 1611 / 2650 = 0.6079\n",
      "Trimmed from 2366 pairs to 1610, 0.6805 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2    # 제외할 단어의 기준이 되는 등장 횟수\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # MIN_COUNT 미만으로 사용된 단어는 voc에서 제외합니다\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # 제외할 단어가 포함된 경우를 pairs에서도 제외합니다\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # 입력 문장을 검사합니다\n",
    "        for word in tokenizer.morphs(input_sentence):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # 출력 문장을 검사합니다\n",
    "        for word in tokenizer.morphs(output_sentence):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # 입출력 문장에 제외하기로 한 단어를 포함하지 않는 경우만을 남겨둡니다\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# voc와 pairs를 정돈합니다\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in tokenizer.morphs(sentence)] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 입력 시퀀스 텐서에 패딩한 결과와 lengths를 반환합니다\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# 패딩한 목표 시퀀스 텐서, 패딩 마스크, 그리고 최대 목표 길이를 반환합니다\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# 입력 배치를 이루는 쌍에 대한 모든 아이템을 반환합니다\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # GRU를 초기화합니다. input_size와 hidden_size 패러미터는 둘 다 'hidden_size'로\n",
    "        # 둡니다. 이는 우리 입력의 크기가 hideen_size 만큼의 피처를 갖는 단어 임베딩이기\n",
    "        # 때문입니다.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 단어 인덱스를 임베딩으로 변환합니다\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        # GRU로 포워드 패스를 수행합니다\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # 패딩을 언패킹합니다\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 양방향 GRU의 출력을 합산합니다\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # 출력과 마지막 은닉 상태를 반환합니다\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong 어텐션 레이어\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Attention 가중치(에너지)를 제안된 방법에 따라 계산합니다\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # max_length와 batch_size의 차원을 뒤집습니다\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # 정규화된 softmax 확률 점수를 반환합니다 (차원을 늘려서)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 참조를 보존해 둡니다\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 레이어를 정의합니다\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n",
    "        # 현재의 입력 단어에 대한 임베딩을 구합니다\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # 무방향 GRU로 포워드 패스를 수행합니다\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # 인코더 출력에 어텐션을 곱하여 새로운 \"가중치 합\" 문백 벡터를 구합니다\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Luong의 논문에 나온 식 5를 이용하여 가중치 문백 벡터와 GRU 출력을 결합합니다\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Luong의 논문에 나온 식 6을 이용하여 다음 단어를 예측합니다\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # 출력과 마지막 은닉 상태를 반환합니다\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # 제로 그라디언트\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # device 옵션을 설정합니다\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # 변수를 초기화합니다\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # 인코더로 포워드 패스를 수행합니다\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # 초기 디코더 입력을 생성합니다(각 문장을 SOS 도큰으로 시작합니다)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로 둡니다\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # 이번 반복에서 teacher forcing을 사용할지를 결정합니다\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # 손실을 계산하고 누적합니다\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # 손실을 계산하고 누적합니다\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # 역전파를 수행합니다\n",
    "    loss.backward()\n",
    "\n",
    "    # 그라디언트 클리핑: 그라디언트를 제자리에서 수정합니다\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # 모델의 가중치를 수정합니다\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 각 단계에 대한 배치를 읽어옵니다\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # 초기화\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # 학습 루프\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # 배치에서 각 필드를 읽어옵니다\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # 배치에 대해 학습을 한 단계 진행합니다\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # 경과를 출력합니다\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Checkpoint를 저장합니다\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # 인코더 모델로 입력을 포워드 패스합니다\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # 인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # 디코더의 첫 번째 입력을 SOS_token으로 초기화합니다\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # 반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다\n",
    "        for _ in range(max_length):\n",
    "            # 디코더로의 포워드 패스를 수행합니다\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # 토큰과 점수를 기록합니다\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # 단어 토큰과 점수를 모아서 반환합니다\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### 입력 시퀀스를 배치 형태로 만듭니다\n",
    "    # 단어 -> 인덱스\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # lengths 텐서를 만듭니다\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # 적절한 디바이스를 사용합니다\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # searcher를 이용하여 문장을 디코딩합니다\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # 인덱스 -> 단어\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 입력 문장을 받아옵니다\n",
    "            input_sentence = input('> ')\n",
    "            # 종료 조건인지 검사합니다\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # 문장을 정규화합니다\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # 문장을 평가합니다\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 응답 문장을 형식에 맞춰 출력합니다\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# 모델을 설정합니다\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# 불러올 checkpoint를 설정합니다. 처음부터 시작할 때는 None으로 둡니다.\n",
    "loadFilename = None\n",
    "checkpoint_iter = 5000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# loadFilename이 제공되는 경우에는 모델을 불러옵니다\n",
    "if loadFilename:\n",
    "    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# 단어 임베딩을 초기화합니다\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# 인코더 및 디코더 모델을 초기화합니다\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# 적절한 디바이스를 사용합니다\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-f319a975feb5>:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:20.)\n",
      "  loss = crossEntropy.masked_select(mask).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1; Percent complete: 0.0%; Average loss: 7.3833\n",
      "Iteration: 2; Percent complete: 0.0%; Average loss: 7.2881\n",
      "Iteration: 3; Percent complete: 0.1%; Average loss: 7.1608\n",
      "Iteration: 4; Percent complete: 0.1%; Average loss: 6.9880\n",
      "Iteration: 5; Percent complete: 0.1%; Average loss: 6.6673\n",
      "Iteration: 6; Percent complete: 0.1%; Average loss: 6.3233\n",
      "Iteration: 7; Percent complete: 0.1%; Average loss: 6.0517\n",
      "Iteration: 8; Percent complete: 0.2%; Average loss: 6.0374\n",
      "Iteration: 9; Percent complete: 0.2%; Average loss: 5.7972\n",
      "Iteration: 10; Percent complete: 0.2%; Average loss: 5.4699\n",
      "Iteration: 11; Percent complete: 0.2%; Average loss: 5.1481\n",
      "Iteration: 12; Percent complete: 0.2%; Average loss: 4.9681\n",
      "Iteration: 13; Percent complete: 0.3%; Average loss: 5.0448\n",
      "Iteration: 14; Percent complete: 0.3%; Average loss: 4.8957\n",
      "Iteration: 15; Percent complete: 0.3%; Average loss: 4.8109\n",
      "Iteration: 16; Percent complete: 0.3%; Average loss: 4.7969\n",
      "Iteration: 17; Percent complete: 0.3%; Average loss: 4.7202\n",
      "Iteration: 18; Percent complete: 0.4%; Average loss: 4.6058\n",
      "Iteration: 19; Percent complete: 0.4%; Average loss: 4.5564\n",
      "Iteration: 20; Percent complete: 0.4%; Average loss: 4.4830\n",
      "Iteration: 21; Percent complete: 0.4%; Average loss: 4.4247\n",
      "Iteration: 22; Percent complete: 0.4%; Average loss: 4.5418\n",
      "Iteration: 23; Percent complete: 0.5%; Average loss: 4.2822\n",
      "Iteration: 24; Percent complete: 0.5%; Average loss: 4.4805\n",
      "Iteration: 25; Percent complete: 0.5%; Average loss: 4.4714\n",
      "Iteration: 26; Percent complete: 0.5%; Average loss: 4.3333\n",
      "Iteration: 27; Percent complete: 0.5%; Average loss: 4.2618\n",
      "Iteration: 28; Percent complete: 0.6%; Average loss: 4.1615\n",
      "Iteration: 29; Percent complete: 0.6%; Average loss: 4.3467\n",
      "Iteration: 30; Percent complete: 0.6%; Average loss: 4.1901\n",
      "Iteration: 31; Percent complete: 0.6%; Average loss: 4.0996\n",
      "Iteration: 32; Percent complete: 0.6%; Average loss: 4.1044\n",
      "Iteration: 33; Percent complete: 0.7%; Average loss: 4.2284\n",
      "Iteration: 34; Percent complete: 0.7%; Average loss: 4.1140\n",
      "Iteration: 35; Percent complete: 0.7%; Average loss: 3.9952\n",
      "Iteration: 36; Percent complete: 0.7%; Average loss: 4.0108\n",
      "Iteration: 37; Percent complete: 0.7%; Average loss: 3.9074\n",
      "Iteration: 38; Percent complete: 0.8%; Average loss: 4.0899\n",
      "Iteration: 39; Percent complete: 0.8%; Average loss: 4.0303\n",
      "Iteration: 40; Percent complete: 0.8%; Average loss: 3.9577\n",
      "Iteration: 41; Percent complete: 0.8%; Average loss: 3.9282\n",
      "Iteration: 42; Percent complete: 0.8%; Average loss: 4.0159\n",
      "Iteration: 43; Percent complete: 0.9%; Average loss: 3.7547\n",
      "Iteration: 44; Percent complete: 0.9%; Average loss: 3.6822\n",
      "Iteration: 45; Percent complete: 0.9%; Average loss: 3.6448\n",
      "Iteration: 46; Percent complete: 0.9%; Average loss: 3.7392\n",
      "Iteration: 47; Percent complete: 0.9%; Average loss: 3.7934\n",
      "Iteration: 48; Percent complete: 1.0%; Average loss: 3.8961\n",
      "Iteration: 49; Percent complete: 1.0%; Average loss: 3.6179\n",
      "Iteration: 50; Percent complete: 1.0%; Average loss: 3.8989\n",
      "Iteration: 51; Percent complete: 1.0%; Average loss: 3.8342\n",
      "Iteration: 52; Percent complete: 1.0%; Average loss: 3.5440\n",
      "Iteration: 53; Percent complete: 1.1%; Average loss: 3.5283\n",
      "Iteration: 54; Percent complete: 1.1%; Average loss: 3.5597\n",
      "Iteration: 55; Percent complete: 1.1%; Average loss: 3.4469\n",
      "Iteration: 56; Percent complete: 1.1%; Average loss: 3.6239\n",
      "Iteration: 57; Percent complete: 1.1%; Average loss: 3.5988\n",
      "Iteration: 58; Percent complete: 1.2%; Average loss: 3.5057\n",
      "Iteration: 59; Percent complete: 1.2%; Average loss: 3.2568\n",
      "Iteration: 60; Percent complete: 1.2%; Average loss: 3.4269\n",
      "Iteration: 61; Percent complete: 1.2%; Average loss: 3.5971\n",
      "Iteration: 62; Percent complete: 1.2%; Average loss: 3.5265\n",
      "Iteration: 63; Percent complete: 1.3%; Average loss: 3.3453\n",
      "Iteration: 64; Percent complete: 1.3%; Average loss: 3.4110\n",
      "Iteration: 65; Percent complete: 1.3%; Average loss: 3.3009\n",
      "Iteration: 66; Percent complete: 1.3%; Average loss: 3.1033\n",
      "Iteration: 67; Percent complete: 1.3%; Average loss: 3.1414\n",
      "Iteration: 68; Percent complete: 1.4%; Average loss: 3.0226\n",
      "Iteration: 69; Percent complete: 1.4%; Average loss: 3.0900\n",
      "Iteration: 70; Percent complete: 1.4%; Average loss: 3.4471\n",
      "Iteration: 71; Percent complete: 1.4%; Average loss: 3.0528\n",
      "Iteration: 72; Percent complete: 1.4%; Average loss: 2.7200\n",
      "Iteration: 73; Percent complete: 1.5%; Average loss: 3.2318\n",
      "Iteration: 74; Percent complete: 1.5%; Average loss: 2.9943\n",
      "Iteration: 75; Percent complete: 1.5%; Average loss: 3.2185\n",
      "Iteration: 76; Percent complete: 1.5%; Average loss: 3.0513\n",
      "Iteration: 77; Percent complete: 1.5%; Average loss: 3.1430\n",
      "Iteration: 78; Percent complete: 1.6%; Average loss: 2.9329\n",
      "Iteration: 79; Percent complete: 1.6%; Average loss: 3.0114\n",
      "Iteration: 80; Percent complete: 1.6%; Average loss: 3.0927\n",
      "Iteration: 81; Percent complete: 1.6%; Average loss: 3.0221\n",
      "Iteration: 82; Percent complete: 1.6%; Average loss: 2.9291\n",
      "Iteration: 83; Percent complete: 1.7%; Average loss: 3.0204\n",
      "Iteration: 84; Percent complete: 1.7%; Average loss: 2.8199\n",
      "Iteration: 85; Percent complete: 1.7%; Average loss: 2.8939\n",
      "Iteration: 86; Percent complete: 1.7%; Average loss: 2.8658\n",
      "Iteration: 87; Percent complete: 1.7%; Average loss: 2.8215\n",
      "Iteration: 88; Percent complete: 1.8%; Average loss: 2.8462\n",
      "Iteration: 89; Percent complete: 1.8%; Average loss: 2.8186\n",
      "Iteration: 90; Percent complete: 1.8%; Average loss: 2.8185\n",
      "Iteration: 91; Percent complete: 1.8%; Average loss: 2.7055\n",
      "Iteration: 92; Percent complete: 1.8%; Average loss: 2.8338\n",
      "Iteration: 93; Percent complete: 1.9%; Average loss: 2.8346\n",
      "Iteration: 94; Percent complete: 1.9%; Average loss: 2.9599\n",
      "Iteration: 95; Percent complete: 1.9%; Average loss: 2.9821\n",
      "Iteration: 96; Percent complete: 1.9%; Average loss: 2.8251\n",
      "Iteration: 97; Percent complete: 1.9%; Average loss: 2.5662\n",
      "Iteration: 98; Percent complete: 2.0%; Average loss: 2.6564\n",
      "Iteration: 99; Percent complete: 2.0%; Average loss: 2.6569\n",
      "Iteration: 100; Percent complete: 2.0%; Average loss: 2.5158\n",
      "Iteration: 101; Percent complete: 2.0%; Average loss: 2.9234\n",
      "Iteration: 102; Percent complete: 2.0%; Average loss: 2.7846\n",
      "Iteration: 103; Percent complete: 2.1%; Average loss: 2.8669\n",
      "Iteration: 104; Percent complete: 2.1%; Average loss: 2.6064\n",
      "Iteration: 105; Percent complete: 2.1%; Average loss: 2.8170\n",
      "Iteration: 106; Percent complete: 2.1%; Average loss: 2.5870\n",
      "Iteration: 107; Percent complete: 2.1%; Average loss: 2.6721\n",
      "Iteration: 108; Percent complete: 2.2%; Average loss: 2.5082\n",
      "Iteration: 109; Percent complete: 2.2%; Average loss: 2.7483\n",
      "Iteration: 110; Percent complete: 2.2%; Average loss: 2.5441\n",
      "Iteration: 111; Percent complete: 2.2%; Average loss: 2.6590\n",
      "Iteration: 112; Percent complete: 2.2%; Average loss: 2.6319\n",
      "Iteration: 113; Percent complete: 2.3%; Average loss: 2.6744\n",
      "Iteration: 114; Percent complete: 2.3%; Average loss: 2.5093\n",
      "Iteration: 115; Percent complete: 2.3%; Average loss: 2.3838\n",
      "Iteration: 116; Percent complete: 2.3%; Average loss: 2.4348\n",
      "Iteration: 117; Percent complete: 2.3%; Average loss: 2.5116\n",
      "Iteration: 118; Percent complete: 2.4%; Average loss: 2.3283\n",
      "Iteration: 119; Percent complete: 2.4%; Average loss: 2.6948\n",
      "Iteration: 120; Percent complete: 2.4%; Average loss: 2.5894\n",
      "Iteration: 121; Percent complete: 2.4%; Average loss: 2.4811\n",
      "Iteration: 122; Percent complete: 2.4%; Average loss: 2.5616\n",
      "Iteration: 123; Percent complete: 2.5%; Average loss: 2.4205\n",
      "Iteration: 124; Percent complete: 2.5%; Average loss: 2.4874\n",
      "Iteration: 125; Percent complete: 2.5%; Average loss: 2.2510\n",
      "Iteration: 126; Percent complete: 2.5%; Average loss: 2.4586\n",
      "Iteration: 127; Percent complete: 2.5%; Average loss: 2.4222\n",
      "Iteration: 128; Percent complete: 2.6%; Average loss: 2.4439\n",
      "Iteration: 129; Percent complete: 2.6%; Average loss: 2.5216\n",
      "Iteration: 130; Percent complete: 2.6%; Average loss: 2.4089\n",
      "Iteration: 131; Percent complete: 2.6%; Average loss: 2.4016\n",
      "Iteration: 132; Percent complete: 2.6%; Average loss: 2.4208\n",
      "Iteration: 133; Percent complete: 2.7%; Average loss: 2.5475\n",
      "Iteration: 134; Percent complete: 2.7%; Average loss: 2.1030\n",
      "Iteration: 135; Percent complete: 2.7%; Average loss: 2.4239\n",
      "Iteration: 136; Percent complete: 2.7%; Average loss: 2.2112\n",
      "Iteration: 137; Percent complete: 2.7%; Average loss: 2.2677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 138; Percent complete: 2.8%; Average loss: 2.2016\n",
      "Iteration: 139; Percent complete: 2.8%; Average loss: 2.4575\n",
      "Iteration: 140; Percent complete: 2.8%; Average loss: 2.2470\n",
      "Iteration: 141; Percent complete: 2.8%; Average loss: 2.4293\n",
      "Iteration: 142; Percent complete: 2.8%; Average loss: 2.4415\n",
      "Iteration: 143; Percent complete: 2.9%; Average loss: 2.3851\n",
      "Iteration: 144; Percent complete: 2.9%; Average loss: 2.1438\n",
      "Iteration: 145; Percent complete: 2.9%; Average loss: 2.5613\n",
      "Iteration: 146; Percent complete: 2.9%; Average loss: 2.2465\n",
      "Iteration: 147; Percent complete: 2.9%; Average loss: 2.2448\n",
      "Iteration: 148; Percent complete: 3.0%; Average loss: 2.2451\n",
      "Iteration: 149; Percent complete: 3.0%; Average loss: 2.0616\n",
      "Iteration: 150; Percent complete: 3.0%; Average loss: 2.2444\n",
      "Iteration: 151; Percent complete: 3.0%; Average loss: 2.5409\n",
      "Iteration: 152; Percent complete: 3.0%; Average loss: 2.4006\n",
      "Iteration: 153; Percent complete: 3.1%; Average loss: 2.2571\n",
      "Iteration: 154; Percent complete: 3.1%; Average loss: 2.4126\n",
      "Iteration: 155; Percent complete: 3.1%; Average loss: 2.2536\n",
      "Iteration: 156; Percent complete: 3.1%; Average loss: 2.3919\n",
      "Iteration: 157; Percent complete: 3.1%; Average loss: 2.2968\n",
      "Iteration: 158; Percent complete: 3.2%; Average loss: 2.2441\n",
      "Iteration: 159; Percent complete: 3.2%; Average loss: 2.1665\n",
      "Iteration: 160; Percent complete: 3.2%; Average loss: 2.0391\n",
      "Iteration: 161; Percent complete: 3.2%; Average loss: 2.0923\n",
      "Iteration: 162; Percent complete: 3.2%; Average loss: 2.3191\n",
      "Iteration: 163; Percent complete: 3.3%; Average loss: 1.9911\n",
      "Iteration: 164; Percent complete: 3.3%; Average loss: 2.1557\n",
      "Iteration: 165; Percent complete: 3.3%; Average loss: 1.9217\n",
      "Iteration: 166; Percent complete: 3.3%; Average loss: 2.1181\n",
      "Iteration: 167; Percent complete: 3.3%; Average loss: 2.1092\n",
      "Iteration: 168; Percent complete: 3.4%; Average loss: 2.1502\n",
      "Iteration: 169; Percent complete: 3.4%; Average loss: 2.1387\n",
      "Iteration: 170; Percent complete: 3.4%; Average loss: 2.0527\n",
      "Iteration: 171; Percent complete: 3.4%; Average loss: 2.0318\n",
      "Iteration: 172; Percent complete: 3.4%; Average loss: 2.0739\n",
      "Iteration: 173; Percent complete: 3.5%; Average loss: 2.1055\n",
      "Iteration: 174; Percent complete: 3.5%; Average loss: 2.0317\n",
      "Iteration: 175; Percent complete: 3.5%; Average loss: 2.0101\n",
      "Iteration: 176; Percent complete: 3.5%; Average loss: 2.0791\n",
      "Iteration: 177; Percent complete: 3.5%; Average loss: 2.0163\n",
      "Iteration: 178; Percent complete: 3.6%; Average loss: 2.2821\n",
      "Iteration: 179; Percent complete: 3.6%; Average loss: 2.0341\n",
      "Iteration: 180; Percent complete: 3.6%; Average loss: 2.2335\n",
      "Iteration: 181; Percent complete: 3.6%; Average loss: 2.2623\n",
      "Iteration: 182; Percent complete: 3.6%; Average loss: 2.0434\n",
      "Iteration: 183; Percent complete: 3.7%; Average loss: 2.0531\n",
      "Iteration: 184; Percent complete: 3.7%; Average loss: 2.1151\n",
      "Iteration: 185; Percent complete: 3.7%; Average loss: 1.9827\n",
      "Iteration: 186; Percent complete: 3.7%; Average loss: 2.0934\n",
      "Iteration: 187; Percent complete: 3.7%; Average loss: 2.0915\n",
      "Iteration: 188; Percent complete: 3.8%; Average loss: 2.0429\n",
      "Iteration: 189; Percent complete: 3.8%; Average loss: 2.1623\n",
      "Iteration: 190; Percent complete: 3.8%; Average loss: 1.9829\n",
      "Iteration: 191; Percent complete: 3.8%; Average loss: 1.9360\n",
      "Iteration: 192; Percent complete: 3.8%; Average loss: 1.9187\n",
      "Iteration: 193; Percent complete: 3.9%; Average loss: 1.8872\n",
      "Iteration: 194; Percent complete: 3.9%; Average loss: 2.1269\n",
      "Iteration: 195; Percent complete: 3.9%; Average loss: 2.1149\n",
      "Iteration: 196; Percent complete: 3.9%; Average loss: 1.9301\n",
      "Iteration: 197; Percent complete: 3.9%; Average loss: 1.9654\n",
      "Iteration: 198; Percent complete: 4.0%; Average loss: 1.8427\n",
      "Iteration: 199; Percent complete: 4.0%; Average loss: 1.9870\n",
      "Iteration: 200; Percent complete: 4.0%; Average loss: 2.0182\n",
      "Iteration: 201; Percent complete: 4.0%; Average loss: 1.9454\n",
      "Iteration: 202; Percent complete: 4.0%; Average loss: 1.9441\n",
      "Iteration: 203; Percent complete: 4.1%; Average loss: 1.7894\n",
      "Iteration: 204; Percent complete: 4.1%; Average loss: 1.9686\n",
      "Iteration: 205; Percent complete: 4.1%; Average loss: 1.8405\n",
      "Iteration: 206; Percent complete: 4.1%; Average loss: 1.8423\n",
      "Iteration: 207; Percent complete: 4.1%; Average loss: 1.7421\n",
      "Iteration: 208; Percent complete: 4.2%; Average loss: 1.9897\n",
      "Iteration: 209; Percent complete: 4.2%; Average loss: 1.7969\n",
      "Iteration: 210; Percent complete: 4.2%; Average loss: 1.8648\n",
      "Iteration: 211; Percent complete: 4.2%; Average loss: 1.9236\n",
      "Iteration: 212; Percent complete: 4.2%; Average loss: 1.8610\n",
      "Iteration: 213; Percent complete: 4.3%; Average loss: 2.1538\n",
      "Iteration: 214; Percent complete: 4.3%; Average loss: 1.9023\n",
      "Iteration: 215; Percent complete: 4.3%; Average loss: 1.8028\n",
      "Iteration: 216; Percent complete: 4.3%; Average loss: 1.8950\n",
      "Iteration: 217; Percent complete: 4.3%; Average loss: 1.8304\n",
      "Iteration: 218; Percent complete: 4.4%; Average loss: 1.8194\n",
      "Iteration: 219; Percent complete: 4.4%; Average loss: 1.8488\n",
      "Iteration: 220; Percent complete: 4.4%; Average loss: 1.6977\n",
      "Iteration: 221; Percent complete: 4.4%; Average loss: 1.7187\n",
      "Iteration: 222; Percent complete: 4.4%; Average loss: 1.8531\n",
      "Iteration: 223; Percent complete: 4.5%; Average loss: 1.7566\n",
      "Iteration: 224; Percent complete: 4.5%; Average loss: 1.7649\n",
      "Iteration: 225; Percent complete: 4.5%; Average loss: 1.8055\n",
      "Iteration: 226; Percent complete: 4.5%; Average loss: 1.7051\n",
      "Iteration: 227; Percent complete: 4.5%; Average loss: 1.9493\n",
      "Iteration: 228; Percent complete: 4.6%; Average loss: 1.8647\n",
      "Iteration: 229; Percent complete: 4.6%; Average loss: 1.8152\n",
      "Iteration: 230; Percent complete: 4.6%; Average loss: 1.7724\n",
      "Iteration: 231; Percent complete: 4.6%; Average loss: 1.6896\n",
      "Iteration: 232; Percent complete: 4.6%; Average loss: 2.0005\n",
      "Iteration: 233; Percent complete: 4.7%; Average loss: 1.6307\n",
      "Iteration: 234; Percent complete: 4.7%; Average loss: 2.0693\n",
      "Iteration: 235; Percent complete: 4.7%; Average loss: 1.7774\n",
      "Iteration: 236; Percent complete: 4.7%; Average loss: 1.7910\n",
      "Iteration: 237; Percent complete: 4.7%; Average loss: 1.7232\n",
      "Iteration: 238; Percent complete: 4.8%; Average loss: 1.6268\n",
      "Iteration: 239; Percent complete: 4.8%; Average loss: 1.7160\n",
      "Iteration: 240; Percent complete: 4.8%; Average loss: 1.9215\n",
      "Iteration: 241; Percent complete: 4.8%; Average loss: 1.7030\n",
      "Iteration: 242; Percent complete: 4.8%; Average loss: 1.8437\n",
      "Iteration: 243; Percent complete: 4.9%; Average loss: 1.7914\n",
      "Iteration: 244; Percent complete: 4.9%; Average loss: 1.5764\n",
      "Iteration: 245; Percent complete: 4.9%; Average loss: 1.6989\n",
      "Iteration: 246; Percent complete: 4.9%; Average loss: 1.8031\n",
      "Iteration: 247; Percent complete: 4.9%; Average loss: 1.8100\n",
      "Iteration: 248; Percent complete: 5.0%; Average loss: 1.7108\n",
      "Iteration: 249; Percent complete: 5.0%; Average loss: 1.6313\n",
      "Iteration: 250; Percent complete: 5.0%; Average loss: 1.5430\n",
      "Iteration: 251; Percent complete: 5.0%; Average loss: 1.5827\n",
      "Iteration: 252; Percent complete: 5.0%; Average loss: 1.8518\n",
      "Iteration: 253; Percent complete: 5.1%; Average loss: 1.5772\n",
      "Iteration: 254; Percent complete: 5.1%; Average loss: 1.5936\n",
      "Iteration: 255; Percent complete: 5.1%; Average loss: 1.5889\n",
      "Iteration: 256; Percent complete: 5.1%; Average loss: 1.6028\n",
      "Iteration: 257; Percent complete: 5.1%; Average loss: 1.5899\n",
      "Iteration: 258; Percent complete: 5.2%; Average loss: 1.8003\n",
      "Iteration: 259; Percent complete: 5.2%; Average loss: 1.5481\n",
      "Iteration: 260; Percent complete: 5.2%; Average loss: 1.5848\n",
      "Iteration: 261; Percent complete: 5.2%; Average loss: 1.7150\n",
      "Iteration: 262; Percent complete: 5.2%; Average loss: 1.5739\n",
      "Iteration: 263; Percent complete: 5.3%; Average loss: 1.5531\n",
      "Iteration: 264; Percent complete: 5.3%; Average loss: 1.6567\n",
      "Iteration: 265; Percent complete: 5.3%; Average loss: 1.6087\n",
      "Iteration: 266; Percent complete: 5.3%; Average loss: 1.5048\n",
      "Iteration: 267; Percent complete: 5.3%; Average loss: 1.5674\n",
      "Iteration: 268; Percent complete: 5.4%; Average loss: 1.6000\n",
      "Iteration: 269; Percent complete: 5.4%; Average loss: 1.4414\n",
      "Iteration: 270; Percent complete: 5.4%; Average loss: 1.6467\n",
      "Iteration: 271; Percent complete: 5.4%; Average loss: 1.6673\n",
      "Iteration: 272; Percent complete: 5.4%; Average loss: 1.5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 273; Percent complete: 5.5%; Average loss: 1.4222\n",
      "Iteration: 274; Percent complete: 5.5%; Average loss: 1.5241\n",
      "Iteration: 275; Percent complete: 5.5%; Average loss: 1.4713\n",
      "Iteration: 276; Percent complete: 5.5%; Average loss: 1.5140\n",
      "Iteration: 277; Percent complete: 5.5%; Average loss: 1.4615\n",
      "Iteration: 278; Percent complete: 5.6%; Average loss: 1.5956\n",
      "Iteration: 279; Percent complete: 5.6%; Average loss: 1.4507\n",
      "Iteration: 280; Percent complete: 5.6%; Average loss: 1.6327\n",
      "Iteration: 281; Percent complete: 5.6%; Average loss: 1.4503\n",
      "Iteration: 282; Percent complete: 5.6%; Average loss: 1.5831\n",
      "Iteration: 283; Percent complete: 5.7%; Average loss: 1.4301\n",
      "Iteration: 284; Percent complete: 5.7%; Average loss: 1.5889\n",
      "Iteration: 285; Percent complete: 5.7%; Average loss: 1.6139\n",
      "Iteration: 286; Percent complete: 5.7%; Average loss: 1.5044\n",
      "Iteration: 287; Percent complete: 5.7%; Average loss: 1.4672\n",
      "Iteration: 288; Percent complete: 5.8%; Average loss: 1.5353\n",
      "Iteration: 289; Percent complete: 5.8%; Average loss: 1.5225\n",
      "Iteration: 290; Percent complete: 5.8%; Average loss: 1.5826\n",
      "Iteration: 291; Percent complete: 5.8%; Average loss: 1.5500\n",
      "Iteration: 292; Percent complete: 5.8%; Average loss: 1.4457\n",
      "Iteration: 293; Percent complete: 5.9%; Average loss: 1.3503\n",
      "Iteration: 294; Percent complete: 5.9%; Average loss: 1.3921\n",
      "Iteration: 295; Percent complete: 5.9%; Average loss: 1.5511\n",
      "Iteration: 296; Percent complete: 5.9%; Average loss: 1.5504\n",
      "Iteration: 297; Percent complete: 5.9%; Average loss: 1.4907\n",
      "Iteration: 298; Percent complete: 6.0%; Average loss: 1.4518\n",
      "Iteration: 299; Percent complete: 6.0%; Average loss: 1.4657\n",
      "Iteration: 300; Percent complete: 6.0%; Average loss: 1.3549\n",
      "Iteration: 301; Percent complete: 6.0%; Average loss: 1.5327\n",
      "Iteration: 302; Percent complete: 6.0%; Average loss: 1.4315\n",
      "Iteration: 303; Percent complete: 6.1%; Average loss: 1.4413\n",
      "Iteration: 304; Percent complete: 6.1%; Average loss: 1.5530\n",
      "Iteration: 305; Percent complete: 6.1%; Average loss: 1.5802\n",
      "Iteration: 306; Percent complete: 6.1%; Average loss: 1.3377\n",
      "Iteration: 307; Percent complete: 6.1%; Average loss: 1.4595\n",
      "Iteration: 308; Percent complete: 6.2%; Average loss: 1.2725\n",
      "Iteration: 309; Percent complete: 6.2%; Average loss: 1.4533\n",
      "Iteration: 310; Percent complete: 6.2%; Average loss: 1.4338\n",
      "Iteration: 311; Percent complete: 6.2%; Average loss: 1.3687\n",
      "Iteration: 312; Percent complete: 6.2%; Average loss: 1.4444\n",
      "Iteration: 313; Percent complete: 6.3%; Average loss: 1.2125\n",
      "Iteration: 314; Percent complete: 6.3%; Average loss: 1.4753\n",
      "Iteration: 315; Percent complete: 6.3%; Average loss: 1.3880\n",
      "Iteration: 316; Percent complete: 6.3%; Average loss: 1.4262\n",
      "Iteration: 317; Percent complete: 6.3%; Average loss: 1.3165\n",
      "Iteration: 318; Percent complete: 6.4%; Average loss: 1.5140\n",
      "Iteration: 319; Percent complete: 6.4%; Average loss: 1.4737\n",
      "Iteration: 320; Percent complete: 6.4%; Average loss: 1.3563\n",
      "Iteration: 321; Percent complete: 6.4%; Average loss: 1.4130\n",
      "Iteration: 322; Percent complete: 6.4%; Average loss: 1.2973\n",
      "Iteration: 323; Percent complete: 6.5%; Average loss: 1.3559\n",
      "Iteration: 324; Percent complete: 6.5%; Average loss: 1.4095\n",
      "Iteration: 325; Percent complete: 6.5%; Average loss: 1.4135\n",
      "Iteration: 326; Percent complete: 6.5%; Average loss: 1.3346\n",
      "Iteration: 327; Percent complete: 6.5%; Average loss: 1.3132\n",
      "Iteration: 328; Percent complete: 6.6%; Average loss: 1.2821\n",
      "Iteration: 329; Percent complete: 6.6%; Average loss: 1.4006\n",
      "Iteration: 330; Percent complete: 6.6%; Average loss: 1.3388\n",
      "Iteration: 331; Percent complete: 6.6%; Average loss: 1.3106\n",
      "Iteration: 332; Percent complete: 6.6%; Average loss: 1.2892\n",
      "Iteration: 333; Percent complete: 6.7%; Average loss: 1.3375\n",
      "Iteration: 334; Percent complete: 6.7%; Average loss: 1.5178\n",
      "Iteration: 335; Percent complete: 6.7%; Average loss: 1.4108\n",
      "Iteration: 336; Percent complete: 6.7%; Average loss: 1.3961\n",
      "Iteration: 337; Percent complete: 6.7%; Average loss: 1.3814\n",
      "Iteration: 338; Percent complete: 6.8%; Average loss: 1.3683\n",
      "Iteration: 339; Percent complete: 6.8%; Average loss: 1.5178\n",
      "Iteration: 340; Percent complete: 6.8%; Average loss: 1.3447\n",
      "Iteration: 341; Percent complete: 6.8%; Average loss: 1.4092\n",
      "Iteration: 342; Percent complete: 6.8%; Average loss: 1.3871\n",
      "Iteration: 343; Percent complete: 6.9%; Average loss: 1.2199\n",
      "Iteration: 344; Percent complete: 6.9%; Average loss: 1.3153\n",
      "Iteration: 345; Percent complete: 6.9%; Average loss: 1.2997\n",
      "Iteration: 346; Percent complete: 6.9%; Average loss: 1.3701\n",
      "Iteration: 347; Percent complete: 6.9%; Average loss: 1.3475\n",
      "Iteration: 348; Percent complete: 7.0%; Average loss: 1.2686\n",
      "Iteration: 349; Percent complete: 7.0%; Average loss: 1.2190\n",
      "Iteration: 350; Percent complete: 7.0%; Average loss: 1.3537\n",
      "Iteration: 351; Percent complete: 7.0%; Average loss: 1.4054\n",
      "Iteration: 352; Percent complete: 7.0%; Average loss: 1.2076\n",
      "Iteration: 353; Percent complete: 7.1%; Average loss: 1.4025\n",
      "Iteration: 354; Percent complete: 7.1%; Average loss: 1.4326\n",
      "Iteration: 355; Percent complete: 7.1%; Average loss: 1.2617\n",
      "Iteration: 356; Percent complete: 7.1%; Average loss: 1.1174\n",
      "Iteration: 357; Percent complete: 7.1%; Average loss: 1.1821\n",
      "Iteration: 358; Percent complete: 7.2%; Average loss: 1.1221\n",
      "Iteration: 359; Percent complete: 7.2%; Average loss: 1.0704\n",
      "Iteration: 360; Percent complete: 7.2%; Average loss: 1.3069\n",
      "Iteration: 361; Percent complete: 7.2%; Average loss: 1.1090\n",
      "Iteration: 362; Percent complete: 7.2%; Average loss: 1.2495\n",
      "Iteration: 363; Percent complete: 7.3%; Average loss: 1.1593\n",
      "Iteration: 364; Percent complete: 7.3%; Average loss: 1.2724\n",
      "Iteration: 365; Percent complete: 7.3%; Average loss: 1.1912\n",
      "Iteration: 366; Percent complete: 7.3%; Average loss: 1.0870\n",
      "Iteration: 367; Percent complete: 7.3%; Average loss: 1.2229\n",
      "Iteration: 368; Percent complete: 7.4%; Average loss: 1.2901\n",
      "Iteration: 369; Percent complete: 7.4%; Average loss: 1.2320\n",
      "Iteration: 370; Percent complete: 7.4%; Average loss: 1.2810\n",
      "Iteration: 371; Percent complete: 7.4%; Average loss: 1.2577\n",
      "Iteration: 372; Percent complete: 7.4%; Average loss: 1.1178\n",
      "Iteration: 373; Percent complete: 7.5%; Average loss: 1.2271\n",
      "Iteration: 374; Percent complete: 7.5%; Average loss: 1.1556\n",
      "Iteration: 375; Percent complete: 7.5%; Average loss: 1.1853\n",
      "Iteration: 376; Percent complete: 7.5%; Average loss: 1.2100\n",
      "Iteration: 377; Percent complete: 7.5%; Average loss: 1.1079\n",
      "Iteration: 378; Percent complete: 7.6%; Average loss: 1.0288\n",
      "Iteration: 379; Percent complete: 7.6%; Average loss: 1.2006\n",
      "Iteration: 380; Percent complete: 7.6%; Average loss: 1.2326\n",
      "Iteration: 381; Percent complete: 7.6%; Average loss: 1.0206\n",
      "Iteration: 382; Percent complete: 7.6%; Average loss: 1.2568\n",
      "Iteration: 383; Percent complete: 7.7%; Average loss: 1.0985\n",
      "Iteration: 384; Percent complete: 7.7%; Average loss: 1.1569\n",
      "Iteration: 385; Percent complete: 7.7%; Average loss: 1.0382\n",
      "Iteration: 386; Percent complete: 7.7%; Average loss: 1.1635\n",
      "Iteration: 387; Percent complete: 7.7%; Average loss: 1.1973\n",
      "Iteration: 388; Percent complete: 7.8%; Average loss: 1.1092\n",
      "Iteration: 389; Percent complete: 7.8%; Average loss: 1.0994\n",
      "Iteration: 390; Percent complete: 7.8%; Average loss: 1.0611\n",
      "Iteration: 391; Percent complete: 7.8%; Average loss: 0.9636\n",
      "Iteration: 392; Percent complete: 7.8%; Average loss: 1.1037\n",
      "Iteration: 393; Percent complete: 7.9%; Average loss: 1.1209\n",
      "Iteration: 394; Percent complete: 7.9%; Average loss: 1.0814\n",
      "Iteration: 395; Percent complete: 7.9%; Average loss: 1.0510\n",
      "Iteration: 396; Percent complete: 7.9%; Average loss: 1.1231\n",
      "Iteration: 397; Percent complete: 7.9%; Average loss: 1.0719\n",
      "Iteration: 398; Percent complete: 8.0%; Average loss: 1.1313\n",
      "Iteration: 399; Percent complete: 8.0%; Average loss: 1.0056\n",
      "Iteration: 400; Percent complete: 8.0%; Average loss: 1.1857\n",
      "Iteration: 401; Percent complete: 8.0%; Average loss: 1.2125\n",
      "Iteration: 402; Percent complete: 8.0%; Average loss: 1.0860\n",
      "Iteration: 403; Percent complete: 8.1%; Average loss: 1.0328\n",
      "Iteration: 404; Percent complete: 8.1%; Average loss: 1.0508\n",
      "Iteration: 405; Percent complete: 8.1%; Average loss: 1.1089\n",
      "Iteration: 406; Percent complete: 8.1%; Average loss: 1.1034\n",
      "Iteration: 407; Percent complete: 8.1%; Average loss: 1.0901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 408; Percent complete: 8.2%; Average loss: 1.1053\n",
      "Iteration: 409; Percent complete: 8.2%; Average loss: 0.9829\n",
      "Iteration: 410; Percent complete: 8.2%; Average loss: 1.1575\n",
      "Iteration: 411; Percent complete: 8.2%; Average loss: 1.0065\n",
      "Iteration: 412; Percent complete: 8.2%; Average loss: 1.0233\n",
      "Iteration: 413; Percent complete: 8.3%; Average loss: 1.0744\n",
      "Iteration: 414; Percent complete: 8.3%; Average loss: 1.0807\n",
      "Iteration: 415; Percent complete: 8.3%; Average loss: 0.9507\n",
      "Iteration: 416; Percent complete: 8.3%; Average loss: 1.2049\n",
      "Iteration: 417; Percent complete: 8.3%; Average loss: 1.1128\n",
      "Iteration: 418; Percent complete: 8.4%; Average loss: 1.0553\n",
      "Iteration: 419; Percent complete: 8.4%; Average loss: 0.9668\n",
      "Iteration: 420; Percent complete: 8.4%; Average loss: 0.9120\n",
      "Iteration: 421; Percent complete: 8.4%; Average loss: 1.0146\n",
      "Iteration: 422; Percent complete: 8.4%; Average loss: 1.0455\n",
      "Iteration: 423; Percent complete: 8.5%; Average loss: 1.0190\n",
      "Iteration: 424; Percent complete: 8.5%; Average loss: 0.9682\n",
      "Iteration: 425; Percent complete: 8.5%; Average loss: 1.0208\n",
      "Iteration: 426; Percent complete: 8.5%; Average loss: 0.9214\n",
      "Iteration: 427; Percent complete: 8.5%; Average loss: 0.9330\n",
      "Iteration: 428; Percent complete: 8.6%; Average loss: 1.0180\n",
      "Iteration: 429; Percent complete: 8.6%; Average loss: 0.9596\n",
      "Iteration: 430; Percent complete: 8.6%; Average loss: 1.0505\n",
      "Iteration: 431; Percent complete: 8.6%; Average loss: 1.0318\n",
      "Iteration: 432; Percent complete: 8.6%; Average loss: 1.0801\n",
      "Iteration: 433; Percent complete: 8.7%; Average loss: 1.0358\n",
      "Iteration: 434; Percent complete: 8.7%; Average loss: 1.0194\n",
      "Iteration: 435; Percent complete: 8.7%; Average loss: 1.0149\n",
      "Iteration: 436; Percent complete: 8.7%; Average loss: 1.0301\n",
      "Iteration: 437; Percent complete: 8.7%; Average loss: 0.9355\n",
      "Iteration: 438; Percent complete: 8.8%; Average loss: 1.1085\n",
      "Iteration: 439; Percent complete: 8.8%; Average loss: 0.9907\n",
      "Iteration: 440; Percent complete: 8.8%; Average loss: 0.8438\n",
      "Iteration: 441; Percent complete: 8.8%; Average loss: 0.9467\n",
      "Iteration: 442; Percent complete: 8.8%; Average loss: 0.9501\n",
      "Iteration: 443; Percent complete: 8.9%; Average loss: 1.0090\n",
      "Iteration: 444; Percent complete: 8.9%; Average loss: 0.9571\n",
      "Iteration: 445; Percent complete: 8.9%; Average loss: 0.8528\n",
      "Iteration: 446; Percent complete: 8.9%; Average loss: 1.0750\n",
      "Iteration: 447; Percent complete: 8.9%; Average loss: 0.9112\n",
      "Iteration: 448; Percent complete: 9.0%; Average loss: 0.9760\n",
      "Iteration: 449; Percent complete: 9.0%; Average loss: 0.9573\n",
      "Iteration: 450; Percent complete: 9.0%; Average loss: 0.8439\n",
      "Iteration: 451; Percent complete: 9.0%; Average loss: 0.8298\n",
      "Iteration: 452; Percent complete: 9.0%; Average loss: 0.9601\n",
      "Iteration: 453; Percent complete: 9.1%; Average loss: 0.9006\n",
      "Iteration: 454; Percent complete: 9.1%; Average loss: 0.8625\n",
      "Iteration: 455; Percent complete: 9.1%; Average loss: 0.9487\n",
      "Iteration: 456; Percent complete: 9.1%; Average loss: 1.0113\n",
      "Iteration: 457; Percent complete: 9.1%; Average loss: 1.0151\n",
      "Iteration: 458; Percent complete: 9.2%; Average loss: 0.8576\n",
      "Iteration: 459; Percent complete: 9.2%; Average loss: 1.0531\n",
      "Iteration: 460; Percent complete: 9.2%; Average loss: 0.8959\n",
      "Iteration: 461; Percent complete: 9.2%; Average loss: 0.8410\n",
      "Iteration: 462; Percent complete: 9.2%; Average loss: 0.9509\n",
      "Iteration: 463; Percent complete: 9.3%; Average loss: 0.8362\n",
      "Iteration: 464; Percent complete: 9.3%; Average loss: 0.8264\n",
      "Iteration: 465; Percent complete: 9.3%; Average loss: 0.8810\n",
      "Iteration: 466; Percent complete: 9.3%; Average loss: 0.9073\n",
      "Iteration: 467; Percent complete: 9.3%; Average loss: 0.8789\n",
      "Iteration: 468; Percent complete: 9.4%; Average loss: 0.8658\n",
      "Iteration: 469; Percent complete: 9.4%; Average loss: 0.9191\n",
      "Iteration: 470; Percent complete: 9.4%; Average loss: 0.8603\n",
      "Iteration: 471; Percent complete: 9.4%; Average loss: 0.7938\n",
      "Iteration: 472; Percent complete: 9.4%; Average loss: 0.9793\n",
      "Iteration: 473; Percent complete: 9.5%; Average loss: 0.8864\n",
      "Iteration: 474; Percent complete: 9.5%; Average loss: 0.8526\n",
      "Iteration: 475; Percent complete: 9.5%; Average loss: 0.8765\n",
      "Iteration: 476; Percent complete: 9.5%; Average loss: 0.8625\n",
      "Iteration: 477; Percent complete: 9.5%; Average loss: 0.8083\n",
      "Iteration: 478; Percent complete: 9.6%; Average loss: 0.8788\n",
      "Iteration: 479; Percent complete: 9.6%; Average loss: 0.8488\n",
      "Iteration: 480; Percent complete: 9.6%; Average loss: 0.8630\n",
      "Iteration: 481; Percent complete: 9.6%; Average loss: 0.7740\n",
      "Iteration: 482; Percent complete: 9.6%; Average loss: 0.8527\n",
      "Iteration: 483; Percent complete: 9.7%; Average loss: 0.7185\n",
      "Iteration: 484; Percent complete: 9.7%; Average loss: 0.8076\n",
      "Iteration: 485; Percent complete: 9.7%; Average loss: 0.8332\n",
      "Iteration: 486; Percent complete: 9.7%; Average loss: 0.8637\n",
      "Iteration: 487; Percent complete: 9.7%; Average loss: 0.7423\n",
      "Iteration: 488; Percent complete: 9.8%; Average loss: 0.8080\n",
      "Iteration: 489; Percent complete: 9.8%; Average loss: 0.7928\n",
      "Iteration: 490; Percent complete: 9.8%; Average loss: 0.8357\n",
      "Iteration: 491; Percent complete: 9.8%; Average loss: 0.7265\n",
      "Iteration: 492; Percent complete: 9.8%; Average loss: 0.7885\n",
      "Iteration: 493; Percent complete: 9.9%; Average loss: 0.7657\n",
      "Iteration: 494; Percent complete: 9.9%; Average loss: 0.7384\n",
      "Iteration: 495; Percent complete: 9.9%; Average loss: 0.6347\n",
      "Iteration: 496; Percent complete: 9.9%; Average loss: 0.8048\n",
      "Iteration: 497; Percent complete: 9.9%; Average loss: 0.7337\n",
      "Iteration: 498; Percent complete: 10.0%; Average loss: 0.7144\n",
      "Iteration: 499; Percent complete: 10.0%; Average loss: 0.7404\n",
      "Iteration: 500; Percent complete: 10.0%; Average loss: 0.7231\n",
      "Iteration: 501; Percent complete: 10.0%; Average loss: 0.7582\n",
      "Iteration: 502; Percent complete: 10.0%; Average loss: 0.7537\n",
      "Iteration: 503; Percent complete: 10.1%; Average loss: 0.6797\n",
      "Iteration: 504; Percent complete: 10.1%; Average loss: 0.6796\n",
      "Iteration: 505; Percent complete: 10.1%; Average loss: 0.8053\n",
      "Iteration: 506; Percent complete: 10.1%; Average loss: 0.7206\n",
      "Iteration: 507; Percent complete: 10.1%; Average loss: 0.6563\n",
      "Iteration: 508; Percent complete: 10.2%; Average loss: 0.7279\n",
      "Iteration: 509; Percent complete: 10.2%; Average loss: 0.8014\n",
      "Iteration: 510; Percent complete: 10.2%; Average loss: 0.7811\n",
      "Iteration: 511; Percent complete: 10.2%; Average loss: 0.7039\n",
      "Iteration: 512; Percent complete: 10.2%; Average loss: 0.6801\n",
      "Iteration: 513; Percent complete: 10.3%; Average loss: 0.8356\n",
      "Iteration: 514; Percent complete: 10.3%; Average loss: 0.7506\n",
      "Iteration: 515; Percent complete: 10.3%; Average loss: 0.7942\n",
      "Iteration: 516; Percent complete: 10.3%; Average loss: 0.7558\n",
      "Iteration: 517; Percent complete: 10.3%; Average loss: 0.7005\n",
      "Iteration: 518; Percent complete: 10.4%; Average loss: 0.7288\n",
      "Iteration: 519; Percent complete: 10.4%; Average loss: 0.7691\n",
      "Iteration: 520; Percent complete: 10.4%; Average loss: 0.6829\n",
      "Iteration: 521; Percent complete: 10.4%; Average loss: 0.6562\n",
      "Iteration: 522; Percent complete: 10.4%; Average loss: 0.7327\n",
      "Iteration: 523; Percent complete: 10.5%; Average loss: 0.6415\n",
      "Iteration: 524; Percent complete: 10.5%; Average loss: 0.7482\n",
      "Iteration: 525; Percent complete: 10.5%; Average loss: 0.7446\n",
      "Iteration: 526; Percent complete: 10.5%; Average loss: 0.6555\n",
      "Iteration: 527; Percent complete: 10.5%; Average loss: 0.6522\n",
      "Iteration: 528; Percent complete: 10.6%; Average loss: 0.6358\n",
      "Iteration: 529; Percent complete: 10.6%; Average loss: 0.6671\n",
      "Iteration: 530; Percent complete: 10.6%; Average loss: 0.8191\n",
      "Iteration: 531; Percent complete: 10.6%; Average loss: 0.6076\n",
      "Iteration: 532; Percent complete: 10.6%; Average loss: 0.6689\n",
      "Iteration: 533; Percent complete: 10.7%; Average loss: 0.7350\n",
      "Iteration: 534; Percent complete: 10.7%; Average loss: 0.7732\n",
      "Iteration: 535; Percent complete: 10.7%; Average loss: 0.6913\n",
      "Iteration: 536; Percent complete: 10.7%; Average loss: 0.5912\n",
      "Iteration: 537; Percent complete: 10.7%; Average loss: 0.6133\n",
      "Iteration: 538; Percent complete: 10.8%; Average loss: 0.6759\n",
      "Iteration: 539; Percent complete: 10.8%; Average loss: 0.6198\n",
      "Iteration: 540; Percent complete: 10.8%; Average loss: 0.6675\n",
      "Iteration: 541; Percent complete: 10.8%; Average loss: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 542; Percent complete: 10.8%; Average loss: 0.6189\n",
      "Iteration: 543; Percent complete: 10.9%; Average loss: 0.6675\n",
      "Iteration: 544; Percent complete: 10.9%; Average loss: 0.6668\n",
      "Iteration: 545; Percent complete: 10.9%; Average loss: 0.6140\n",
      "Iteration: 546; Percent complete: 10.9%; Average loss: 0.6580\n",
      "Iteration: 547; Percent complete: 10.9%; Average loss: 0.6079\n",
      "Iteration: 548; Percent complete: 11.0%; Average loss: 0.5154\n",
      "Iteration: 549; Percent complete: 11.0%; Average loss: 0.6335\n",
      "Iteration: 550; Percent complete: 11.0%; Average loss: 0.6014\n",
      "Iteration: 551; Percent complete: 11.0%; Average loss: 0.6386\n",
      "Iteration: 552; Percent complete: 11.0%; Average loss: 0.6194\n",
      "Iteration: 553; Percent complete: 11.1%; Average loss: 0.5961\n",
      "Iteration: 554; Percent complete: 11.1%; Average loss: 0.6110\n",
      "Iteration: 555; Percent complete: 11.1%; Average loss: 0.5617\n",
      "Iteration: 556; Percent complete: 11.1%; Average loss: 0.5605\n",
      "Iteration: 557; Percent complete: 11.1%; Average loss: 0.6680\n",
      "Iteration: 558; Percent complete: 11.2%; Average loss: 0.5768\n",
      "Iteration: 559; Percent complete: 11.2%; Average loss: 0.6554\n",
      "Iteration: 560; Percent complete: 11.2%; Average loss: 0.5821\n",
      "Iteration: 561; Percent complete: 11.2%; Average loss: 0.5959\n",
      "Iteration: 562; Percent complete: 11.2%; Average loss: 0.5219\n",
      "Iteration: 563; Percent complete: 11.3%; Average loss: 0.6581\n",
      "Iteration: 564; Percent complete: 11.3%; Average loss: 0.5815\n",
      "Iteration: 565; Percent complete: 11.3%; Average loss: 0.5852\n",
      "Iteration: 566; Percent complete: 11.3%; Average loss: 0.5156\n",
      "Iteration: 567; Percent complete: 11.3%; Average loss: 0.5522\n",
      "Iteration: 568; Percent complete: 11.4%; Average loss: 0.6102\n",
      "Iteration: 569; Percent complete: 11.4%; Average loss: 0.5449\n",
      "Iteration: 570; Percent complete: 11.4%; Average loss: 0.5999\n",
      "Iteration: 571; Percent complete: 11.4%; Average loss: 0.6355\n",
      "Iteration: 572; Percent complete: 11.4%; Average loss: 0.5545\n",
      "Iteration: 573; Percent complete: 11.5%; Average loss: 0.6630\n",
      "Iteration: 574; Percent complete: 11.5%; Average loss: 0.6237\n",
      "Iteration: 575; Percent complete: 11.5%; Average loss: 0.5644\n",
      "Iteration: 576; Percent complete: 11.5%; Average loss: 0.5790\n",
      "Iteration: 577; Percent complete: 11.5%; Average loss: 0.5310\n",
      "Iteration: 578; Percent complete: 11.6%; Average loss: 0.5909\n",
      "Iteration: 579; Percent complete: 11.6%; Average loss: 0.5353\n",
      "Iteration: 580; Percent complete: 11.6%; Average loss: 0.4920\n",
      "Iteration: 581; Percent complete: 11.6%; Average loss: 0.5745\n",
      "Iteration: 582; Percent complete: 11.6%; Average loss: 0.5208\n",
      "Iteration: 583; Percent complete: 11.7%; Average loss: 0.5676\n",
      "Iteration: 584; Percent complete: 11.7%; Average loss: 0.5845\n",
      "Iteration: 585; Percent complete: 11.7%; Average loss: 0.5069\n",
      "Iteration: 586; Percent complete: 11.7%; Average loss: 0.5346\n",
      "Iteration: 587; Percent complete: 11.7%; Average loss: 0.5848\n",
      "Iteration: 588; Percent complete: 11.8%; Average loss: 0.5389\n",
      "Iteration: 589; Percent complete: 11.8%; Average loss: 0.5106\n",
      "Iteration: 590; Percent complete: 11.8%; Average loss: 0.5960\n",
      "Iteration: 591; Percent complete: 11.8%; Average loss: 0.5608\n",
      "Iteration: 592; Percent complete: 11.8%; Average loss: 0.5144\n",
      "Iteration: 593; Percent complete: 11.9%; Average loss: 0.5040\n",
      "Iteration: 594; Percent complete: 11.9%; Average loss: 0.5431\n",
      "Iteration: 595; Percent complete: 11.9%; Average loss: 0.5226\n",
      "Iteration: 596; Percent complete: 11.9%; Average loss: 0.5369\n",
      "Iteration: 597; Percent complete: 11.9%; Average loss: 0.5129\n",
      "Iteration: 598; Percent complete: 12.0%; Average loss: 0.5898\n",
      "Iteration: 599; Percent complete: 12.0%; Average loss: 0.4881\n",
      "Iteration: 600; Percent complete: 12.0%; Average loss: 0.5027\n",
      "Iteration: 601; Percent complete: 12.0%; Average loss: 0.4466\n",
      "Iteration: 602; Percent complete: 12.0%; Average loss: 0.5319\n",
      "Iteration: 603; Percent complete: 12.1%; Average loss: 0.5286\n",
      "Iteration: 604; Percent complete: 12.1%; Average loss: 0.5426\n",
      "Iteration: 605; Percent complete: 12.1%; Average loss: 0.5033\n",
      "Iteration: 606; Percent complete: 12.1%; Average loss: 0.5265\n",
      "Iteration: 607; Percent complete: 12.1%; Average loss: 0.5169\n",
      "Iteration: 608; Percent complete: 12.2%; Average loss: 0.5011\n",
      "Iteration: 609; Percent complete: 12.2%; Average loss: 0.4995\n",
      "Iteration: 610; Percent complete: 12.2%; Average loss: 0.5410\n",
      "Iteration: 611; Percent complete: 12.2%; Average loss: 0.5119\n",
      "Iteration: 612; Percent complete: 12.2%; Average loss: 0.4720\n",
      "Iteration: 613; Percent complete: 12.3%; Average loss: 0.4849\n",
      "Iteration: 614; Percent complete: 12.3%; Average loss: 0.4415\n",
      "Iteration: 615; Percent complete: 12.3%; Average loss: 0.5254\n",
      "Iteration: 616; Percent complete: 12.3%; Average loss: 0.5110\n",
      "Iteration: 617; Percent complete: 12.3%; Average loss: 0.5895\n",
      "Iteration: 618; Percent complete: 12.4%; Average loss: 0.4854\n",
      "Iteration: 619; Percent complete: 12.4%; Average loss: 0.4822\n",
      "Iteration: 620; Percent complete: 12.4%; Average loss: 0.4560\n",
      "Iteration: 621; Percent complete: 12.4%; Average loss: 0.5776\n",
      "Iteration: 622; Percent complete: 12.4%; Average loss: 0.4892\n",
      "Iteration: 623; Percent complete: 12.5%; Average loss: 0.4409\n",
      "Iteration: 624; Percent complete: 12.5%; Average loss: 0.4506\n",
      "Iteration: 625; Percent complete: 12.5%; Average loss: 0.4827\n",
      "Iteration: 626; Percent complete: 12.5%; Average loss: 0.4959\n",
      "Iteration: 627; Percent complete: 12.5%; Average loss: 0.4578\n",
      "Iteration: 628; Percent complete: 12.6%; Average loss: 0.4868\n",
      "Iteration: 629; Percent complete: 12.6%; Average loss: 0.4836\n",
      "Iteration: 630; Percent complete: 12.6%; Average loss: 0.5044\n",
      "Iteration: 631; Percent complete: 12.6%; Average loss: 0.3909\n",
      "Iteration: 632; Percent complete: 12.6%; Average loss: 0.4751\n",
      "Iteration: 633; Percent complete: 12.7%; Average loss: 0.3968\n",
      "Iteration: 634; Percent complete: 12.7%; Average loss: 0.4924\n",
      "Iteration: 635; Percent complete: 12.7%; Average loss: 0.3931\n",
      "Iteration: 636; Percent complete: 12.7%; Average loss: 0.4554\n",
      "Iteration: 637; Percent complete: 12.7%; Average loss: 0.4078\n",
      "Iteration: 638; Percent complete: 12.8%; Average loss: 0.4220\n",
      "Iteration: 639; Percent complete: 12.8%; Average loss: 0.4133\n",
      "Iteration: 640; Percent complete: 12.8%; Average loss: 0.4686\n",
      "Iteration: 641; Percent complete: 12.8%; Average loss: 0.4571\n",
      "Iteration: 642; Percent complete: 12.8%; Average loss: 0.4463\n",
      "Iteration: 643; Percent complete: 12.9%; Average loss: 0.4471\n",
      "Iteration: 644; Percent complete: 12.9%; Average loss: 0.4348\n",
      "Iteration: 645; Percent complete: 12.9%; Average loss: 0.4082\n",
      "Iteration: 646; Percent complete: 12.9%; Average loss: 0.3760\n",
      "Iteration: 647; Percent complete: 12.9%; Average loss: 0.3366\n",
      "Iteration: 648; Percent complete: 13.0%; Average loss: 0.4606\n",
      "Iteration: 649; Percent complete: 13.0%; Average loss: 0.4338\n",
      "Iteration: 650; Percent complete: 13.0%; Average loss: 0.4864\n",
      "Iteration: 651; Percent complete: 13.0%; Average loss: 0.4351\n",
      "Iteration: 652; Percent complete: 13.0%; Average loss: 0.3729\n",
      "Iteration: 653; Percent complete: 13.1%; Average loss: 0.3679\n",
      "Iteration: 654; Percent complete: 13.1%; Average loss: 0.4396\n",
      "Iteration: 655; Percent complete: 13.1%; Average loss: 0.4214\n",
      "Iteration: 656; Percent complete: 13.1%; Average loss: 0.3906\n",
      "Iteration: 657; Percent complete: 13.1%; Average loss: 0.3930\n",
      "Iteration: 658; Percent complete: 13.2%; Average loss: 0.4053\n",
      "Iteration: 659; Percent complete: 13.2%; Average loss: 0.3735\n",
      "Iteration: 660; Percent complete: 13.2%; Average loss: 0.3687\n",
      "Iteration: 661; Percent complete: 13.2%; Average loss: 0.4520\n",
      "Iteration: 662; Percent complete: 13.2%; Average loss: 0.4184\n",
      "Iteration: 663; Percent complete: 13.3%; Average loss: 0.3822\n",
      "Iteration: 664; Percent complete: 13.3%; Average loss: 0.3850\n",
      "Iteration: 665; Percent complete: 13.3%; Average loss: 0.3803\n",
      "Iteration: 666; Percent complete: 13.3%; Average loss: 0.4078\n",
      "Iteration: 667; Percent complete: 13.3%; Average loss: 0.4667\n",
      "Iteration: 668; Percent complete: 13.4%; Average loss: 0.3738\n",
      "Iteration: 669; Percent complete: 13.4%; Average loss: 0.4023\n",
      "Iteration: 670; Percent complete: 13.4%; Average loss: 0.3707\n",
      "Iteration: 671; Percent complete: 13.4%; Average loss: 0.3713\n",
      "Iteration: 672; Percent complete: 13.4%; Average loss: 0.4269\n",
      "Iteration: 673; Percent complete: 13.5%; Average loss: 0.4092\n",
      "Iteration: 674; Percent complete: 13.5%; Average loss: 0.3330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 675; Percent complete: 13.5%; Average loss: 0.3550\n",
      "Iteration: 676; Percent complete: 13.5%; Average loss: 0.4073\n",
      "Iteration: 677; Percent complete: 13.5%; Average loss: 0.3874\n",
      "Iteration: 678; Percent complete: 13.6%; Average loss: 0.3290\n",
      "Iteration: 679; Percent complete: 13.6%; Average loss: 0.3941\n",
      "Iteration: 680; Percent complete: 13.6%; Average loss: 0.3368\n",
      "Iteration: 681; Percent complete: 13.6%; Average loss: 0.3416\n",
      "Iteration: 682; Percent complete: 13.6%; Average loss: 0.3603\n",
      "Iteration: 683; Percent complete: 13.7%; Average loss: 0.3420\n",
      "Iteration: 684; Percent complete: 13.7%; Average loss: 0.3526\n",
      "Iteration: 685; Percent complete: 13.7%; Average loss: 0.3211\n",
      "Iteration: 686; Percent complete: 13.7%; Average loss: 0.3082\n",
      "Iteration: 687; Percent complete: 13.7%; Average loss: 0.3706\n",
      "Iteration: 688; Percent complete: 13.8%; Average loss: 0.3771\n",
      "Iteration: 689; Percent complete: 13.8%; Average loss: 0.3551\n",
      "Iteration: 690; Percent complete: 13.8%; Average loss: 0.3642\n",
      "Iteration: 691; Percent complete: 13.8%; Average loss: 0.3432\n",
      "Iteration: 692; Percent complete: 13.8%; Average loss: 0.3527\n",
      "Iteration: 693; Percent complete: 13.9%; Average loss: 0.3512\n",
      "Iteration: 694; Percent complete: 13.9%; Average loss: 0.3134\n",
      "Iteration: 695; Percent complete: 13.9%; Average loss: 0.3501\n",
      "Iteration: 696; Percent complete: 13.9%; Average loss: 0.3385\n",
      "Iteration: 697; Percent complete: 13.9%; Average loss: 0.2889\n",
      "Iteration: 698; Percent complete: 14.0%; Average loss: 0.3362\n",
      "Iteration: 699; Percent complete: 14.0%; Average loss: 0.3335\n",
      "Iteration: 700; Percent complete: 14.0%; Average loss: 0.3194\n",
      "Iteration: 701; Percent complete: 14.0%; Average loss: 0.2963\n",
      "Iteration: 702; Percent complete: 14.0%; Average loss: 0.3352\n",
      "Iteration: 703; Percent complete: 14.1%; Average loss: 0.3512\n",
      "Iteration: 704; Percent complete: 14.1%; Average loss: 0.2853\n",
      "Iteration: 705; Percent complete: 14.1%; Average loss: 0.3236\n",
      "Iteration: 706; Percent complete: 14.1%; Average loss: 0.3041\n",
      "Iteration: 707; Percent complete: 14.1%; Average loss: 0.3404\n",
      "Iteration: 708; Percent complete: 14.2%; Average loss: 0.2950\n",
      "Iteration: 709; Percent complete: 14.2%; Average loss: 0.3032\n",
      "Iteration: 710; Percent complete: 14.2%; Average loss: 0.2677\n",
      "Iteration: 711; Percent complete: 14.2%; Average loss: 0.3358\n",
      "Iteration: 712; Percent complete: 14.2%; Average loss: 0.2812\n",
      "Iteration: 713; Percent complete: 14.3%; Average loss: 0.3114\n",
      "Iteration: 714; Percent complete: 14.3%; Average loss: 0.2567\n",
      "Iteration: 715; Percent complete: 14.3%; Average loss: 0.2900\n",
      "Iteration: 716; Percent complete: 14.3%; Average loss: 0.3113\n",
      "Iteration: 717; Percent complete: 14.3%; Average loss: 0.2893\n",
      "Iteration: 718; Percent complete: 14.4%; Average loss: 0.2657\n",
      "Iteration: 719; Percent complete: 14.4%; Average loss: 0.3080\n",
      "Iteration: 720; Percent complete: 14.4%; Average loss: 0.2830\n",
      "Iteration: 721; Percent complete: 14.4%; Average loss: 0.2594\n",
      "Iteration: 722; Percent complete: 14.4%; Average loss: 0.3069\n",
      "Iteration: 723; Percent complete: 14.5%; Average loss: 0.2636\n",
      "Iteration: 724; Percent complete: 14.5%; Average loss: 0.3107\n",
      "Iteration: 725; Percent complete: 14.5%; Average loss: 0.2776\n",
      "Iteration: 726; Percent complete: 14.5%; Average loss: 0.2942\n",
      "Iteration: 727; Percent complete: 14.5%; Average loss: 0.2602\n",
      "Iteration: 728; Percent complete: 14.6%; Average loss: 0.2488\n",
      "Iteration: 729; Percent complete: 14.6%; Average loss: 0.2396\n",
      "Iteration: 730; Percent complete: 14.6%; Average loss: 0.2927\n",
      "Iteration: 731; Percent complete: 14.6%; Average loss: 0.3119\n",
      "Iteration: 732; Percent complete: 14.6%; Average loss: 0.3112\n",
      "Iteration: 733; Percent complete: 14.7%; Average loss: 0.2821\n",
      "Iteration: 734; Percent complete: 14.7%; Average loss: 0.2575\n",
      "Iteration: 735; Percent complete: 14.7%; Average loss: 0.3315\n",
      "Iteration: 736; Percent complete: 14.7%; Average loss: 0.2969\n",
      "Iteration: 737; Percent complete: 14.7%; Average loss: 0.3308\n",
      "Iteration: 738; Percent complete: 14.8%; Average loss: 0.3222\n",
      "Iteration: 739; Percent complete: 14.8%; Average loss: 0.2765\n",
      "Iteration: 740; Percent complete: 14.8%; Average loss: 0.2409\n",
      "Iteration: 741; Percent complete: 14.8%; Average loss: 0.3018\n",
      "Iteration: 742; Percent complete: 14.8%; Average loss: 0.2335\n",
      "Iteration: 743; Percent complete: 14.9%; Average loss: 0.2988\n",
      "Iteration: 744; Percent complete: 14.9%; Average loss: 0.2740\n",
      "Iteration: 745; Percent complete: 14.9%; Average loss: 0.2790\n",
      "Iteration: 746; Percent complete: 14.9%; Average loss: 0.2876\n",
      "Iteration: 747; Percent complete: 14.9%; Average loss: 0.2949\n",
      "Iteration: 748; Percent complete: 15.0%; Average loss: 0.2255\n",
      "Iteration: 749; Percent complete: 15.0%; Average loss: 0.2862\n",
      "Iteration: 750; Percent complete: 15.0%; Average loss: 0.2399\n",
      "Iteration: 751; Percent complete: 15.0%; Average loss: 0.2490\n",
      "Iteration: 752; Percent complete: 15.0%; Average loss: 0.2648\n",
      "Iteration: 753; Percent complete: 15.1%; Average loss: 0.2233\n",
      "Iteration: 754; Percent complete: 15.1%; Average loss: 0.2925\n",
      "Iteration: 755; Percent complete: 15.1%; Average loss: 0.2616\n",
      "Iteration: 756; Percent complete: 15.1%; Average loss: 0.2628\n",
      "Iteration: 757; Percent complete: 15.1%; Average loss: 0.2060\n",
      "Iteration: 758; Percent complete: 15.2%; Average loss: 0.2497\n",
      "Iteration: 759; Percent complete: 15.2%; Average loss: 0.2548\n",
      "Iteration: 760; Percent complete: 15.2%; Average loss: 0.2739\n",
      "Iteration: 761; Percent complete: 15.2%; Average loss: 0.2132\n",
      "Iteration: 762; Percent complete: 15.2%; Average loss: 0.2588\n",
      "Iteration: 763; Percent complete: 15.3%; Average loss: 0.2178\n",
      "Iteration: 764; Percent complete: 15.3%; Average loss: 0.2601\n",
      "Iteration: 765; Percent complete: 15.3%; Average loss: 0.2684\n",
      "Iteration: 766; Percent complete: 15.3%; Average loss: 0.2363\n",
      "Iteration: 767; Percent complete: 15.3%; Average loss: 0.2088\n",
      "Iteration: 768; Percent complete: 15.4%; Average loss: 0.2214\n",
      "Iteration: 769; Percent complete: 15.4%; Average loss: 0.2246\n",
      "Iteration: 770; Percent complete: 15.4%; Average loss: 0.2205\n",
      "Iteration: 771; Percent complete: 15.4%; Average loss: 0.2294\n",
      "Iteration: 772; Percent complete: 15.4%; Average loss: 0.2366\n",
      "Iteration: 773; Percent complete: 15.5%; Average loss: 0.2286\n",
      "Iteration: 774; Percent complete: 15.5%; Average loss: 0.2029\n",
      "Iteration: 775; Percent complete: 15.5%; Average loss: 0.2303\n",
      "Iteration: 776; Percent complete: 15.5%; Average loss: 0.2537\n",
      "Iteration: 777; Percent complete: 15.5%; Average loss: 0.2315\n",
      "Iteration: 778; Percent complete: 15.6%; Average loss: 0.1915\n",
      "Iteration: 779; Percent complete: 15.6%; Average loss: 0.2317\n",
      "Iteration: 780; Percent complete: 15.6%; Average loss: 0.2279\n",
      "Iteration: 781; Percent complete: 15.6%; Average loss: 0.2039\n",
      "Iteration: 782; Percent complete: 15.6%; Average loss: 0.2395\n",
      "Iteration: 783; Percent complete: 15.7%; Average loss: 0.2244\n",
      "Iteration: 784; Percent complete: 15.7%; Average loss: 0.2224\n",
      "Iteration: 785; Percent complete: 15.7%; Average loss: 0.2257\n",
      "Iteration: 786; Percent complete: 15.7%; Average loss: 0.2256\n",
      "Iteration: 787; Percent complete: 15.7%; Average loss: 0.2013\n",
      "Iteration: 788; Percent complete: 15.8%; Average loss: 0.2320\n",
      "Iteration: 789; Percent complete: 15.8%; Average loss: 0.2377\n",
      "Iteration: 790; Percent complete: 15.8%; Average loss: 0.2146\n",
      "Iteration: 791; Percent complete: 15.8%; Average loss: 0.1991\n",
      "Iteration: 792; Percent complete: 15.8%; Average loss: 0.1792\n",
      "Iteration: 793; Percent complete: 15.9%; Average loss: 0.2288\n",
      "Iteration: 794; Percent complete: 15.9%; Average loss: 0.2077\n",
      "Iteration: 795; Percent complete: 15.9%; Average loss: 0.1953\n",
      "Iteration: 796; Percent complete: 15.9%; Average loss: 0.1918\n",
      "Iteration: 797; Percent complete: 15.9%; Average loss: 0.2124\n",
      "Iteration: 798; Percent complete: 16.0%; Average loss: 0.1879\n",
      "Iteration: 799; Percent complete: 16.0%; Average loss: 0.1739\n",
      "Iteration: 800; Percent complete: 16.0%; Average loss: 0.2169\n",
      "Iteration: 801; Percent complete: 16.0%; Average loss: 0.2058\n",
      "Iteration: 802; Percent complete: 16.0%; Average loss: 0.1882\n",
      "Iteration: 803; Percent complete: 16.1%; Average loss: 0.1724\n",
      "Iteration: 804; Percent complete: 16.1%; Average loss: 0.2025\n",
      "Iteration: 805; Percent complete: 16.1%; Average loss: 0.1849\n",
      "Iteration: 806; Percent complete: 16.1%; Average loss: 0.2086\n",
      "Iteration: 807; Percent complete: 16.1%; Average loss: 0.2016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 808; Percent complete: 16.2%; Average loss: 0.1791\n",
      "Iteration: 809; Percent complete: 16.2%; Average loss: 0.1955\n",
      "Iteration: 810; Percent complete: 16.2%; Average loss: 0.2024\n",
      "Iteration: 811; Percent complete: 16.2%; Average loss: 0.1701\n",
      "Iteration: 812; Percent complete: 16.2%; Average loss: 0.1996\n",
      "Iteration: 813; Percent complete: 16.3%; Average loss: 0.1918\n",
      "Iteration: 814; Percent complete: 16.3%; Average loss: 0.1828\n",
      "Iteration: 815; Percent complete: 16.3%; Average loss: 0.2110\n",
      "Iteration: 816; Percent complete: 16.3%; Average loss: 0.2051\n",
      "Iteration: 817; Percent complete: 16.3%; Average loss: 0.1997\n",
      "Iteration: 818; Percent complete: 16.4%; Average loss: 0.1749\n",
      "Iteration: 819; Percent complete: 16.4%; Average loss: 0.1855\n",
      "Iteration: 820; Percent complete: 16.4%; Average loss: 0.1838\n",
      "Iteration: 821; Percent complete: 16.4%; Average loss: 0.1752\n",
      "Iteration: 822; Percent complete: 16.4%; Average loss: 0.1855\n",
      "Iteration: 823; Percent complete: 16.5%; Average loss: 0.1801\n",
      "Iteration: 824; Percent complete: 16.5%; Average loss: 0.1903\n",
      "Iteration: 825; Percent complete: 16.5%; Average loss: 0.1584\n",
      "Iteration: 826; Percent complete: 16.5%; Average loss: 0.1610\n",
      "Iteration: 827; Percent complete: 16.5%; Average loss: 0.1795\n",
      "Iteration: 828; Percent complete: 16.6%; Average loss: 0.1830\n",
      "Iteration: 829; Percent complete: 16.6%; Average loss: 0.1740\n",
      "Iteration: 830; Percent complete: 16.6%; Average loss: 0.1836\n",
      "Iteration: 831; Percent complete: 16.6%; Average loss: 0.1656\n",
      "Iteration: 832; Percent complete: 16.6%; Average loss: 0.1476\n",
      "Iteration: 833; Percent complete: 16.7%; Average loss: 0.1808\n",
      "Iteration: 834; Percent complete: 16.7%; Average loss: 0.2088\n",
      "Iteration: 835; Percent complete: 16.7%; Average loss: 0.1747\n",
      "Iteration: 836; Percent complete: 16.7%; Average loss: 0.1652\n",
      "Iteration: 837; Percent complete: 16.7%; Average loss: 0.1620\n",
      "Iteration: 838; Percent complete: 16.8%; Average loss: 0.1645\n",
      "Iteration: 839; Percent complete: 16.8%; Average loss: 0.1748\n",
      "Iteration: 840; Percent complete: 16.8%; Average loss: 0.1567\n",
      "Iteration: 841; Percent complete: 16.8%; Average loss: 0.1694\n",
      "Iteration: 842; Percent complete: 16.8%; Average loss: 0.1586\n",
      "Iteration: 843; Percent complete: 16.9%; Average loss: 0.1781\n",
      "Iteration: 844; Percent complete: 16.9%; Average loss: 0.1710\n",
      "Iteration: 845; Percent complete: 16.9%; Average loss: 0.1536\n",
      "Iteration: 846; Percent complete: 16.9%; Average loss: 0.1578\n",
      "Iteration: 847; Percent complete: 16.9%; Average loss: 0.1524\n",
      "Iteration: 848; Percent complete: 17.0%; Average loss: 0.1634\n",
      "Iteration: 849; Percent complete: 17.0%; Average loss: 0.1748\n",
      "Iteration: 850; Percent complete: 17.0%; Average loss: 0.1538\n",
      "Iteration: 851; Percent complete: 17.0%; Average loss: 0.1743\n",
      "Iteration: 852; Percent complete: 17.0%; Average loss: 0.1787\n",
      "Iteration: 853; Percent complete: 17.1%; Average loss: 0.1794\n",
      "Iteration: 854; Percent complete: 17.1%; Average loss: 0.1893\n",
      "Iteration: 855; Percent complete: 17.1%; Average loss: 0.1805\n",
      "Iteration: 856; Percent complete: 17.1%; Average loss: 0.1671\n",
      "Iteration: 857; Percent complete: 17.1%; Average loss: 0.1679\n",
      "Iteration: 858; Percent complete: 17.2%; Average loss: 0.1530\n",
      "Iteration: 859; Percent complete: 17.2%; Average loss: 0.1702\n",
      "Iteration: 860; Percent complete: 17.2%; Average loss: 0.1471\n",
      "Iteration: 861; Percent complete: 17.2%; Average loss: 0.1598\n",
      "Iteration: 862; Percent complete: 17.2%; Average loss: 0.1699\n",
      "Iteration: 863; Percent complete: 17.3%; Average loss: 0.1683\n",
      "Iteration: 864; Percent complete: 17.3%; Average loss: 0.1399\n",
      "Iteration: 865; Percent complete: 17.3%; Average loss: 0.1617\n",
      "Iteration: 866; Percent complete: 17.3%; Average loss: 0.1601\n",
      "Iteration: 867; Percent complete: 17.3%; Average loss: 0.1686\n",
      "Iteration: 868; Percent complete: 17.4%; Average loss: 0.1678\n",
      "Iteration: 869; Percent complete: 17.4%; Average loss: 0.1506\n",
      "Iteration: 870; Percent complete: 17.4%; Average loss: 0.1330\n",
      "Iteration: 871; Percent complete: 17.4%; Average loss: 0.1587\n",
      "Iteration: 872; Percent complete: 17.4%; Average loss: 0.1719\n",
      "Iteration: 873; Percent complete: 17.5%; Average loss: 0.1540\n",
      "Iteration: 874; Percent complete: 17.5%; Average loss: 0.1500\n",
      "Iteration: 875; Percent complete: 17.5%; Average loss: 0.1424\n",
      "Iteration: 876; Percent complete: 17.5%; Average loss: 0.1207\n",
      "Iteration: 877; Percent complete: 17.5%; Average loss: 0.1630\n",
      "Iteration: 878; Percent complete: 17.6%; Average loss: 0.1570\n",
      "Iteration: 879; Percent complete: 17.6%; Average loss: 0.1268\n",
      "Iteration: 880; Percent complete: 17.6%; Average loss: 0.1514\n",
      "Iteration: 881; Percent complete: 17.6%; Average loss: 0.1519\n",
      "Iteration: 882; Percent complete: 17.6%; Average loss: 0.1424\n",
      "Iteration: 883; Percent complete: 17.7%; Average loss: 0.1168\n",
      "Iteration: 884; Percent complete: 17.7%; Average loss: 0.1390\n",
      "Iteration: 885; Percent complete: 17.7%; Average loss: 0.1442\n",
      "Iteration: 886; Percent complete: 17.7%; Average loss: 0.1410\n",
      "Iteration: 887; Percent complete: 17.7%; Average loss: 0.1577\n",
      "Iteration: 888; Percent complete: 17.8%; Average loss: 0.1400\n",
      "Iteration: 889; Percent complete: 17.8%; Average loss: 0.1649\n",
      "Iteration: 890; Percent complete: 17.8%; Average loss: 0.1520\n",
      "Iteration: 891; Percent complete: 17.8%; Average loss: 0.1174\n",
      "Iteration: 892; Percent complete: 17.8%; Average loss: 0.1386\n",
      "Iteration: 893; Percent complete: 17.9%; Average loss: 0.1379\n",
      "Iteration: 894; Percent complete: 17.9%; Average loss: 0.1209\n",
      "Iteration: 895; Percent complete: 17.9%; Average loss: 0.1107\n",
      "Iteration: 896; Percent complete: 17.9%; Average loss: 0.1227\n",
      "Iteration: 897; Percent complete: 17.9%; Average loss: 0.1193\n",
      "Iteration: 898; Percent complete: 18.0%; Average loss: 0.1364\n",
      "Iteration: 899; Percent complete: 18.0%; Average loss: 0.1261\n",
      "Iteration: 900; Percent complete: 18.0%; Average loss: 0.1256\n",
      "Iteration: 901; Percent complete: 18.0%; Average loss: 0.1435\n",
      "Iteration: 902; Percent complete: 18.0%; Average loss: 0.1417\n",
      "Iteration: 903; Percent complete: 18.1%; Average loss: 0.1286\n",
      "Iteration: 904; Percent complete: 18.1%; Average loss: 0.1212\n",
      "Iteration: 905; Percent complete: 18.1%; Average loss: 0.1202\n",
      "Iteration: 906; Percent complete: 18.1%; Average loss: 0.1316\n",
      "Iteration: 907; Percent complete: 18.1%; Average loss: 0.1215\n",
      "Iteration: 908; Percent complete: 18.2%; Average loss: 0.1224\n",
      "Iteration: 909; Percent complete: 18.2%; Average loss: 0.1383\n",
      "Iteration: 910; Percent complete: 18.2%; Average loss: 0.1179\n",
      "Iteration: 911; Percent complete: 18.2%; Average loss: 0.1423\n",
      "Iteration: 912; Percent complete: 18.2%; Average loss: 0.1254\n",
      "Iteration: 913; Percent complete: 18.3%; Average loss: 0.1220\n",
      "Iteration: 914; Percent complete: 18.3%; Average loss: 0.1202\n",
      "Iteration: 915; Percent complete: 18.3%; Average loss: 0.1098\n",
      "Iteration: 916; Percent complete: 18.3%; Average loss: 0.1156\n",
      "Iteration: 917; Percent complete: 18.3%; Average loss: 0.1110\n",
      "Iteration: 918; Percent complete: 18.4%; Average loss: 0.1231\n",
      "Iteration: 919; Percent complete: 18.4%; Average loss: 0.1197\n",
      "Iteration: 920; Percent complete: 18.4%; Average loss: 0.1223\n",
      "Iteration: 921; Percent complete: 18.4%; Average loss: 0.1134\n",
      "Iteration: 922; Percent complete: 18.4%; Average loss: 0.1143\n",
      "Iteration: 923; Percent complete: 18.5%; Average loss: 0.1227\n",
      "Iteration: 924; Percent complete: 18.5%; Average loss: 0.0961\n",
      "Iteration: 925; Percent complete: 18.5%; Average loss: 0.1247\n",
      "Iteration: 926; Percent complete: 18.5%; Average loss: 0.1113\n",
      "Iteration: 927; Percent complete: 18.5%; Average loss: 0.1126\n",
      "Iteration: 928; Percent complete: 18.6%; Average loss: 0.1202\n",
      "Iteration: 929; Percent complete: 18.6%; Average loss: 0.1024\n",
      "Iteration: 930; Percent complete: 18.6%; Average loss: 0.0981\n",
      "Iteration: 931; Percent complete: 18.6%; Average loss: 0.1241\n",
      "Iteration: 932; Percent complete: 18.6%; Average loss: 0.0984\n",
      "Iteration: 933; Percent complete: 18.7%; Average loss: 0.1128\n",
      "Iteration: 934; Percent complete: 18.7%; Average loss: 0.1174\n",
      "Iteration: 935; Percent complete: 18.7%; Average loss: 0.1074\n",
      "Iteration: 936; Percent complete: 18.7%; Average loss: 0.1022\n",
      "Iteration: 937; Percent complete: 18.7%; Average loss: 0.1009\n",
      "Iteration: 938; Percent complete: 18.8%; Average loss: 0.1034\n",
      "Iteration: 939; Percent complete: 18.8%; Average loss: 0.1028\n",
      "Iteration: 940; Percent complete: 18.8%; Average loss: 0.1116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 941; Percent complete: 18.8%; Average loss: 0.0981\n",
      "Iteration: 942; Percent complete: 18.8%; Average loss: 0.1032\n",
      "Iteration: 943; Percent complete: 18.9%; Average loss: 0.1111\n",
      "Iteration: 944; Percent complete: 18.9%; Average loss: 0.1244\n",
      "Iteration: 945; Percent complete: 18.9%; Average loss: 0.1160\n",
      "Iteration: 946; Percent complete: 18.9%; Average loss: 0.1297\n",
      "Iteration: 947; Percent complete: 18.9%; Average loss: 0.1085\n",
      "Iteration: 948; Percent complete: 19.0%; Average loss: 0.1094\n",
      "Iteration: 949; Percent complete: 19.0%; Average loss: 0.1138\n",
      "Iteration: 950; Percent complete: 19.0%; Average loss: 0.1046\n",
      "Iteration: 951; Percent complete: 19.0%; Average loss: 0.0968\n",
      "Iteration: 952; Percent complete: 19.0%; Average loss: 0.1185\n",
      "Iteration: 953; Percent complete: 19.1%; Average loss: 0.1019\n",
      "Iteration: 954; Percent complete: 19.1%; Average loss: 0.1176\n",
      "Iteration: 955; Percent complete: 19.1%; Average loss: 0.0947\n",
      "Iteration: 956; Percent complete: 19.1%; Average loss: 0.1077\n",
      "Iteration: 957; Percent complete: 19.1%; Average loss: 0.0937\n",
      "Iteration: 958; Percent complete: 19.2%; Average loss: 0.1060\n",
      "Iteration: 959; Percent complete: 19.2%; Average loss: 0.1082\n",
      "Iteration: 960; Percent complete: 19.2%; Average loss: 0.0880\n",
      "Iteration: 961; Percent complete: 19.2%; Average loss: 0.0968\n",
      "Iteration: 962; Percent complete: 19.2%; Average loss: 0.0899\n",
      "Iteration: 963; Percent complete: 19.3%; Average loss: 0.0922\n",
      "Iteration: 964; Percent complete: 19.3%; Average loss: 0.1029\n",
      "Iteration: 965; Percent complete: 19.3%; Average loss: 0.1048\n",
      "Iteration: 966; Percent complete: 19.3%; Average loss: 0.0878\n",
      "Iteration: 967; Percent complete: 19.3%; Average loss: 0.0947\n",
      "Iteration: 968; Percent complete: 19.4%; Average loss: 0.0931\n",
      "Iteration: 969; Percent complete: 19.4%; Average loss: 0.0979\n",
      "Iteration: 970; Percent complete: 19.4%; Average loss: 0.0831\n",
      "Iteration: 971; Percent complete: 19.4%; Average loss: 0.0999\n",
      "Iteration: 972; Percent complete: 19.4%; Average loss: 0.1151\n",
      "Iteration: 973; Percent complete: 19.5%; Average loss: 0.0986\n",
      "Iteration: 974; Percent complete: 19.5%; Average loss: 0.0976\n",
      "Iteration: 975; Percent complete: 19.5%; Average loss: 0.1041\n",
      "Iteration: 976; Percent complete: 19.5%; Average loss: 0.0953\n",
      "Iteration: 977; Percent complete: 19.5%; Average loss: 0.0909\n",
      "Iteration: 978; Percent complete: 19.6%; Average loss: 0.0874\n",
      "Iteration: 979; Percent complete: 19.6%; Average loss: 0.0988\n",
      "Iteration: 980; Percent complete: 19.6%; Average loss: 0.0874\n",
      "Iteration: 981; Percent complete: 19.6%; Average loss: 0.0976\n",
      "Iteration: 982; Percent complete: 19.6%; Average loss: 0.0882\n",
      "Iteration: 983; Percent complete: 19.7%; Average loss: 0.0784\n",
      "Iteration: 984; Percent complete: 19.7%; Average loss: 0.1055\n",
      "Iteration: 985; Percent complete: 19.7%; Average loss: 0.0902\n",
      "Iteration: 986; Percent complete: 19.7%; Average loss: 0.0889\n",
      "Iteration: 987; Percent complete: 19.7%; Average loss: 0.1059\n",
      "Iteration: 988; Percent complete: 19.8%; Average loss: 0.0899\n",
      "Iteration: 989; Percent complete: 19.8%; Average loss: 0.1093\n",
      "Iteration: 990; Percent complete: 19.8%; Average loss: 0.1035\n",
      "Iteration: 991; Percent complete: 19.8%; Average loss: 0.0847\n",
      "Iteration: 992; Percent complete: 19.8%; Average loss: 0.0890\n",
      "Iteration: 993; Percent complete: 19.9%; Average loss: 0.0800\n",
      "Iteration: 994; Percent complete: 19.9%; Average loss: 0.0988\n",
      "Iteration: 995; Percent complete: 19.9%; Average loss: 0.0842\n",
      "Iteration: 996; Percent complete: 19.9%; Average loss: 0.0785\n",
      "Iteration: 997; Percent complete: 19.9%; Average loss: 0.0861\n",
      "Iteration: 998; Percent complete: 20.0%; Average loss: 0.1047\n",
      "Iteration: 999; Percent complete: 20.0%; Average loss: 0.1078\n",
      "Iteration: 1000; Percent complete: 20.0%; Average loss: 0.0961\n",
      "Iteration: 1001; Percent complete: 20.0%; Average loss: 0.0826\n",
      "Iteration: 1002; Percent complete: 20.0%; Average loss: 0.0732\n",
      "Iteration: 1003; Percent complete: 20.1%; Average loss: 0.0745\n",
      "Iteration: 1004; Percent complete: 20.1%; Average loss: 0.0867\n",
      "Iteration: 1005; Percent complete: 20.1%; Average loss: 0.0940\n",
      "Iteration: 1006; Percent complete: 20.1%; Average loss: 0.1018\n",
      "Iteration: 1007; Percent complete: 20.1%; Average loss: 0.0723\n",
      "Iteration: 1008; Percent complete: 20.2%; Average loss: 0.0889\n",
      "Iteration: 1009; Percent complete: 20.2%; Average loss: 0.0756\n",
      "Iteration: 1010; Percent complete: 20.2%; Average loss: 0.0848\n",
      "Iteration: 1011; Percent complete: 20.2%; Average loss: 0.0827\n",
      "Iteration: 1012; Percent complete: 20.2%; Average loss: 0.0895\n",
      "Iteration: 1013; Percent complete: 20.3%; Average loss: 0.0808\n",
      "Iteration: 1014; Percent complete: 20.3%; Average loss: 0.0783\n",
      "Iteration: 1015; Percent complete: 20.3%; Average loss: 0.0955\n",
      "Iteration: 1016; Percent complete: 20.3%; Average loss: 0.0886\n",
      "Iteration: 1017; Percent complete: 20.3%; Average loss: 0.0717\n",
      "Iteration: 1018; Percent complete: 20.4%; Average loss: 0.0857\n",
      "Iteration: 1019; Percent complete: 20.4%; Average loss: 0.0834\n",
      "Iteration: 1020; Percent complete: 20.4%; Average loss: 0.0678\n",
      "Iteration: 1021; Percent complete: 20.4%; Average loss: 0.0931\n",
      "Iteration: 1022; Percent complete: 20.4%; Average loss: 0.0706\n",
      "Iteration: 1023; Percent complete: 20.5%; Average loss: 0.0754\n",
      "Iteration: 1024; Percent complete: 20.5%; Average loss: 0.0808\n",
      "Iteration: 1025; Percent complete: 20.5%; Average loss: 0.0692\n",
      "Iteration: 1026; Percent complete: 20.5%; Average loss: 0.0914\n",
      "Iteration: 1027; Percent complete: 20.5%; Average loss: 0.0705\n",
      "Iteration: 1028; Percent complete: 20.6%; Average loss: 0.0774\n",
      "Iteration: 1029; Percent complete: 20.6%; Average loss: 0.0760\n",
      "Iteration: 1030; Percent complete: 20.6%; Average loss: 0.0715\n",
      "Iteration: 1031; Percent complete: 20.6%; Average loss: 0.0784\n",
      "Iteration: 1032; Percent complete: 20.6%; Average loss: 0.0809\n",
      "Iteration: 1033; Percent complete: 20.7%; Average loss: 0.0947\n",
      "Iteration: 1034; Percent complete: 20.7%; Average loss: 0.0712\n",
      "Iteration: 1035; Percent complete: 20.7%; Average loss: 0.0784\n",
      "Iteration: 1036; Percent complete: 20.7%; Average loss: 0.0761\n",
      "Iteration: 1037; Percent complete: 20.7%; Average loss: 0.0726\n",
      "Iteration: 1038; Percent complete: 20.8%; Average loss: 0.0697\n",
      "Iteration: 1039; Percent complete: 20.8%; Average loss: 0.0739\n",
      "Iteration: 1040; Percent complete: 20.8%; Average loss: 0.0676\n",
      "Iteration: 1041; Percent complete: 20.8%; Average loss: 0.0605\n",
      "Iteration: 1042; Percent complete: 20.8%; Average loss: 0.0647\n",
      "Iteration: 1043; Percent complete: 20.9%; Average loss: 0.0631\n",
      "Iteration: 1044; Percent complete: 20.9%; Average loss: 0.0737\n",
      "Iteration: 1045; Percent complete: 20.9%; Average loss: 0.0717\n",
      "Iteration: 1046; Percent complete: 20.9%; Average loss: 0.0740\n",
      "Iteration: 1047; Percent complete: 20.9%; Average loss: 0.0745\n",
      "Iteration: 1048; Percent complete: 21.0%; Average loss: 0.0735\n",
      "Iteration: 1049; Percent complete: 21.0%; Average loss: 0.0676\n",
      "Iteration: 1050; Percent complete: 21.0%; Average loss: 0.0660\n",
      "Iteration: 1051; Percent complete: 21.0%; Average loss: 0.0696\n",
      "Iteration: 1052; Percent complete: 21.0%; Average loss: 0.0748\n",
      "Iteration: 1053; Percent complete: 21.1%; Average loss: 0.0631\n",
      "Iteration: 1054; Percent complete: 21.1%; Average loss: 0.0815\n",
      "Iteration: 1055; Percent complete: 21.1%; Average loss: 0.0712\n",
      "Iteration: 1056; Percent complete: 21.1%; Average loss: 0.0842\n",
      "Iteration: 1057; Percent complete: 21.1%; Average loss: 0.0757\n",
      "Iteration: 1058; Percent complete: 21.2%; Average loss: 0.0656\n",
      "Iteration: 1059; Percent complete: 21.2%; Average loss: 0.0699\n",
      "Iteration: 1060; Percent complete: 21.2%; Average loss: 0.0698\n",
      "Iteration: 1061; Percent complete: 21.2%; Average loss: 0.0637\n",
      "Iteration: 1062; Percent complete: 21.2%; Average loss: 0.0730\n",
      "Iteration: 1063; Percent complete: 21.3%; Average loss: 0.0823\n",
      "Iteration: 1064; Percent complete: 21.3%; Average loss: 0.0785\n",
      "Iteration: 1065; Percent complete: 21.3%; Average loss: 0.0722\n",
      "Iteration: 1066; Percent complete: 21.3%; Average loss: 0.0779\n",
      "Iteration: 1067; Percent complete: 21.3%; Average loss: 0.0875\n",
      "Iteration: 1068; Percent complete: 21.4%; Average loss: 0.0593\n",
      "Iteration: 1069; Percent complete: 21.4%; Average loss: 0.0608\n",
      "Iteration: 1070; Percent complete: 21.4%; Average loss: 0.0604\n",
      "Iteration: 1071; Percent complete: 21.4%; Average loss: 0.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1072; Percent complete: 21.4%; Average loss: 0.0666\n",
      "Iteration: 1073; Percent complete: 21.5%; Average loss: 0.0525\n",
      "Iteration: 1074; Percent complete: 21.5%; Average loss: 0.0622\n",
      "Iteration: 1075; Percent complete: 21.5%; Average loss: 0.0685\n",
      "Iteration: 1076; Percent complete: 21.5%; Average loss: 0.0590\n",
      "Iteration: 1077; Percent complete: 21.5%; Average loss: 0.0761\n",
      "Iteration: 1078; Percent complete: 21.6%; Average loss: 0.0741\n",
      "Iteration: 1079; Percent complete: 21.6%; Average loss: 0.0727\n",
      "Iteration: 1080; Percent complete: 21.6%; Average loss: 0.0678\n",
      "Iteration: 1081; Percent complete: 21.6%; Average loss: 0.0556\n",
      "Iteration: 1082; Percent complete: 21.6%; Average loss: 0.0667\n",
      "Iteration: 1083; Percent complete: 21.7%; Average loss: 0.0652\n",
      "Iteration: 1084; Percent complete: 21.7%; Average loss: 0.0666\n",
      "Iteration: 1085; Percent complete: 21.7%; Average loss: 0.0635\n",
      "Iteration: 1086; Percent complete: 21.7%; Average loss: 0.0706\n",
      "Iteration: 1087; Percent complete: 21.7%; Average loss: 0.0623\n",
      "Iteration: 1088; Percent complete: 21.8%; Average loss: 0.0679\n",
      "Iteration: 1089; Percent complete: 21.8%; Average loss: 0.0638\n",
      "Iteration: 1090; Percent complete: 21.8%; Average loss: 0.0557\n",
      "Iteration: 1091; Percent complete: 21.8%; Average loss: 0.0625\n",
      "Iteration: 1092; Percent complete: 21.8%; Average loss: 0.0629\n",
      "Iteration: 1093; Percent complete: 21.9%; Average loss: 0.0650\n",
      "Iteration: 1094; Percent complete: 21.9%; Average loss: 0.0649\n",
      "Iteration: 1095; Percent complete: 21.9%; Average loss: 0.0505\n",
      "Iteration: 1096; Percent complete: 21.9%; Average loss: 0.0622\n",
      "Iteration: 1097; Percent complete: 21.9%; Average loss: 0.0614\n",
      "Iteration: 1098; Percent complete: 22.0%; Average loss: 0.0665\n",
      "Iteration: 1099; Percent complete: 22.0%; Average loss: 0.0748\n",
      "Iteration: 1100; Percent complete: 22.0%; Average loss: 0.0482\n",
      "Iteration: 1101; Percent complete: 22.0%; Average loss: 0.0624\n",
      "Iteration: 1102; Percent complete: 22.0%; Average loss: 0.0572\n",
      "Iteration: 1103; Percent complete: 22.1%; Average loss: 0.0475\n",
      "Iteration: 1104; Percent complete: 22.1%; Average loss: 0.0574\n",
      "Iteration: 1105; Percent complete: 22.1%; Average loss: 0.0643\n",
      "Iteration: 1106; Percent complete: 22.1%; Average loss: 0.0607\n",
      "Iteration: 1107; Percent complete: 22.1%; Average loss: 0.0530\n",
      "Iteration: 1108; Percent complete: 22.2%; Average loss: 0.0637\n",
      "Iteration: 1109; Percent complete: 22.2%; Average loss: 0.0583\n",
      "Iteration: 1110; Percent complete: 22.2%; Average loss: 0.0596\n",
      "Iteration: 1111; Percent complete: 22.2%; Average loss: 0.0553\n",
      "Iteration: 1112; Percent complete: 22.2%; Average loss: 0.0575\n",
      "Iteration: 1113; Percent complete: 22.3%; Average loss: 0.0554\n",
      "Iteration: 1114; Percent complete: 22.3%; Average loss: 0.0593\n",
      "Iteration: 1115; Percent complete: 22.3%; Average loss: 0.0430\n",
      "Iteration: 1116; Percent complete: 22.3%; Average loss: 0.0755\n",
      "Iteration: 1117; Percent complete: 22.3%; Average loss: 0.0523\n",
      "Iteration: 1118; Percent complete: 22.4%; Average loss: 0.0540\n",
      "Iteration: 1119; Percent complete: 22.4%; Average loss: 0.0524\n",
      "Iteration: 1120; Percent complete: 22.4%; Average loss: 0.0440\n",
      "Iteration: 1121; Percent complete: 22.4%; Average loss: 0.0547\n",
      "Iteration: 1122; Percent complete: 22.4%; Average loss: 0.0501\n",
      "Iteration: 1123; Percent complete: 22.5%; Average loss: 0.0801\n",
      "Iteration: 1124; Percent complete: 22.5%; Average loss: 0.0578\n",
      "Iteration: 1125; Percent complete: 22.5%; Average loss: 0.0445\n",
      "Iteration: 1126; Percent complete: 22.5%; Average loss: 0.0512\n",
      "Iteration: 1127; Percent complete: 22.5%; Average loss: 0.0611\n",
      "Iteration: 1128; Percent complete: 22.6%; Average loss: 0.0634\n",
      "Iteration: 1129; Percent complete: 22.6%; Average loss: 0.0506\n",
      "Iteration: 1130; Percent complete: 22.6%; Average loss: 0.0625\n",
      "Iteration: 1131; Percent complete: 22.6%; Average loss: 0.0621\n",
      "Iteration: 1132; Percent complete: 22.6%; Average loss: 0.0560\n",
      "Iteration: 1133; Percent complete: 22.7%; Average loss: 0.0522\n",
      "Iteration: 1134; Percent complete: 22.7%; Average loss: 0.0556\n",
      "Iteration: 1135; Percent complete: 22.7%; Average loss: 0.0545\n",
      "Iteration: 1136; Percent complete: 22.7%; Average loss: 0.0668\n",
      "Iteration: 1137; Percent complete: 22.7%; Average loss: 0.0586\n",
      "Iteration: 1138; Percent complete: 22.8%; Average loss: 0.0511\n",
      "Iteration: 1139; Percent complete: 22.8%; Average loss: 0.0585\n",
      "Iteration: 1140; Percent complete: 22.8%; Average loss: 0.0492\n",
      "Iteration: 1141; Percent complete: 22.8%; Average loss: 0.0549\n",
      "Iteration: 1142; Percent complete: 22.8%; Average loss: 0.0577\n",
      "Iteration: 1143; Percent complete: 22.9%; Average loss: 0.0600\n",
      "Iteration: 1144; Percent complete: 22.9%; Average loss: 0.0519\n",
      "Iteration: 1145; Percent complete: 22.9%; Average loss: 0.0578\n",
      "Iteration: 1146; Percent complete: 22.9%; Average loss: 0.0575\n",
      "Iteration: 1147; Percent complete: 22.9%; Average loss: 0.0486\n",
      "Iteration: 1148; Percent complete: 23.0%; Average loss: 0.0514\n",
      "Iteration: 1149; Percent complete: 23.0%; Average loss: 0.0593\n",
      "Iteration: 1150; Percent complete: 23.0%; Average loss: 0.0455\n",
      "Iteration: 1151; Percent complete: 23.0%; Average loss: 0.0543\n",
      "Iteration: 1152; Percent complete: 23.0%; Average loss: 0.0525\n",
      "Iteration: 1153; Percent complete: 23.1%; Average loss: 0.0457\n",
      "Iteration: 1154; Percent complete: 23.1%; Average loss: 0.0535\n",
      "Iteration: 1155; Percent complete: 23.1%; Average loss: 0.0519\n",
      "Iteration: 1156; Percent complete: 23.1%; Average loss: 0.0490\n",
      "Iteration: 1157; Percent complete: 23.1%; Average loss: 0.0516\n",
      "Iteration: 1158; Percent complete: 23.2%; Average loss: 0.0528\n",
      "Iteration: 1159; Percent complete: 23.2%; Average loss: 0.0520\n",
      "Iteration: 1160; Percent complete: 23.2%; Average loss: 0.0489\n",
      "Iteration: 1161; Percent complete: 23.2%; Average loss: 0.0456\n",
      "Iteration: 1162; Percent complete: 23.2%; Average loss: 0.0581\n",
      "Iteration: 1163; Percent complete: 23.3%; Average loss: 0.0674\n",
      "Iteration: 1164; Percent complete: 23.3%; Average loss: 0.0438\n",
      "Iteration: 1165; Percent complete: 23.3%; Average loss: 0.0588\n",
      "Iteration: 1166; Percent complete: 23.3%; Average loss: 0.0443\n",
      "Iteration: 1167; Percent complete: 23.3%; Average loss: 0.0587\n",
      "Iteration: 1168; Percent complete: 23.4%; Average loss: 0.0530\n",
      "Iteration: 1169; Percent complete: 23.4%; Average loss: 0.0589\n",
      "Iteration: 1170; Percent complete: 23.4%; Average loss: 0.0573\n",
      "Iteration: 1171; Percent complete: 23.4%; Average loss: 0.0594\n",
      "Iteration: 1172; Percent complete: 23.4%; Average loss: 0.0554\n",
      "Iteration: 1173; Percent complete: 23.5%; Average loss: 0.0454\n",
      "Iteration: 1174; Percent complete: 23.5%; Average loss: 0.0431\n",
      "Iteration: 1175; Percent complete: 23.5%; Average loss: 0.0438\n",
      "Iteration: 1176; Percent complete: 23.5%; Average loss: 0.0572\n",
      "Iteration: 1177; Percent complete: 23.5%; Average loss: 0.0483\n",
      "Iteration: 1178; Percent complete: 23.6%; Average loss: 0.0482\n",
      "Iteration: 1179; Percent complete: 23.6%; Average loss: 0.0472\n",
      "Iteration: 1180; Percent complete: 23.6%; Average loss: 0.0526\n",
      "Iteration: 1181; Percent complete: 23.6%; Average loss: 0.0551\n",
      "Iteration: 1182; Percent complete: 23.6%; Average loss: 0.0444\n",
      "Iteration: 1183; Percent complete: 23.7%; Average loss: 0.0394\n",
      "Iteration: 1184; Percent complete: 23.7%; Average loss: 0.0465\n",
      "Iteration: 1185; Percent complete: 23.7%; Average loss: 0.0462\n",
      "Iteration: 1186; Percent complete: 23.7%; Average loss: 0.0481\n",
      "Iteration: 1187; Percent complete: 23.7%; Average loss: 0.0555\n",
      "Iteration: 1188; Percent complete: 23.8%; Average loss: 0.0605\n",
      "Iteration: 1189; Percent complete: 23.8%; Average loss: 0.0476\n",
      "Iteration: 1190; Percent complete: 23.8%; Average loss: 0.0484\n",
      "Iteration: 1191; Percent complete: 23.8%; Average loss: 0.0517\n",
      "Iteration: 1192; Percent complete: 23.8%; Average loss: 0.0485\n",
      "Iteration: 1193; Percent complete: 23.9%; Average loss: 0.0407\n",
      "Iteration: 1194; Percent complete: 23.9%; Average loss: 0.0464\n",
      "Iteration: 1195; Percent complete: 23.9%; Average loss: 0.0510\n",
      "Iteration: 1196; Percent complete: 23.9%; Average loss: 0.0405\n",
      "Iteration: 1197; Percent complete: 23.9%; Average loss: 0.0505\n",
      "Iteration: 1198; Percent complete: 24.0%; Average loss: 0.0438\n",
      "Iteration: 1199; Percent complete: 24.0%; Average loss: 0.0451\n",
      "Iteration: 1200; Percent complete: 24.0%; Average loss: 0.0468\n",
      "Iteration: 1201; Percent complete: 24.0%; Average loss: 0.0488\n",
      "Iteration: 1202; Percent complete: 24.0%; Average loss: 0.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1203; Percent complete: 24.1%; Average loss: 0.0455\n",
      "Iteration: 1204; Percent complete: 24.1%; Average loss: 0.0445\n",
      "Iteration: 1205; Percent complete: 24.1%; Average loss: 0.0437\n",
      "Iteration: 1206; Percent complete: 24.1%; Average loss: 0.0409\n",
      "Iteration: 1207; Percent complete: 24.1%; Average loss: 0.0433\n",
      "Iteration: 1208; Percent complete: 24.2%; Average loss: 0.0405\n",
      "Iteration: 1209; Percent complete: 24.2%; Average loss: 0.0440\n",
      "Iteration: 1210; Percent complete: 24.2%; Average loss: 0.0412\n",
      "Iteration: 1211; Percent complete: 24.2%; Average loss: 0.0341\n",
      "Iteration: 1212; Percent complete: 24.2%; Average loss: 0.0414\n",
      "Iteration: 1213; Percent complete: 24.3%; Average loss: 0.0437\n",
      "Iteration: 1214; Percent complete: 24.3%; Average loss: 0.0493\n",
      "Iteration: 1215; Percent complete: 24.3%; Average loss: 0.0412\n",
      "Iteration: 1216; Percent complete: 24.3%; Average loss: 0.0406\n",
      "Iteration: 1217; Percent complete: 24.3%; Average loss: 0.0458\n",
      "Iteration: 1218; Percent complete: 24.4%; Average loss: 0.0481\n",
      "Iteration: 1219; Percent complete: 24.4%; Average loss: 0.0420\n",
      "Iteration: 1220; Percent complete: 24.4%; Average loss: 0.0536\n",
      "Iteration: 1221; Percent complete: 24.4%; Average loss: 0.0433\n",
      "Iteration: 1222; Percent complete: 24.4%; Average loss: 0.0449\n",
      "Iteration: 1223; Percent complete: 24.5%; Average loss: 0.0539\n",
      "Iteration: 1224; Percent complete: 24.5%; Average loss: 0.0528\n",
      "Iteration: 1225; Percent complete: 24.5%; Average loss: 0.0468\n",
      "Iteration: 1226; Percent complete: 24.5%; Average loss: 0.0458\n",
      "Iteration: 1227; Percent complete: 24.5%; Average loss: 0.0392\n",
      "Iteration: 1228; Percent complete: 24.6%; Average loss: 0.0377\n",
      "Iteration: 1229; Percent complete: 24.6%; Average loss: 0.0503\n",
      "Iteration: 1230; Percent complete: 24.6%; Average loss: 0.0443\n",
      "Iteration: 1231; Percent complete: 24.6%; Average loss: 0.0363\n",
      "Iteration: 1232; Percent complete: 24.6%; Average loss: 0.0383\n",
      "Iteration: 1233; Percent complete: 24.7%; Average loss: 0.0474\n",
      "Iteration: 1234; Percent complete: 24.7%; Average loss: 0.0503\n",
      "Iteration: 1235; Percent complete: 24.7%; Average loss: 0.0491\n",
      "Iteration: 1236; Percent complete: 24.7%; Average loss: 0.0558\n",
      "Iteration: 1237; Percent complete: 24.7%; Average loss: 0.0428\n",
      "Iteration: 1238; Percent complete: 24.8%; Average loss: 0.0404\n",
      "Iteration: 1239; Percent complete: 24.8%; Average loss: 0.0304\n",
      "Iteration: 1240; Percent complete: 24.8%; Average loss: 0.0406\n",
      "Iteration: 1241; Percent complete: 24.8%; Average loss: 0.0416\n",
      "Iteration: 1242; Percent complete: 24.8%; Average loss: 0.0407\n",
      "Iteration: 1243; Percent complete: 24.9%; Average loss: 0.0505\n",
      "Iteration: 1244; Percent complete: 24.9%; Average loss: 0.0483\n",
      "Iteration: 1245; Percent complete: 24.9%; Average loss: 0.0506\n",
      "Iteration: 1246; Percent complete: 24.9%; Average loss: 0.0544\n",
      "Iteration: 1247; Percent complete: 24.9%; Average loss: 0.0347\n",
      "Iteration: 1248; Percent complete: 25.0%; Average loss: 0.0423\n",
      "Iteration: 1249; Percent complete: 25.0%; Average loss: 0.0421\n",
      "Iteration: 1250; Percent complete: 25.0%; Average loss: 0.0520\n",
      "Iteration: 1251; Percent complete: 25.0%; Average loss: 0.0485\n",
      "Iteration: 1252; Percent complete: 25.0%; Average loss: 0.0492\n",
      "Iteration: 1253; Percent complete: 25.1%; Average loss: 0.0425\n",
      "Iteration: 1254; Percent complete: 25.1%; Average loss: 0.0395\n",
      "Iteration: 1255; Percent complete: 25.1%; Average loss: 0.0480\n",
      "Iteration: 1256; Percent complete: 25.1%; Average loss: 0.0425\n",
      "Iteration: 1257; Percent complete: 25.1%; Average loss: 0.0429\n",
      "Iteration: 1258; Percent complete: 25.2%; Average loss: 0.0506\n",
      "Iteration: 1259; Percent complete: 25.2%; Average loss: 0.0486\n",
      "Iteration: 1260; Percent complete: 25.2%; Average loss: 0.0377\n",
      "Iteration: 1261; Percent complete: 25.2%; Average loss: 0.0462\n",
      "Iteration: 1262; Percent complete: 25.2%; Average loss: 0.0399\n",
      "Iteration: 1263; Percent complete: 25.3%; Average loss: 0.0457\n",
      "Iteration: 1264; Percent complete: 25.3%; Average loss: 0.0416\n",
      "Iteration: 1265; Percent complete: 25.3%; Average loss: 0.0381\n",
      "Iteration: 1266; Percent complete: 25.3%; Average loss: 0.0396\n",
      "Iteration: 1267; Percent complete: 25.3%; Average loss: 0.0406\n",
      "Iteration: 1268; Percent complete: 25.4%; Average loss: 0.0515\n",
      "Iteration: 1269; Percent complete: 25.4%; Average loss: 0.0391\n",
      "Iteration: 1270; Percent complete: 25.4%; Average loss: 0.0307\n",
      "Iteration: 1271; Percent complete: 25.4%; Average loss: 0.0487\n",
      "Iteration: 1272; Percent complete: 25.4%; Average loss: 0.0360\n",
      "Iteration: 1273; Percent complete: 25.5%; Average loss: 0.0494\n",
      "Iteration: 1274; Percent complete: 25.5%; Average loss: 0.0465\n",
      "Iteration: 1275; Percent complete: 25.5%; Average loss: 0.0424\n",
      "Iteration: 1276; Percent complete: 25.5%; Average loss: 0.0477\n",
      "Iteration: 1277; Percent complete: 25.5%; Average loss: 0.0605\n",
      "Iteration: 1278; Percent complete: 25.6%; Average loss: 0.0381\n",
      "Iteration: 1279; Percent complete: 25.6%; Average loss: 0.0345\n",
      "Iteration: 1280; Percent complete: 25.6%; Average loss: 0.0512\n",
      "Iteration: 1281; Percent complete: 25.6%; Average loss: 0.0401\n",
      "Iteration: 1282; Percent complete: 25.6%; Average loss: 0.0411\n",
      "Iteration: 1283; Percent complete: 25.7%; Average loss: 0.0418\n",
      "Iteration: 1284; Percent complete: 25.7%; Average loss: 0.0434\n",
      "Iteration: 1285; Percent complete: 25.7%; Average loss: 0.0426\n",
      "Iteration: 1286; Percent complete: 25.7%; Average loss: 0.0484\n",
      "Iteration: 1287; Percent complete: 25.7%; Average loss: 0.0451\n",
      "Iteration: 1288; Percent complete: 25.8%; Average loss: 0.0416\n",
      "Iteration: 1289; Percent complete: 25.8%; Average loss: 0.0362\n",
      "Iteration: 1290; Percent complete: 25.8%; Average loss: 0.0397\n",
      "Iteration: 1291; Percent complete: 25.8%; Average loss: 0.0520\n",
      "Iteration: 1292; Percent complete: 25.8%; Average loss: 0.0438\n",
      "Iteration: 1293; Percent complete: 25.9%; Average loss: 0.0394\n",
      "Iteration: 1294; Percent complete: 25.9%; Average loss: 0.0422\n",
      "Iteration: 1295; Percent complete: 25.9%; Average loss: 0.0328\n",
      "Iteration: 1296; Percent complete: 25.9%; Average loss: 0.0305\n",
      "Iteration: 1297; Percent complete: 25.9%; Average loss: 0.0380\n",
      "Iteration: 1298; Percent complete: 26.0%; Average loss: 0.0347\n",
      "Iteration: 1299; Percent complete: 26.0%; Average loss: 0.0367\n",
      "Iteration: 1300; Percent complete: 26.0%; Average loss: 0.0376\n",
      "Iteration: 1301; Percent complete: 26.0%; Average loss: 0.0385\n",
      "Iteration: 1302; Percent complete: 26.0%; Average loss: 0.0438\n",
      "Iteration: 1303; Percent complete: 26.1%; Average loss: 0.0344\n",
      "Iteration: 1304; Percent complete: 26.1%; Average loss: 0.0384\n",
      "Iteration: 1305; Percent complete: 26.1%; Average loss: 0.0543\n",
      "Iteration: 1306; Percent complete: 26.1%; Average loss: 0.0354\n",
      "Iteration: 1307; Percent complete: 26.1%; Average loss: 0.0351\n",
      "Iteration: 1308; Percent complete: 26.2%; Average loss: 0.0382\n",
      "Iteration: 1309; Percent complete: 26.2%; Average loss: 0.0400\n",
      "Iteration: 1310; Percent complete: 26.2%; Average loss: 0.0343\n",
      "Iteration: 1311; Percent complete: 26.2%; Average loss: 0.0365\n",
      "Iteration: 1312; Percent complete: 26.2%; Average loss: 0.0327\n",
      "Iteration: 1313; Percent complete: 26.3%; Average loss: 0.0353\n",
      "Iteration: 1314; Percent complete: 26.3%; Average loss: 0.0441\n",
      "Iteration: 1315; Percent complete: 26.3%; Average loss: 0.0398\n",
      "Iteration: 1316; Percent complete: 26.3%; Average loss: 0.0340\n",
      "Iteration: 1317; Percent complete: 26.3%; Average loss: 0.0302\n",
      "Iteration: 1318; Percent complete: 26.4%; Average loss: 0.0363\n",
      "Iteration: 1319; Percent complete: 26.4%; Average loss: 0.0347\n",
      "Iteration: 1320; Percent complete: 26.4%; Average loss: 0.0397\n",
      "Iteration: 1321; Percent complete: 26.4%; Average loss: 0.0354\n",
      "Iteration: 1322; Percent complete: 26.4%; Average loss: 0.0260\n",
      "Iteration: 1323; Percent complete: 26.5%; Average loss: 0.0325\n",
      "Iteration: 1324; Percent complete: 26.5%; Average loss: 0.0312\n",
      "Iteration: 1325; Percent complete: 26.5%; Average loss: 0.0350\n",
      "Iteration: 1326; Percent complete: 26.5%; Average loss: 0.0436\n",
      "Iteration: 1327; Percent complete: 26.5%; Average loss: 0.0293\n",
      "Iteration: 1328; Percent complete: 26.6%; Average loss: 0.0365\n",
      "Iteration: 1329; Percent complete: 26.6%; Average loss: 0.0287\n",
      "Iteration: 1330; Percent complete: 26.6%; Average loss: 0.0366\n",
      "Iteration: 1331; Percent complete: 26.6%; Average loss: 0.0371\n",
      "Iteration: 1332; Percent complete: 26.6%; Average loss: 0.0306\n",
      "Iteration: 1333; Percent complete: 26.7%; Average loss: 0.0302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1334; Percent complete: 26.7%; Average loss: 0.0351\n",
      "Iteration: 1335; Percent complete: 26.7%; Average loss: 0.0364\n",
      "Iteration: 1336; Percent complete: 26.7%; Average loss: 0.0369\n",
      "Iteration: 1337; Percent complete: 26.7%; Average loss: 0.0442\n",
      "Iteration: 1338; Percent complete: 26.8%; Average loss: 0.0289\n",
      "Iteration: 1339; Percent complete: 26.8%; Average loss: 0.0319\n",
      "Iteration: 1340; Percent complete: 26.8%; Average loss: 0.0347\n",
      "Iteration: 1341; Percent complete: 26.8%; Average loss: 0.0384\n",
      "Iteration: 1342; Percent complete: 26.8%; Average loss: 0.0362\n",
      "Iteration: 1343; Percent complete: 26.9%; Average loss: 0.0335\n",
      "Iteration: 1344; Percent complete: 26.9%; Average loss: 0.0319\n",
      "Iteration: 1345; Percent complete: 26.9%; Average loss: 0.0337\n",
      "Iteration: 1346; Percent complete: 26.9%; Average loss: 0.0420\n",
      "Iteration: 1347; Percent complete: 26.9%; Average loss: 0.0290\n",
      "Iteration: 1348; Percent complete: 27.0%; Average loss: 0.0372\n",
      "Iteration: 1349; Percent complete: 27.0%; Average loss: 0.0280\n",
      "Iteration: 1350; Percent complete: 27.0%; Average loss: 0.0246\n",
      "Iteration: 1351; Percent complete: 27.0%; Average loss: 0.0451\n",
      "Iteration: 1352; Percent complete: 27.0%; Average loss: 0.0333\n",
      "Iteration: 1353; Percent complete: 27.1%; Average loss: 0.0407\n",
      "Iteration: 1354; Percent complete: 27.1%; Average loss: 0.0312\n",
      "Iteration: 1355; Percent complete: 27.1%; Average loss: 0.0335\n",
      "Iteration: 1356; Percent complete: 27.1%; Average loss: 0.0377\n",
      "Iteration: 1357; Percent complete: 27.1%; Average loss: 0.0478\n",
      "Iteration: 1358; Percent complete: 27.2%; Average loss: 0.0383\n",
      "Iteration: 1359; Percent complete: 27.2%; Average loss: 0.0362\n",
      "Iteration: 1360; Percent complete: 27.2%; Average loss: 0.0310\n",
      "Iteration: 1361; Percent complete: 27.2%; Average loss: 0.0318\n",
      "Iteration: 1362; Percent complete: 27.2%; Average loss: 0.0412\n",
      "Iteration: 1363; Percent complete: 27.3%; Average loss: 0.0301\n",
      "Iteration: 1364; Percent complete: 27.3%; Average loss: 0.0352\n",
      "Iteration: 1365; Percent complete: 27.3%; Average loss: 0.0387\n",
      "Iteration: 1366; Percent complete: 27.3%; Average loss: 0.0353\n",
      "Iteration: 1367; Percent complete: 27.3%; Average loss: 0.0335\n",
      "Iteration: 1368; Percent complete: 27.4%; Average loss: 0.0378\n",
      "Iteration: 1369; Percent complete: 27.4%; Average loss: 0.0307\n",
      "Iteration: 1370; Percent complete: 27.4%; Average loss: 0.0283\n",
      "Iteration: 1371; Percent complete: 27.4%; Average loss: 0.0316\n",
      "Iteration: 1372; Percent complete: 27.4%; Average loss: 0.0302\n",
      "Iteration: 1373; Percent complete: 27.5%; Average loss: 0.0366\n",
      "Iteration: 1374; Percent complete: 27.5%; Average loss: 0.0336\n",
      "Iteration: 1375; Percent complete: 27.5%; Average loss: 0.0380\n",
      "Iteration: 1376; Percent complete: 27.5%; Average loss: 0.0255\n",
      "Iteration: 1377; Percent complete: 27.5%; Average loss: 0.0304\n",
      "Iteration: 1378; Percent complete: 27.6%; Average loss: 0.0310\n",
      "Iteration: 1379; Percent complete: 27.6%; Average loss: 0.0341\n",
      "Iteration: 1380; Percent complete: 27.6%; Average loss: 0.0306\n",
      "Iteration: 1381; Percent complete: 27.6%; Average loss: 0.0235\n",
      "Iteration: 1382; Percent complete: 27.6%; Average loss: 0.0259\n",
      "Iteration: 1383; Percent complete: 27.7%; Average loss: 0.0343\n",
      "Iteration: 1384; Percent complete: 27.7%; Average loss: 0.0285\n",
      "Iteration: 1385; Percent complete: 27.7%; Average loss: 0.0284\n",
      "Iteration: 1386; Percent complete: 27.7%; Average loss: 0.0306\n",
      "Iteration: 1387; Percent complete: 27.7%; Average loss: 0.0308\n",
      "Iteration: 1388; Percent complete: 27.8%; Average loss: 0.0284\n",
      "Iteration: 1389; Percent complete: 27.8%; Average loss: 0.0350\n",
      "Iteration: 1390; Percent complete: 27.8%; Average loss: 0.0352\n",
      "Iteration: 1391; Percent complete: 27.8%; Average loss: 0.0246\n",
      "Iteration: 1392; Percent complete: 27.8%; Average loss: 0.0318\n",
      "Iteration: 1393; Percent complete: 27.9%; Average loss: 0.0327\n",
      "Iteration: 1394; Percent complete: 27.9%; Average loss: 0.0272\n",
      "Iteration: 1395; Percent complete: 27.9%; Average loss: 0.0315\n",
      "Iteration: 1396; Percent complete: 27.9%; Average loss: 0.0329\n",
      "Iteration: 1397; Percent complete: 27.9%; Average loss: 0.0358\n",
      "Iteration: 1398; Percent complete: 28.0%; Average loss: 0.0264\n",
      "Iteration: 1399; Percent complete: 28.0%; Average loss: 0.0277\n",
      "Iteration: 1400; Percent complete: 28.0%; Average loss: 0.0321\n",
      "Iteration: 1401; Percent complete: 28.0%; Average loss: 0.0293\n",
      "Iteration: 1402; Percent complete: 28.0%; Average loss: 0.0304\n",
      "Iteration: 1403; Percent complete: 28.1%; Average loss: 0.0235\n",
      "Iteration: 1404; Percent complete: 28.1%; Average loss: 0.0331\n",
      "Iteration: 1405; Percent complete: 28.1%; Average loss: 0.0234\n",
      "Iteration: 1406; Percent complete: 28.1%; Average loss: 0.0278\n",
      "Iteration: 1407; Percent complete: 28.1%; Average loss: 0.0285\n",
      "Iteration: 1408; Percent complete: 28.2%; Average loss: 0.0308\n",
      "Iteration: 1409; Percent complete: 28.2%; Average loss: 0.0259\n",
      "Iteration: 1410; Percent complete: 28.2%; Average loss: 0.0252\n",
      "Iteration: 1411; Percent complete: 28.2%; Average loss: 0.0323\n",
      "Iteration: 1412; Percent complete: 28.2%; Average loss: 0.0292\n",
      "Iteration: 1413; Percent complete: 28.3%; Average loss: 0.0255\n",
      "Iteration: 1414; Percent complete: 28.3%; Average loss: 0.0266\n",
      "Iteration: 1415; Percent complete: 28.3%; Average loss: 0.0281\n",
      "Iteration: 1416; Percent complete: 28.3%; Average loss: 0.0252\n",
      "Iteration: 1417; Percent complete: 28.3%; Average loss: 0.0299\n",
      "Iteration: 1418; Percent complete: 28.4%; Average loss: 0.0261\n",
      "Iteration: 1419; Percent complete: 28.4%; Average loss: 0.0243\n",
      "Iteration: 1420; Percent complete: 28.4%; Average loss: 0.0236\n",
      "Iteration: 1421; Percent complete: 28.4%; Average loss: 0.0287\n",
      "Iteration: 1422; Percent complete: 28.4%; Average loss: 0.0215\n",
      "Iteration: 1423; Percent complete: 28.5%; Average loss: 0.0337\n",
      "Iteration: 1424; Percent complete: 28.5%; Average loss: 0.0277\n",
      "Iteration: 1425; Percent complete: 28.5%; Average loss: 0.0221\n",
      "Iteration: 1426; Percent complete: 28.5%; Average loss: 0.0236\n",
      "Iteration: 1427; Percent complete: 28.5%; Average loss: 0.0322\n",
      "Iteration: 1428; Percent complete: 28.6%; Average loss: 0.0255\n",
      "Iteration: 1429; Percent complete: 28.6%; Average loss: 0.0278\n",
      "Iteration: 1430; Percent complete: 28.6%; Average loss: 0.0299\n",
      "Iteration: 1431; Percent complete: 28.6%; Average loss: 0.0284\n",
      "Iteration: 1432; Percent complete: 28.6%; Average loss: 0.0284\n",
      "Iteration: 1433; Percent complete: 28.7%; Average loss: 0.0266\n",
      "Iteration: 1434; Percent complete: 28.7%; Average loss: 0.0202\n",
      "Iteration: 1435; Percent complete: 28.7%; Average loss: 0.0207\n",
      "Iteration: 1436; Percent complete: 28.7%; Average loss: 0.0249\n",
      "Iteration: 1437; Percent complete: 28.7%; Average loss: 0.0253\n",
      "Iteration: 1438; Percent complete: 28.8%; Average loss: 0.0344\n",
      "Iteration: 1439; Percent complete: 28.8%; Average loss: 0.0261\n",
      "Iteration: 1440; Percent complete: 28.8%; Average loss: 0.0242\n",
      "Iteration: 1441; Percent complete: 28.8%; Average loss: 0.0374\n",
      "Iteration: 1442; Percent complete: 28.8%; Average loss: 0.0285\n",
      "Iteration: 1443; Percent complete: 28.9%; Average loss: 0.0258\n",
      "Iteration: 1444; Percent complete: 28.9%; Average loss: 0.0238\n",
      "Iteration: 1445; Percent complete: 28.9%; Average loss: 0.0213\n",
      "Iteration: 1446; Percent complete: 28.9%; Average loss: 0.0231\n",
      "Iteration: 1447; Percent complete: 28.9%; Average loss: 0.0400\n",
      "Iteration: 1448; Percent complete: 29.0%; Average loss: 0.0226\n",
      "Iteration: 1449; Percent complete: 29.0%; Average loss: 0.0244\n",
      "Iteration: 1450; Percent complete: 29.0%; Average loss: 0.0329\n",
      "Iteration: 1451; Percent complete: 29.0%; Average loss: 0.0259\n",
      "Iteration: 1452; Percent complete: 29.0%; Average loss: 0.0214\n",
      "Iteration: 1453; Percent complete: 29.1%; Average loss: 0.0227\n",
      "Iteration: 1454; Percent complete: 29.1%; Average loss: 0.0189\n",
      "Iteration: 1455; Percent complete: 29.1%; Average loss: 0.0242\n",
      "Iteration: 1456; Percent complete: 29.1%; Average loss: 0.0238\n",
      "Iteration: 1457; Percent complete: 29.1%; Average loss: 0.0243\n",
      "Iteration: 1458; Percent complete: 29.2%; Average loss: 0.0285\n",
      "Iteration: 1459; Percent complete: 29.2%; Average loss: 0.0213\n",
      "Iteration: 1460; Percent complete: 29.2%; Average loss: 0.0334\n",
      "Iteration: 1461; Percent complete: 29.2%; Average loss: 0.0237\n",
      "Iteration: 1462; Percent complete: 29.2%; Average loss: 0.0243\n",
      "Iteration: 1463; Percent complete: 29.3%; Average loss: 0.0231\n",
      "Iteration: 1464; Percent complete: 29.3%; Average loss: 0.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1465; Percent complete: 29.3%; Average loss: 0.0241\n",
      "Iteration: 1466; Percent complete: 29.3%; Average loss: 0.0210\n",
      "Iteration: 1467; Percent complete: 29.3%; Average loss: 0.0229\n",
      "Iteration: 1468; Percent complete: 29.4%; Average loss: 0.0260\n",
      "Iteration: 1469; Percent complete: 29.4%; Average loss: 0.0305\n",
      "Iteration: 1470; Percent complete: 29.4%; Average loss: 0.0286\n",
      "Iteration: 1471; Percent complete: 29.4%; Average loss: 0.0279\n",
      "Iteration: 1472; Percent complete: 29.4%; Average loss: 0.0238\n",
      "Iteration: 1473; Percent complete: 29.5%; Average loss: 0.0223\n",
      "Iteration: 1474; Percent complete: 29.5%; Average loss: 0.0256\n",
      "Iteration: 1475; Percent complete: 29.5%; Average loss: 0.0292\n",
      "Iteration: 1476; Percent complete: 29.5%; Average loss: 0.0216\n",
      "Iteration: 1477; Percent complete: 29.5%; Average loss: 0.0231\n",
      "Iteration: 1478; Percent complete: 29.6%; Average loss: 0.0246\n",
      "Iteration: 1479; Percent complete: 29.6%; Average loss: 0.0353\n",
      "Iteration: 1480; Percent complete: 29.6%; Average loss: 0.0312\n",
      "Iteration: 1481; Percent complete: 29.6%; Average loss: 0.0229\n",
      "Iteration: 1482; Percent complete: 29.6%; Average loss: 0.0331\n",
      "Iteration: 1483; Percent complete: 29.7%; Average loss: 0.0358\n",
      "Iteration: 1484; Percent complete: 29.7%; Average loss: 0.0305\n",
      "Iteration: 1485; Percent complete: 29.7%; Average loss: 0.0235\n",
      "Iteration: 1486; Percent complete: 29.7%; Average loss: 0.0333\n",
      "Iteration: 1487; Percent complete: 29.7%; Average loss: 0.0244\n",
      "Iteration: 1488; Percent complete: 29.8%; Average loss: 0.0195\n",
      "Iteration: 1489; Percent complete: 29.8%; Average loss: 0.0269\n",
      "Iteration: 1490; Percent complete: 29.8%; Average loss: 0.0260\n",
      "Iteration: 1491; Percent complete: 29.8%; Average loss: 0.0236\n",
      "Iteration: 1492; Percent complete: 29.8%; Average loss: 0.0223\n",
      "Iteration: 1493; Percent complete: 29.9%; Average loss: 0.0273\n",
      "Iteration: 1494; Percent complete: 29.9%; Average loss: 0.0222\n",
      "Iteration: 1495; Percent complete: 29.9%; Average loss: 0.0232\n",
      "Iteration: 1496; Percent complete: 29.9%; Average loss: 0.0288\n",
      "Iteration: 1497; Percent complete: 29.9%; Average loss: 0.0255\n",
      "Iteration: 1498; Percent complete: 30.0%; Average loss: 0.0271\n",
      "Iteration: 1499; Percent complete: 30.0%; Average loss: 0.0264\n",
      "Iteration: 1500; Percent complete: 30.0%; Average loss: 0.0235\n",
      "Iteration: 1501; Percent complete: 30.0%; Average loss: 0.0252\n",
      "Iteration: 1502; Percent complete: 30.0%; Average loss: 0.0187\n",
      "Iteration: 1503; Percent complete: 30.1%; Average loss: 0.0200\n",
      "Iteration: 1504; Percent complete: 30.1%; Average loss: 0.0227\n",
      "Iteration: 1505; Percent complete: 30.1%; Average loss: 0.0330\n",
      "Iteration: 1506; Percent complete: 30.1%; Average loss: 0.0251\n",
      "Iteration: 1507; Percent complete: 30.1%; Average loss: 0.0205\n",
      "Iteration: 1508; Percent complete: 30.2%; Average loss: 0.0276\n",
      "Iteration: 1509; Percent complete: 30.2%; Average loss: 0.0216\n",
      "Iteration: 1510; Percent complete: 30.2%; Average loss: 0.0273\n",
      "Iteration: 1511; Percent complete: 30.2%; Average loss: 0.0340\n",
      "Iteration: 1512; Percent complete: 30.2%; Average loss: 0.0265\n",
      "Iteration: 1513; Percent complete: 30.3%; Average loss: 0.0216\n",
      "Iteration: 1514; Percent complete: 30.3%; Average loss: 0.0246\n",
      "Iteration: 1515; Percent complete: 30.3%; Average loss: 0.0221\n",
      "Iteration: 1516; Percent complete: 30.3%; Average loss: 0.0206\n",
      "Iteration: 1517; Percent complete: 30.3%; Average loss: 0.0258\n",
      "Iteration: 1518; Percent complete: 30.4%; Average loss: 0.0226\n",
      "Iteration: 1519; Percent complete: 30.4%; Average loss: 0.0264\n",
      "Iteration: 1520; Percent complete: 30.4%; Average loss: 0.0213\n",
      "Iteration: 1521; Percent complete: 30.4%; Average loss: 0.0241\n",
      "Iteration: 1522; Percent complete: 30.4%; Average loss: 0.0221\n",
      "Iteration: 1523; Percent complete: 30.5%; Average loss: 0.0218\n",
      "Iteration: 1524; Percent complete: 30.5%; Average loss: 0.0387\n",
      "Iteration: 1525; Percent complete: 30.5%; Average loss: 0.0318\n",
      "Iteration: 1526; Percent complete: 30.5%; Average loss: 0.0223\n",
      "Iteration: 1527; Percent complete: 30.5%; Average loss: 0.0210\n",
      "Iteration: 1528; Percent complete: 30.6%; Average loss: 0.0177\n",
      "Iteration: 1529; Percent complete: 30.6%; Average loss: 0.0204\n",
      "Iteration: 1530; Percent complete: 30.6%; Average loss: 0.0300\n",
      "Iteration: 1531; Percent complete: 30.6%; Average loss: 0.0214\n",
      "Iteration: 1532; Percent complete: 30.6%; Average loss: 0.0285\n",
      "Iteration: 1533; Percent complete: 30.7%; Average loss: 0.0255\n",
      "Iteration: 1534; Percent complete: 30.7%; Average loss: 0.0257\n",
      "Iteration: 1535; Percent complete: 30.7%; Average loss: 0.0318\n",
      "Iteration: 1536; Percent complete: 30.7%; Average loss: 0.0308\n",
      "Iteration: 1537; Percent complete: 30.7%; Average loss: 0.0331\n",
      "Iteration: 1538; Percent complete: 30.8%; Average loss: 0.0247\n",
      "Iteration: 1539; Percent complete: 30.8%; Average loss: 0.0243\n",
      "Iteration: 1540; Percent complete: 30.8%; Average loss: 0.0319\n",
      "Iteration: 1541; Percent complete: 30.8%; Average loss: 0.0253\n",
      "Iteration: 1542; Percent complete: 30.8%; Average loss: 0.0304\n",
      "Iteration: 1543; Percent complete: 30.9%; Average loss: 0.0252\n",
      "Iteration: 1544; Percent complete: 30.9%; Average loss: 0.0241\n",
      "Iteration: 1545; Percent complete: 30.9%; Average loss: 0.0218\n",
      "Iteration: 1546; Percent complete: 30.9%; Average loss: 0.0309\n",
      "Iteration: 1547; Percent complete: 30.9%; Average loss: 0.0284\n",
      "Iteration: 1548; Percent complete: 31.0%; Average loss: 0.0288\n",
      "Iteration: 1549; Percent complete: 31.0%; Average loss: 0.0210\n",
      "Iteration: 1550; Percent complete: 31.0%; Average loss: 0.0220\n",
      "Iteration: 1551; Percent complete: 31.0%; Average loss: 0.0410\n",
      "Iteration: 1552; Percent complete: 31.0%; Average loss: 0.0321\n",
      "Iteration: 1553; Percent complete: 31.1%; Average loss: 0.0295\n",
      "Iteration: 1554; Percent complete: 31.1%; Average loss: 0.0334\n",
      "Iteration: 1555; Percent complete: 31.1%; Average loss: 0.0324\n",
      "Iteration: 1556; Percent complete: 31.1%; Average loss: 0.0276\n",
      "Iteration: 1557; Percent complete: 31.1%; Average loss: 0.0319\n",
      "Iteration: 1558; Percent complete: 31.2%; Average loss: 0.0252\n",
      "Iteration: 1559; Percent complete: 31.2%; Average loss: 0.0379\n",
      "Iteration: 1560; Percent complete: 31.2%; Average loss: 0.0273\n",
      "Iteration: 1561; Percent complete: 31.2%; Average loss: 0.0385\n",
      "Iteration: 1562; Percent complete: 31.2%; Average loss: 0.0261\n",
      "Iteration: 1563; Percent complete: 31.3%; Average loss: 0.0213\n",
      "Iteration: 1564; Percent complete: 31.3%; Average loss: 0.0284\n",
      "Iteration: 1565; Percent complete: 31.3%; Average loss: 0.0266\n",
      "Iteration: 1566; Percent complete: 31.3%; Average loss: 0.0268\n",
      "Iteration: 1567; Percent complete: 31.3%; Average loss: 0.0435\n",
      "Iteration: 1568; Percent complete: 31.4%; Average loss: 0.0249\n",
      "Iteration: 1569; Percent complete: 31.4%; Average loss: 0.0168\n",
      "Iteration: 1570; Percent complete: 31.4%; Average loss: 0.0243\n",
      "Iteration: 1571; Percent complete: 31.4%; Average loss: 0.0325\n",
      "Iteration: 1572; Percent complete: 31.4%; Average loss: 0.0277\n",
      "Iteration: 1573; Percent complete: 31.5%; Average loss: 0.0219\n",
      "Iteration: 1574; Percent complete: 31.5%; Average loss: 0.0314\n",
      "Iteration: 1575; Percent complete: 31.5%; Average loss: 0.0370\n",
      "Iteration: 1576; Percent complete: 31.5%; Average loss: 0.0222\n",
      "Iteration: 1577; Percent complete: 31.5%; Average loss: 0.0303\n",
      "Iteration: 1578; Percent complete: 31.6%; Average loss: 0.0211\n",
      "Iteration: 1579; Percent complete: 31.6%; Average loss: 0.0400\n",
      "Iteration: 1580; Percent complete: 31.6%; Average loss: 0.0393\n",
      "Iteration: 1581; Percent complete: 31.6%; Average loss: 0.0315\n",
      "Iteration: 1582; Percent complete: 31.6%; Average loss: 0.0378\n",
      "Iteration: 1583; Percent complete: 31.7%; Average loss: 0.0265\n",
      "Iteration: 1584; Percent complete: 31.7%; Average loss: 0.0233\n",
      "Iteration: 1585; Percent complete: 31.7%; Average loss: 0.0326\n",
      "Iteration: 1586; Percent complete: 31.7%; Average loss: 0.0275\n",
      "Iteration: 1587; Percent complete: 31.7%; Average loss: 0.0308\n",
      "Iteration: 1588; Percent complete: 31.8%; Average loss: 0.0220\n",
      "Iteration: 1589; Percent complete: 31.8%; Average loss: 0.0309\n",
      "Iteration: 1590; Percent complete: 31.8%; Average loss: 0.0217\n",
      "Iteration: 1591; Percent complete: 31.8%; Average loss: 0.0264\n",
      "Iteration: 1592; Percent complete: 31.8%; Average loss: 0.0253\n",
      "Iteration: 1593; Percent complete: 31.9%; Average loss: 0.0181\n",
      "Iteration: 1594; Percent complete: 31.9%; Average loss: 0.0217\n",
      "Iteration: 1595; Percent complete: 31.9%; Average loss: 0.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1596; Percent complete: 31.9%; Average loss: 0.0203\n",
      "Iteration: 1597; Percent complete: 31.9%; Average loss: 0.0201\n",
      "Iteration: 1598; Percent complete: 32.0%; Average loss: 0.0172\n",
      "Iteration: 1599; Percent complete: 32.0%; Average loss: 0.0300\n",
      "Iteration: 1600; Percent complete: 32.0%; Average loss: 0.0234\n",
      "Iteration: 1601; Percent complete: 32.0%; Average loss: 0.0241\n",
      "Iteration: 1602; Percent complete: 32.0%; Average loss: 0.0233\n",
      "Iteration: 1603; Percent complete: 32.1%; Average loss: 0.0230\n",
      "Iteration: 1604; Percent complete: 32.1%; Average loss: 0.0209\n",
      "Iteration: 1605; Percent complete: 32.1%; Average loss: 0.0276\n",
      "Iteration: 1606; Percent complete: 32.1%; Average loss: 0.0209\n",
      "Iteration: 1607; Percent complete: 32.1%; Average loss: 0.0339\n",
      "Iteration: 1608; Percent complete: 32.2%; Average loss: 0.0272\n",
      "Iteration: 1609; Percent complete: 32.2%; Average loss: 0.0202\n",
      "Iteration: 1610; Percent complete: 32.2%; Average loss: 0.0206\n",
      "Iteration: 1611; Percent complete: 32.2%; Average loss: 0.0191\n",
      "Iteration: 1612; Percent complete: 32.2%; Average loss: 0.0241\n",
      "Iteration: 1613; Percent complete: 32.3%; Average loss: 0.0334\n",
      "Iteration: 1614; Percent complete: 32.3%; Average loss: 0.0200\n",
      "Iteration: 1615; Percent complete: 32.3%; Average loss: 0.0203\n",
      "Iteration: 1616; Percent complete: 32.3%; Average loss: 0.0222\n",
      "Iteration: 1617; Percent complete: 32.3%; Average loss: 0.0237\n",
      "Iteration: 1618; Percent complete: 32.4%; Average loss: 0.0192\n",
      "Iteration: 1619; Percent complete: 32.4%; Average loss: 0.0308\n",
      "Iteration: 1620; Percent complete: 32.4%; Average loss: 0.0254\n",
      "Iteration: 1621; Percent complete: 32.4%; Average loss: 0.0300\n",
      "Iteration: 1622; Percent complete: 32.4%; Average loss: 0.0188\n",
      "Iteration: 1623; Percent complete: 32.5%; Average loss: 0.0183\n",
      "Iteration: 1624; Percent complete: 32.5%; Average loss: 0.0208\n",
      "Iteration: 1625; Percent complete: 32.5%; Average loss: 0.0222\n",
      "Iteration: 1626; Percent complete: 32.5%; Average loss: 0.0201\n",
      "Iteration: 1627; Percent complete: 32.5%; Average loss: 0.0256\n",
      "Iteration: 1628; Percent complete: 32.6%; Average loss: 0.0425\n",
      "Iteration: 1629; Percent complete: 32.6%; Average loss: 0.0207\n",
      "Iteration: 1630; Percent complete: 32.6%; Average loss: 0.0250\n",
      "Iteration: 1631; Percent complete: 32.6%; Average loss: 0.0214\n",
      "Iteration: 1632; Percent complete: 32.6%; Average loss: 0.0207\n",
      "Iteration: 1633; Percent complete: 32.7%; Average loss: 0.0284\n",
      "Iteration: 1634; Percent complete: 32.7%; Average loss: 0.0267\n",
      "Iteration: 1635; Percent complete: 32.7%; Average loss: 0.0198\n",
      "Iteration: 1636; Percent complete: 32.7%; Average loss: 0.0176\n",
      "Iteration: 1637; Percent complete: 32.7%; Average loss: 0.0218\n",
      "Iteration: 1638; Percent complete: 32.8%; Average loss: 0.0219\n",
      "Iteration: 1639; Percent complete: 32.8%; Average loss: 0.0243\n",
      "Iteration: 1640; Percent complete: 32.8%; Average loss: 0.0229\n",
      "Iteration: 1641; Percent complete: 32.8%; Average loss: 0.0179\n",
      "Iteration: 1642; Percent complete: 32.8%; Average loss: 0.0220\n",
      "Iteration: 1643; Percent complete: 32.9%; Average loss: 0.0262\n",
      "Iteration: 1644; Percent complete: 32.9%; Average loss: 0.0254\n",
      "Iteration: 1645; Percent complete: 32.9%; Average loss: 0.0175\n",
      "Iteration: 1646; Percent complete: 32.9%; Average loss: 0.0196\n",
      "Iteration: 1647; Percent complete: 32.9%; Average loss: 0.0262\n",
      "Iteration: 1648; Percent complete: 33.0%; Average loss: 0.0345\n",
      "Iteration: 1649; Percent complete: 33.0%; Average loss: 0.0298\n",
      "Iteration: 1650; Percent complete: 33.0%; Average loss: 0.0186\n",
      "Iteration: 1651; Percent complete: 33.0%; Average loss: 0.0187\n",
      "Iteration: 1652; Percent complete: 33.0%; Average loss: 0.0224\n",
      "Iteration: 1653; Percent complete: 33.1%; Average loss: 0.0166\n",
      "Iteration: 1654; Percent complete: 33.1%; Average loss: 0.0256\n",
      "Iteration: 1655; Percent complete: 33.1%; Average loss: 0.0194\n",
      "Iteration: 1656; Percent complete: 33.1%; Average loss: 0.0163\n",
      "Iteration: 1657; Percent complete: 33.1%; Average loss: 0.0192\n",
      "Iteration: 1658; Percent complete: 33.2%; Average loss: 0.0237\n",
      "Iteration: 1659; Percent complete: 33.2%; Average loss: 0.0290\n",
      "Iteration: 1660; Percent complete: 33.2%; Average loss: 0.0163\n",
      "Iteration: 1661; Percent complete: 33.2%; Average loss: 0.0209\n",
      "Iteration: 1662; Percent complete: 33.2%; Average loss: 0.0186\n",
      "Iteration: 1663; Percent complete: 33.3%; Average loss: 0.0238\n",
      "Iteration: 1664; Percent complete: 33.3%; Average loss: 0.0248\n",
      "Iteration: 1665; Percent complete: 33.3%; Average loss: 0.0260\n",
      "Iteration: 1666; Percent complete: 33.3%; Average loss: 0.0242\n",
      "Iteration: 1667; Percent complete: 33.3%; Average loss: 0.0275\n",
      "Iteration: 1668; Percent complete: 33.4%; Average loss: 0.0417\n",
      "Iteration: 1669; Percent complete: 33.4%; Average loss: 0.0210\n",
      "Iteration: 1670; Percent complete: 33.4%; Average loss: 0.0246\n",
      "Iteration: 1671; Percent complete: 33.4%; Average loss: 0.0265\n",
      "Iteration: 1672; Percent complete: 33.4%; Average loss: 0.0289\n",
      "Iteration: 1673; Percent complete: 33.5%; Average loss: 0.0185\n",
      "Iteration: 1674; Percent complete: 33.5%; Average loss: 0.0309\n",
      "Iteration: 1675; Percent complete: 33.5%; Average loss: 0.0216\n",
      "Iteration: 1676; Percent complete: 33.5%; Average loss: 0.0366\n",
      "Iteration: 1677; Percent complete: 33.5%; Average loss: 0.0242\n",
      "Iteration: 1678; Percent complete: 33.6%; Average loss: 0.0257\n",
      "Iteration: 1679; Percent complete: 33.6%; Average loss: 0.0214\n",
      "Iteration: 1680; Percent complete: 33.6%; Average loss: 0.0214\n",
      "Iteration: 1681; Percent complete: 33.6%; Average loss: 0.0201\n",
      "Iteration: 1682; Percent complete: 33.6%; Average loss: 0.0236\n",
      "Iteration: 1683; Percent complete: 33.7%; Average loss: 0.0255\n",
      "Iteration: 1684; Percent complete: 33.7%; Average loss: 0.0239\n",
      "Iteration: 1685; Percent complete: 33.7%; Average loss: 0.0254\n",
      "Iteration: 1686; Percent complete: 33.7%; Average loss: 0.0280\n",
      "Iteration: 1687; Percent complete: 33.7%; Average loss: 0.0434\n",
      "Iteration: 1688; Percent complete: 33.8%; Average loss: 0.0348\n",
      "Iteration: 1689; Percent complete: 33.8%; Average loss: 0.0188\n",
      "Iteration: 1690; Percent complete: 33.8%; Average loss: 0.0228\n",
      "Iteration: 1691; Percent complete: 33.8%; Average loss: 0.0311\n",
      "Iteration: 1692; Percent complete: 33.8%; Average loss: 0.0189\n",
      "Iteration: 1693; Percent complete: 33.9%; Average loss: 0.0297\n",
      "Iteration: 1694; Percent complete: 33.9%; Average loss: 0.0163\n",
      "Iteration: 1695; Percent complete: 33.9%; Average loss: 0.0192\n",
      "Iteration: 1696; Percent complete: 33.9%; Average loss: 0.0226\n",
      "Iteration: 1697; Percent complete: 33.9%; Average loss: 0.0197\n",
      "Iteration: 1698; Percent complete: 34.0%; Average loss: 0.0211\n",
      "Iteration: 1699; Percent complete: 34.0%; Average loss: 0.0180\n",
      "Iteration: 1700; Percent complete: 34.0%; Average loss: 0.0180\n",
      "Iteration: 1701; Percent complete: 34.0%; Average loss: 0.0323\n",
      "Iteration: 1702; Percent complete: 34.0%; Average loss: 0.0176\n",
      "Iteration: 1703; Percent complete: 34.1%; Average loss: 0.0266\n",
      "Iteration: 1704; Percent complete: 34.1%; Average loss: 0.0207\n",
      "Iteration: 1705; Percent complete: 34.1%; Average loss: 0.0206\n",
      "Iteration: 1706; Percent complete: 34.1%; Average loss: 0.0335\n",
      "Iteration: 1707; Percent complete: 34.1%; Average loss: 0.0204\n",
      "Iteration: 1708; Percent complete: 34.2%; Average loss: 0.0120\n",
      "Iteration: 1709; Percent complete: 34.2%; Average loss: 0.0198\n",
      "Iteration: 1710; Percent complete: 34.2%; Average loss: 0.0361\n",
      "Iteration: 1711; Percent complete: 34.2%; Average loss: 0.0313\n",
      "Iteration: 1712; Percent complete: 34.2%; Average loss: 0.0289\n",
      "Iteration: 1713; Percent complete: 34.3%; Average loss: 0.0195\n",
      "Iteration: 1714; Percent complete: 34.3%; Average loss: 0.0187\n",
      "Iteration: 1715; Percent complete: 34.3%; Average loss: 0.0295\n",
      "Iteration: 1716; Percent complete: 34.3%; Average loss: 0.0270\n",
      "Iteration: 1717; Percent complete: 34.3%; Average loss: 0.0224\n",
      "Iteration: 1718; Percent complete: 34.4%; Average loss: 0.0175\n",
      "Iteration: 1719; Percent complete: 34.4%; Average loss: 0.0296\n",
      "Iteration: 1720; Percent complete: 34.4%; Average loss: 0.0211\n",
      "Iteration: 1721; Percent complete: 34.4%; Average loss: 0.0177\n",
      "Iteration: 1722; Percent complete: 34.4%; Average loss: 0.0213\n",
      "Iteration: 1723; Percent complete: 34.5%; Average loss: 0.0207\n",
      "Iteration: 1724; Percent complete: 34.5%; Average loss: 0.0210\n",
      "Iteration: 1725; Percent complete: 34.5%; Average loss: 0.0172\n",
      "Iteration: 1726; Percent complete: 34.5%; Average loss: 0.0209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1727; Percent complete: 34.5%; Average loss: 0.0275\n",
      "Iteration: 1728; Percent complete: 34.6%; Average loss: 0.0200\n",
      "Iteration: 1729; Percent complete: 34.6%; Average loss: 0.0175\n",
      "Iteration: 1730; Percent complete: 34.6%; Average loss: 0.0192\n",
      "Iteration: 1731; Percent complete: 34.6%; Average loss: 0.0165\n",
      "Iteration: 1732; Percent complete: 34.6%; Average loss: 0.0206\n",
      "Iteration: 1733; Percent complete: 34.7%; Average loss: 0.0202\n",
      "Iteration: 1734; Percent complete: 34.7%; Average loss: 0.0180\n",
      "Iteration: 1735; Percent complete: 34.7%; Average loss: 0.0179\n",
      "Iteration: 1736; Percent complete: 34.7%; Average loss: 0.0155\n",
      "Iteration: 1737; Percent complete: 34.7%; Average loss: 0.0212\n",
      "Iteration: 1738; Percent complete: 34.8%; Average loss: 0.0171\n",
      "Iteration: 1739; Percent complete: 34.8%; Average loss: 0.0134\n",
      "Iteration: 1740; Percent complete: 34.8%; Average loss: 0.0217\n",
      "Iteration: 1741; Percent complete: 34.8%; Average loss: 0.0150\n",
      "Iteration: 1742; Percent complete: 34.8%; Average loss: 0.0178\n",
      "Iteration: 1743; Percent complete: 34.9%; Average loss: 0.0148\n",
      "Iteration: 1744; Percent complete: 34.9%; Average loss: 0.0178\n",
      "Iteration: 1745; Percent complete: 34.9%; Average loss: 0.0159\n",
      "Iteration: 1746; Percent complete: 34.9%; Average loss: 0.0162\n",
      "Iteration: 1747; Percent complete: 34.9%; Average loss: 0.0160\n",
      "Iteration: 1748; Percent complete: 35.0%; Average loss: 0.0141\n",
      "Iteration: 1749; Percent complete: 35.0%; Average loss: 0.0191\n",
      "Iteration: 1750; Percent complete: 35.0%; Average loss: 0.0256\n",
      "Iteration: 1751; Percent complete: 35.0%; Average loss: 0.0183\n",
      "Iteration: 1752; Percent complete: 35.0%; Average loss: 0.0165\n",
      "Iteration: 1753; Percent complete: 35.1%; Average loss: 0.0162\n",
      "Iteration: 1754; Percent complete: 35.1%; Average loss: 0.0158\n",
      "Iteration: 1755; Percent complete: 35.1%; Average loss: 0.0160\n",
      "Iteration: 1756; Percent complete: 35.1%; Average loss: 0.0176\n",
      "Iteration: 1757; Percent complete: 35.1%; Average loss: 0.0171\n",
      "Iteration: 1758; Percent complete: 35.2%; Average loss: 0.0151\n",
      "Iteration: 1759; Percent complete: 35.2%; Average loss: 0.0186\n",
      "Iteration: 1760; Percent complete: 35.2%; Average loss: 0.0170\n",
      "Iteration: 1761; Percent complete: 35.2%; Average loss: 0.0148\n",
      "Iteration: 1762; Percent complete: 35.2%; Average loss: 0.0222\n",
      "Iteration: 1763; Percent complete: 35.3%; Average loss: 0.0183\n",
      "Iteration: 1764; Percent complete: 35.3%; Average loss: 0.0138\n",
      "Iteration: 1765; Percent complete: 35.3%; Average loss: 0.0167\n",
      "Iteration: 1766; Percent complete: 35.3%; Average loss: 0.0204\n",
      "Iteration: 1767; Percent complete: 35.3%; Average loss: 0.0149\n",
      "Iteration: 1768; Percent complete: 35.4%; Average loss: 0.0267\n",
      "Iteration: 1769; Percent complete: 35.4%; Average loss: 0.0130\n",
      "Iteration: 1770; Percent complete: 35.4%; Average loss: 0.0162\n",
      "Iteration: 1771; Percent complete: 35.4%; Average loss: 0.0167\n",
      "Iteration: 1772; Percent complete: 35.4%; Average loss: 0.0181\n",
      "Iteration: 1773; Percent complete: 35.5%; Average loss: 0.0176\n",
      "Iteration: 1774; Percent complete: 35.5%; Average loss: 0.0151\n",
      "Iteration: 1775; Percent complete: 35.5%; Average loss: 0.0166\n",
      "Iteration: 1776; Percent complete: 35.5%; Average loss: 0.0230\n",
      "Iteration: 1777; Percent complete: 35.5%; Average loss: 0.0196\n",
      "Iteration: 1778; Percent complete: 35.6%; Average loss: 0.0193\n",
      "Iteration: 1779; Percent complete: 35.6%; Average loss: 0.0218\n",
      "Iteration: 1780; Percent complete: 35.6%; Average loss: 0.0300\n",
      "Iteration: 1781; Percent complete: 35.6%; Average loss: 0.0253\n",
      "Iteration: 1782; Percent complete: 35.6%; Average loss: 0.0120\n",
      "Iteration: 1783; Percent complete: 35.7%; Average loss: 0.0176\n",
      "Iteration: 1784; Percent complete: 35.7%; Average loss: 0.0189\n",
      "Iteration: 1785; Percent complete: 35.7%; Average loss: 0.0202\n",
      "Iteration: 1786; Percent complete: 35.7%; Average loss: 0.0168\n",
      "Iteration: 1787; Percent complete: 35.7%; Average loss: 0.0170\n",
      "Iteration: 1788; Percent complete: 35.8%; Average loss: 0.0145\n",
      "Iteration: 1789; Percent complete: 35.8%; Average loss: 0.0165\n",
      "Iteration: 1790; Percent complete: 35.8%; Average loss: 0.0185\n",
      "Iteration: 1791; Percent complete: 35.8%; Average loss: 0.0143\n",
      "Iteration: 1792; Percent complete: 35.8%; Average loss: 0.0129\n",
      "Iteration: 1793; Percent complete: 35.9%; Average loss: 0.0192\n",
      "Iteration: 1794; Percent complete: 35.9%; Average loss: 0.0172\n",
      "Iteration: 1795; Percent complete: 35.9%; Average loss: 0.0151\n",
      "Iteration: 1796; Percent complete: 35.9%; Average loss: 0.0149\n",
      "Iteration: 1797; Percent complete: 35.9%; Average loss: 0.0142\n",
      "Iteration: 1798; Percent complete: 36.0%; Average loss: 0.0233\n",
      "Iteration: 1799; Percent complete: 36.0%; Average loss: 0.0224\n",
      "Iteration: 1800; Percent complete: 36.0%; Average loss: 0.0140\n",
      "Iteration: 1801; Percent complete: 36.0%; Average loss: 0.0144\n",
      "Iteration: 1802; Percent complete: 36.0%; Average loss: 0.0136\n",
      "Iteration: 1803; Percent complete: 36.1%; Average loss: 0.0197\n",
      "Iteration: 1804; Percent complete: 36.1%; Average loss: 0.0167\n",
      "Iteration: 1805; Percent complete: 36.1%; Average loss: 0.0169\n",
      "Iteration: 1806; Percent complete: 36.1%; Average loss: 0.0277\n",
      "Iteration: 1807; Percent complete: 36.1%; Average loss: 0.0142\n",
      "Iteration: 1808; Percent complete: 36.2%; Average loss: 0.0203\n",
      "Iteration: 1809; Percent complete: 36.2%; Average loss: 0.0197\n",
      "Iteration: 1810; Percent complete: 36.2%; Average loss: 0.0179\n",
      "Iteration: 1811; Percent complete: 36.2%; Average loss: 0.0137\n",
      "Iteration: 1812; Percent complete: 36.2%; Average loss: 0.0145\n",
      "Iteration: 1813; Percent complete: 36.3%; Average loss: 0.0138\n",
      "Iteration: 1814; Percent complete: 36.3%; Average loss: 0.0181\n",
      "Iteration: 1815; Percent complete: 36.3%; Average loss: 0.0155\n",
      "Iteration: 1816; Percent complete: 36.3%; Average loss: 0.0147\n",
      "Iteration: 1817; Percent complete: 36.3%; Average loss: 0.0132\n",
      "Iteration: 1818; Percent complete: 36.4%; Average loss: 0.0124\n",
      "Iteration: 1819; Percent complete: 36.4%; Average loss: 0.0115\n",
      "Iteration: 1820; Percent complete: 36.4%; Average loss: 0.0158\n",
      "Iteration: 1821; Percent complete: 36.4%; Average loss: 0.0194\n",
      "Iteration: 1822; Percent complete: 36.4%; Average loss: 0.0185\n",
      "Iteration: 1823; Percent complete: 36.5%; Average loss: 0.0183\n",
      "Iteration: 1824; Percent complete: 36.5%; Average loss: 0.0212\n",
      "Iteration: 1825; Percent complete: 36.5%; Average loss: 0.0123\n",
      "Iteration: 1826; Percent complete: 36.5%; Average loss: 0.0167\n",
      "Iteration: 1827; Percent complete: 36.5%; Average loss: 0.0175\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 최적화 설정\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 5000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Dropout 레이어를 학습 모드로 둡니다\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Optimizer를 초기화합니다\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# cuda가 있다면 cuda를 설정합니다\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "    \n",
    "# 학습 단계를 수행합니다\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropout 레이어를 평가 모드로 설정합니다\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 탐색 모듈을 초기화합니다\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# 채팅을 시작합니다 (다음 줄의 주석을 제거하면 시작해볼 수 있습니다)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
